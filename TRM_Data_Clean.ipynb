{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§ª TRM Dataset Comparison - Faithful Reproduction (Fixed)\n",
        "\n",
        "**å®Œå…¨å¿ å®žäºŽ TinyRecursiveModels åŽŸå§‹å®žçŽ°**ï¼Œä»…ä¿®æ”¹ï¼š\n",
        "- ä¼˜åŒ–å™¨: AdamAtan2 â†’ AdamW\n",
        "- å• GPU è¿è¡Œï¼ˆç§»é™¤åˆ†å¸ƒå¼ä»£ç ï¼‰\n",
        "\n",
        "**æ–°å¢žæ•ˆçŽ‡æŒ‡æ ‡**:\n",
        "- GPU å†…å­˜è¿½è¸ª\n",
        "- FLOPs ä¼°ç®—\n",
        "- EMA è¯„ä¼°æ”¯æŒ\n",
        "\n",
        "| é…ç½® | è°œé¢˜æ•° | å¢žå¼º | Epochs | Samples Seen |\n",
        "|------|-------|-----|--------|-------------|\n",
        "| `1k-aug1000` | 1K | 1000Ã— | **2** | ~2.0M |\n",
        "| `10k-aug100` | 10K | 100Ã— | **2** | ~2.0M |\n",
        "| `100k-aug10` | 100K | 10Ã— | **2** | ~2.2M |\n",
        "| `1M-aug0` | 1M | 0Ã— | **2** | ~2.0M |\n",
        "\n",
        "**æ³¨æ„**: æ‰€æœ‰é…ç½®è®¾ç½®ä¸º2 epochsï¼Œç¡®ä¿çœ‹åˆ°ç›¸åŒçš„~2Mæ ·æœ¬ï¼Œå®žçŽ°å…¬å¹³å¯¹æ¯”ã€‚\n",
        "\n",
        "**v2 ä¿®å¤å†…å®¹**:\n",
        "- ä¿®å¤ EMA + torch.compile å…¼å®¹æ€§é—®é¢˜\n",
        "- æ·»åŠ  CUDA ç¼“å­˜æ¸…ç†\n",
        "- ä¿®å¤è¯„ä¼°å¾ªçŽ¯ä¸­çš„å†…å­˜æ³„æ¼\n",
        "- æ·»åŠ æ›´å¥½çš„é”™è¯¯å¤„ç†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1ï¸âƒ£ Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch einops tqdm numpy pydantic wandb coolname huggingface_hub\n",
        "print(\"âœ… Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ”‘ API Tokens\n",
        "# ============================================================================\n",
        "WANDB_API_KEY = ''\n",
        "HF_TOKEN = \"\"\n",
        "\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "import wandb\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "print(\"âœ… Logged in\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸŽ¯ EXPERIMENT CONFIGURATION\n",
        "# ============================================================================\n",
        "SELECTED_CONFIG = \"1k-aug1000\"  # \"1k-aug1000\", \"10k-aug100\", \"100k-aug10\", \"1M-aug0\"\n",
        "BATCH_SIZE = 1024\n",
        "PROJECT_NAME = \"TRM-DataVS\"\n",
        "FORCE_REBUILD = False\n",
        "USE_EMA = True  # ä½¿ç”¨ EMA è¯„ä¼°ï¼ˆåŽŸå§‹å®žçŽ°æ”¯æŒï¼‰\n",
        "MAX_EVAL_BATCHES = 100  # é™åˆ¶è¯„ä¼°batchæ•°é‡ï¼ŒèŠ‚çœæ—¶é—´ï¼ˆè®¾ç½®ä¸ºNoneè¯„ä¼°å®Œæ•´æ•°æ®é›†ï¼‰\n",
        "\n",
        "DATASET_CONFIGS = {\n",
        "    # æ‰€æœ‰é…ç½®è®¾ç½®ä¸º2 epochsï¼Œç¡®ä¿çœ‹åˆ°ç›¸åŒçš„~2Mæ ·æœ¬ï¼ˆå…¬å¹³å¯¹æ¯”ï¼‰\n",
        "    # è®¡ç®—å…¬å¼: epochs = 2,000,000 / (total_groups Ã— mean_puzzle_examples)\n",
        "    \"1k-aug1000\": {\"subsample\": 1000, \"augmentation\": 1000, \"epochs\": 20, \"desc\": \"1K Ã— 1000 aug (baseline)\"},\n",
        "    \"10k-aug100\": {\"subsample\": 10000, \"augmentation\": 100, \"epochs\": 20, \"desc\": \"10K Ã— 100 aug\"},\n",
        "    \"100k-aug10\": {\"subsample\": 100000, \"augmentation\": 10, \"epochs\": 20, \"desc\": \"100K Ã— 10 aug\"},\n",
        "    \"1M-aug0\": {\"subsample\": 1000000, \"augmentation\": 0, \"epochs\": 20, \"desc\": \"1M, no aug\"},\n",
        "}\n",
        "\n",
        "cfg = DATASET_CONFIGS[SELECTED_CONFIG]\n",
        "print(f\"ðŸ“Š Config: {SELECTED_CONFIG} - {cfg['desc']}\")\n",
        "print(f\"   Epochs: {cfg['epochs']}, Batch: {BATCH_SIZE}, EMA: {USE_EMA}\")\n",
        "print(f\"   Max Eval Batches: {MAX_EVAL_BATCHES if MAX_EVAL_BATCHES else 'Full dataset'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2ï¸âƒ£ Core Implementation (Faithful to TinyRecursiveModels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Imports & GPU Setup\n",
        "# ============================================================================\n",
        "import os, sys, math, json, csv, copy, time, shutil\n",
        "from typing import Optional, Any, Sequence, List, Dict, Tuple, Union\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, IterableDataset, get_worker_info\n",
        "from torch.optim.optimizer import Optimizer, ParamsT\n",
        "from tqdm import tqdm\n",
        "import pydantic\n",
        "from pydantic import BaseModel\n",
        "import einops\n",
        "from torch.nn.functional import scaled_dot_product_attention\n",
        "\n",
        "# GPU Setup (H100/A100 TF32)\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"ðŸ–¥ï¸ GPU: {gpu_name}\")\n",
        "    if \"H100\" in gpu_name or \"A100\" in gpu_name:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        print(\"ðŸš€ TF32 enabled\")\n",
        "\n",
        "IGNORE_LABEL_ID = -100\n",
        "print(\"âœ… Imports complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Common Utils (models/common.py)\n",
        "# ============================================================================\n",
        "def trunc_normal_init_(tensor: torch.Tensor, std: float = 1.0, lower: float = -2.0, upper: float = 2.0):\n",
        "    \"\"\"Truncated normal init - JAX compatible (exact copy from TinyRecursiveModels)\"\"\"\n",
        "    with torch.no_grad():\n",
        "        if std == 0:\n",
        "            tensor.zero_()\n",
        "        else:\n",
        "            sqrt2 = math.sqrt(2)\n",
        "            a = math.erf(lower / sqrt2)\n",
        "            b = math.erf(upper / sqrt2)\n",
        "            z = (b - a) / 2\n",
        "            c = (2 * math.pi) ** -0.5\n",
        "            pdf_u = c * math.exp(-0.5 * lower ** 2)\n",
        "            pdf_l = c * math.exp(-0.5 * upper ** 2)\n",
        "            comp_std = std / math.sqrt(1 - (upper * pdf_u - lower * pdf_l) / z - ((pdf_u - pdf_l) / z) ** 2)\n",
        "            tensor.uniform_(a, b)\n",
        "            tensor.erfinv_()\n",
        "            tensor.mul_(sqrt2 * comp_std)\n",
        "            tensor.clip_(lower * comp_std, upper * comp_std)\n",
        "    return tensor\n",
        "\n",
        "print(\"âœ… Common utils loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Layers (models/layers.py) - Exact copy\n",
        "# ============================================================================\n",
        "CosSin = Tuple[torch.Tensor, torch.Tensor]\n",
        "\n",
        "def _find_multiple(a, b):\n",
        "    return (-(a // -b)) * b\n",
        "\n",
        "def rotate_half(x: torch.Tensor):\n",
        "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor):\n",
        "    orig_dtype = q.dtype\n",
        "    q = q.to(cos.dtype)\n",
        "    k = k.to(cos.dtype)\n",
        "    q_embed = (q * cos.unsqueeze(-2)) + (rotate_half(q) * sin.unsqueeze(-2))\n",
        "    k_embed = (k * cos.unsqueeze(-2)) + (rotate_half(k) * sin.unsqueeze(-2))\n",
        "    return q_embed.to(orig_dtype), k_embed.to(orig_dtype)\n",
        "\n",
        "def rms_norm(hidden_states: torch.Tensor, variance_epsilon: float) -> torch.Tensor:\n",
        "    input_dtype = hidden_states.dtype\n",
        "    hidden_states = hidden_states.to(torch.float32)\n",
        "    variance = hidden_states.square().mean(-1, keepdim=True)\n",
        "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
        "    return hidden_states.to(input_dtype)\n",
        "\n",
        "class CastedLinear(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(trunc_normal_init_(torch.empty((out_features, in_features)), std=1.0 / (in_features ** 0.5)))\n",
        "        self.bias = nn.Parameter(torch.zeros((out_features,))) if bias else None\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return F.linear(input, self.weight.to(input.dtype), bias=self.bias.to(input.dtype) if self.bias is not None else None)\n",
        "\n",
        "class CastedEmbedding(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, init_std: float, cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "        self.embedding_weight = nn.Parameter(trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std))\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return F.embedding(input, self.embedding_weight.to(self.cast_to))\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_position_embeddings, base, device=None):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))\n",
        "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
        "        freqs = torch.outer(t, inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.cos_cached = nn.Buffer(emb.cos(), persistent=False)\n",
        "        self.sin_cached = nn.Buffer(emb.sin(), persistent=False)\n",
        "    def forward(self):\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, head_dim, num_heads, num_key_value_heads, causal=False):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.head_dim = head_dim\n",
        "        self.output_size = head_dim * num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.num_key_value_heads = num_key_value_heads\n",
        "        self.causal = causal\n",
        "        self.qkv_proj = CastedLinear(self.hidden_size, (self.num_heads + 2 * self.num_key_value_heads) * self.head_dim, bias=False)\n",
        "        self.o_proj = CastedLinear(self.output_size, self.hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "        qkv = self.qkv_proj(hidden_states)\n",
        "        qkv = qkv.view(batch_size, seq_len, self.num_heads + 2 * self.num_key_value_heads, self.head_dim)\n",
        "        query = qkv[:, :, :self.num_heads]\n",
        "        key = qkv[:, :, self.num_heads: self.num_heads + self.num_key_value_heads]\n",
        "        value = qkv[:, :, self.num_heads + self.num_key_value_heads:]\n",
        "        if cos_sin is not None:\n",
        "            cos, sin = cos_sin\n",
        "            query, key = apply_rotary_pos_emb(query, key, cos, sin)\n",
        "        query, key, value = map(lambda t: einops.rearrange(t, 'B S H D -> B H S D'), (query, key, value))\n",
        "        attn_output = scaled_dot_product_attention(query=query, key=key, value=value, is_causal=self.causal)\n",
        "        attn_output = einops.rearrange(attn_output, 'B H S D -> B S H D')\n",
        "        attn_output = attn_output.view(batch_size, seq_len, self.output_size)\n",
        "        return self.o_proj(attn_output)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, hidden_size: int, expansion: float):\n",
        "        super().__init__()\n",
        "        inter = _find_multiple(round(expansion * hidden_size * 2 / 3), 256)\n",
        "        self.gate_up_proj = CastedLinear(hidden_size, inter * 2, bias=False)\n",
        "        self.down_proj = CastedLinear(inter, hidden_size, bias=False)\n",
        "    def forward(self, x):\n",
        "        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
        "        return self.down_proj(F.silu(gate) * up)\n",
        "\n",
        "print(\"âœ… Layers loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Sparse Embedding (models/sparse_embedding.py) - Exact copy\n",
        "# ============================================================================\n",
        "class CastedSparseEmbedding(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, batch_size: int, init_std: float, cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "        self.weights = nn.Buffer(trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std), persistent=True)\n",
        "        self.local_weights = nn.Buffer(torch.zeros(batch_size, embedding_dim, requires_grad=True), persistent=False)\n",
        "        self.local_ids = nn.Buffer(torch.zeros(batch_size, dtype=torch.int32), persistent=False)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training:\n",
        "            return self.weights[inputs].to(self.cast_to)\n",
        "        with torch.no_grad():\n",
        "            self.local_weights.copy_(self.weights[inputs])\n",
        "            self.local_ids.copy_(inputs)\n",
        "        return self.local_weights.to(self.cast_to)\n",
        "\n",
        "class CastedSparseEmbeddingSignSGD(Optimizer):\n",
        "    \"\"\"Single GPU version of CastedSparseEmbeddingSignSGD_Distributed\"\"\"\n",
        "    def __init__(self, params: ParamsT, lr: Union[float, torch.Tensor] = 1e-3, weight_decay: float = 1e-2):\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            local_weights_grad, local_ids, weights = None, None, None\n",
        "            assert len(group[\"params\"]) == 3\n",
        "            for p in group[\"params\"]:\n",
        "                if p.requires_grad: local_weights_grad = p.grad\n",
        "                elif p.ndim == 1: local_ids = p\n",
        "                elif p.ndim == 2: weights = p\n",
        "            if local_weights_grad is None or local_ids is None or weights is None: continue\n",
        "            # SignSGD update\n",
        "            N, D = local_weights_grad.shape\n",
        "            grad_ids, inv = local_ids.unique(return_inverse=True)\n",
        "            grad = torch.zeros((grad_ids.shape[0], D), dtype=local_weights_grad.dtype, device=local_weights_grad.device)\n",
        "            grad.scatter_add_(0, inv.unsqueeze(-1).expand(-1, D), local_weights_grad)\n",
        "            p = weights[grad_ids]\n",
        "            p.mul_(1.0 - group[\"lr\"] * group[\"weight_decay\"]).add_(torch.sign(grad), alpha=-group[\"lr\"])\n",
        "            weights[grad_ids] = p\n",
        "\n",
        "print(\"âœ… Sparse embedding loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EMA Helper (models/ema.py) - Fixed for torch.compile compatibility\n",
        "# ============================================================================\n",
        "class EMAHelper(object):\n",
        "    def __init__(self, mu=0.999):\n",
        "        self.mu = mu\n",
        "        self.shadow = {}\n",
        "\n",
        "    def _unwrap_module(self, module):\n",
        "        \"\"\"Unwrap DataParallel or compiled module\"\"\"\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        # Handle torch.compile wrapped modules\n",
        "        if hasattr(module, '_orig_mod'):\n",
        "            return module._orig_mod\n",
        "        return module\n",
        "\n",
        "    def register(self, module):\n",
        "        module = self._unwrap_module(module)\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self, module):\n",
        "        module = self._unwrap_module(module)\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name].data = (1. - self.mu) * param.data + self.mu * self.shadow[name].data\n",
        "\n",
        "    def ema(self, module):\n",
        "        module = self._unwrap_module(module)\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data.copy_(self.shadow[name].data)\n",
        "\n",
        "    def ema_copy(self, module):\n",
        "        \"\"\"Create a copy with EMA weights - handles compiled models\"\"\"\n",
        "        # For compiled models, we need special handling\n",
        "        try:\n",
        "            module_copy = copy.deepcopy(module)\n",
        "        except Exception as e:\n",
        "            # If deepcopy fails (e.g., with compiled models), try alternative approach\n",
        "            print(f\"âš ï¸ deepcopy failed, using state_dict approach: {e}\")\n",
        "            # Get the underlying module\n",
        "            orig_module = self._unwrap_module(module)\n",
        "            module_copy = copy.deepcopy(orig_module)\n",
        "            # Re-compile if the original was compiled\n",
        "            if hasattr(module, '_orig_mod') and \"DISABLE_COMPILE\" not in os.environ:\n",
        "                module_copy = torch.compile(module_copy)\n",
        "        \n",
        "        self.ema(module_copy)\n",
        "        return module_copy\n",
        "\n",
        "    def state_dict(self): return self.shadow\n",
        "    def load_state_dict(self, state_dict): self.shadow = state_dict\n",
        "\n",
        "print(\"âœ… EMA helper loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRM Model (models/recursive_reasoning/trm.py) - Exact copy\n",
        "# ============================================================================\n",
        "@dataclass\n",
        "class TinyRecursiveReasoningModel_ACTV1InnerCarry:\n",
        "    z_H: torch.Tensor\n",
        "    z_L: torch.Tensor\n",
        "\n",
        "@dataclass\n",
        "class TinyRecursiveReasoningModel_ACTV1Carry:\n",
        "    inner_carry: TinyRecursiveReasoningModel_ACTV1InnerCarry\n",
        "    steps: torch.Tensor\n",
        "    halted: torch.Tensor\n",
        "    current_data: Dict[str, torch.Tensor]\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1Config(BaseModel):\n",
        "    batch_size: int; seq_len: int; puzzle_emb_ndim: int = 0; num_puzzle_identifiers: int; vocab_size: int\n",
        "    H_cycles: int; L_cycles: int; H_layers: int; L_layers: int\n",
        "    hidden_size: int; expansion: float; num_heads: int; pos_encodings: str\n",
        "    rms_norm_eps: float = 1e-5; rope_theta: float = 10000.0\n",
        "    halt_max_steps: int; halt_exploration_prob: float; forward_dtype: str = \"bfloat16\"\n",
        "    mlp_t: bool = False; puzzle_emb_len: int = 16; no_ACT_continue: bool = True\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1Block(nn.Module):\n",
        "    def __init__(self, config: TinyRecursiveReasoningModel_ACTV1Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        if self.config.mlp_t:\n",
        "            self.puzzle_emb_len = -(self.config.puzzle_emb_ndim // -self.config.hidden_size) if self.config.puzzle_emb_len == 0 else self.config.puzzle_emb_len\n",
        "            self.mlp_t = SwiGLU(hidden_size=self.config.seq_len + self.puzzle_emb_len, expansion=config.expansion)\n",
        "        else:\n",
        "            self.self_attn = Attention(hidden_size=config.hidden_size, head_dim=config.hidden_size // config.num_heads,\n",
        "                                       num_heads=config.num_heads, num_key_value_heads=config.num_heads, causal=False)\n",
        "        self.mlp = SwiGLU(hidden_size=config.hidden_size, expansion=config.expansion)\n",
        "        self.norm_eps = config.rms_norm_eps\n",
        "\n",
        "    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        if self.config.mlp_t:\n",
        "            hidden_states = hidden_states.transpose(1, 2)\n",
        "            out = self.mlp_t(hidden_states)\n",
        "            hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "            hidden_states = hidden_states.transpose(1, 2)\n",
        "        else:\n",
        "            hidden_states = rms_norm(hidden_states + self.self_attn(cos_sin=cos_sin, hidden_states=hidden_states), variance_epsilon=self.norm_eps)\n",
        "        out = self.mlp(hidden_states)\n",
        "        hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "        return hidden_states\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1ReasoningModule(nn.Module):\n",
        "    def __init__(self, layers: List[TinyRecursiveReasoningModel_ACTV1Block]):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "    def forward(self, hidden_states: torch.Tensor, input_injection: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "        hidden_states = hidden_states + input_injection\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states=hidden_states, **kwargs)\n",
        "        return hidden_states\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1_Inner(nn.Module):\n",
        "    def __init__(self, config: TinyRecursiveReasoningModel_ACTV1Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.forward_dtype = getattr(torch, self.config.forward_dtype)\n",
        "        self.embed_scale = math.sqrt(self.config.hidden_size)\n",
        "        embed_init_std = 1.0 / self.embed_scale\n",
        "        self.embed_tokens = CastedEmbedding(self.config.vocab_size, self.config.hidden_size, init_std=embed_init_std, cast_to=self.forward_dtype)\n",
        "        self.lm_head = CastedLinear(self.config.hidden_size, self.config.vocab_size, bias=False)\n",
        "        self.q_head = CastedLinear(self.config.hidden_size, 2, bias=True)\n",
        "        self.puzzle_emb_len = -(self.config.puzzle_emb_ndim // -self.config.hidden_size) if self.config.puzzle_emb_len == 0 else self.config.puzzle_emb_len\n",
        "        if self.config.puzzle_emb_ndim > 0:\n",
        "            self.puzzle_emb = CastedSparseEmbedding(self.config.num_puzzle_identifiers, self.config.puzzle_emb_ndim,\n",
        "                                                    batch_size=self.config.batch_size, init_std=0, cast_to=self.forward_dtype)\n",
        "        if self.config.pos_encodings == \"rope\":\n",
        "            self.rotary_emb = RotaryEmbedding(dim=self.config.hidden_size // self.config.num_heads,\n",
        "                                              max_position_embeddings=self.config.seq_len + self.puzzle_emb_len, base=self.config.rope_theta)\n",
        "        elif self.config.pos_encodings == \"learned\":\n",
        "            self.embed_pos = CastedEmbedding(self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, init_std=embed_init_std, cast_to=self.forward_dtype)\n",
        "        self.L_level = TinyRecursiveReasoningModel_ACTV1ReasoningModule(\n",
        "            layers=[TinyRecursiveReasoningModel_ACTV1Block(self.config) for _ in range(self.config.L_layers)])\n",
        "        self.H_init = nn.Buffer(trunc_normal_init_(torch.empty(self.config.hidden_size, dtype=self.forward_dtype), std=1), persistent=True)\n",
        "        self.L_init = nn.Buffer(trunc_normal_init_(torch.empty(self.config.hidden_size, dtype=self.forward_dtype), std=1), persistent=True)\n",
        "        with torch.no_grad():\n",
        "            self.q_head.weight.zero_()\n",
        "            self.q_head.bias.fill_(-5)\n",
        "\n",
        "    def _input_embeddings(self, input: torch.Tensor, puzzle_identifiers: torch.Tensor):\n",
        "        embedding = self.embed_tokens(input.to(torch.int32))\n",
        "        if self.config.puzzle_emb_ndim > 0:\n",
        "            puzzle_embedding = self.puzzle_emb(puzzle_identifiers)\n",
        "            pad_count = self.puzzle_emb_len * self.config.hidden_size - puzzle_embedding.shape[-1]\n",
        "            if pad_count > 0:\n",
        "                puzzle_embedding = F.pad(puzzle_embedding, (0, pad_count))\n",
        "            embedding = torch.cat((puzzle_embedding.view(-1, self.puzzle_emb_len, self.config.hidden_size), embedding), dim=-2)\n",
        "        if self.config.pos_encodings == \"learned\":\n",
        "            embedding = 0.707106781 * (embedding + self.embed_pos.embedding_weight.to(self.forward_dtype))\n",
        "        return self.embed_scale * embedding\n",
        "\n",
        "    def empty_carry(self, batch_size: int):\n",
        "        return TinyRecursiveReasoningModel_ACTV1InnerCarry(\n",
        "            z_H=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype),\n",
        "            z_L=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype))\n",
        "\n",
        "    def reset_carry(self, reset_flag: torch.Tensor, carry: TinyRecursiveReasoningModel_ACTV1InnerCarry):\n",
        "        return TinyRecursiveReasoningModel_ACTV1InnerCarry(\n",
        "            z_H=torch.where(reset_flag.view(-1, 1, 1), self.H_init, carry.z_H),\n",
        "            z_L=torch.where(reset_flag.view(-1, 1, 1), self.L_init, carry.z_L))\n",
        "\n",
        "    def forward(self, carry: TinyRecursiveReasoningModel_ACTV1InnerCarry, batch: Dict[str, torch.Tensor]):\n",
        "        seq_info = dict(cos_sin=self.rotary_emb() if hasattr(self, \"rotary_emb\") else None)\n",
        "        input_embeddings = self._input_embeddings(batch[\"inputs\"], batch[\"puzzle_identifiers\"])\n",
        "        z_H, z_L = carry.z_H, carry.z_L\n",
        "        with torch.no_grad():\n",
        "            for _H_step in range(self.config.H_cycles - 1):\n",
        "                for _L_step in range(self.config.L_cycles):\n",
        "                    z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "                z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "        for _L_step in range(self.config.L_cycles):\n",
        "            z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "        z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "        new_carry = TinyRecursiveReasoningModel_ACTV1InnerCarry(z_H=z_H.detach(), z_L=z_L.detach())\n",
        "        output = self.lm_head(z_H)[:, self.puzzle_emb_len:]\n",
        "        q_logits = self.q_head(z_H[:, 0]).to(torch.float32)\n",
        "        return new_carry, output, (q_logits[..., 0], q_logits[..., 1])\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1(nn.Module):\n",
        "    def __init__(self, config_dict: dict):\n",
        "        super().__init__()\n",
        "        self.config = TinyRecursiveReasoningModel_ACTV1Config(**config_dict)\n",
        "        self.inner = TinyRecursiveReasoningModel_ACTV1_Inner(self.config)\n",
        "\n",
        "    @property\n",
        "    def puzzle_emb(self): return self.inner.puzzle_emb\n",
        "\n",
        "    def initial_carry(self, batch: Dict[str, torch.Tensor]):\n",
        "        batch_size = batch[\"inputs\"].shape[0]\n",
        "        return TinyRecursiveReasoningModel_ACTV1Carry(\n",
        "            inner_carry=self.inner.empty_carry(batch_size),\n",
        "            steps=torch.zeros((batch_size,), dtype=torch.int32),\n",
        "            halted=torch.ones((batch_size,), dtype=torch.bool),\n",
        "            current_data={k: torch.empty_like(v) for k, v in batch.items()})\n",
        "\n",
        "    def forward(self, carry: TinyRecursiveReasoningModel_ACTV1Carry, batch: Dict[str, torch.Tensor]):\n",
        "        new_inner_carry = self.inner.reset_carry(carry.halted, carry.inner_carry)\n",
        "        new_steps = torch.where(carry.halted, 0, carry.steps)\n",
        "        new_current_data = {k: torch.where(carry.halted.view((-1,) + (1,) * (batch[k].ndim - 1)), batch[k], v) for k, v in carry.current_data.items()}\n",
        "        new_inner_carry, logits, (q_halt_logits, q_continue_logits) = self.inner(new_inner_carry, new_current_data)\n",
        "        outputs = {\"logits\": logits, \"q_halt_logits\": q_halt_logits, \"q_continue_logits\": q_continue_logits}\n",
        "        with torch.no_grad():\n",
        "            new_steps = new_steps + 1\n",
        "            is_last_step = new_steps >= self.config.halt_max_steps\n",
        "            halted = is_last_step\n",
        "            if self.training and (self.config.halt_max_steps > 1):\n",
        "                if self.config.no_ACT_continue:\n",
        "                    halted = halted | (q_halt_logits > 0)\n",
        "                else:\n",
        "                    halted = halted | (q_halt_logits > q_continue_logits)\n",
        "                min_halt_steps = (torch.rand_like(q_halt_logits) < self.config.halt_exploration_prob) * torch.randint_like(new_steps, low=2, high=self.config.halt_max_steps + 1)\n",
        "                halted = halted & (new_steps >= min_halt_steps)\n",
        "                if not self.config.no_ACT_continue:\n",
        "                    _, _, (next_q_halt, next_q_cont) = self.inner(new_inner_carry, new_current_data)\n",
        "                    outputs[\"target_q_continue\"] = torch.sigmoid(torch.where(is_last_step, next_q_halt, torch.maximum(next_q_halt, next_q_cont)))\n",
        "        return TinyRecursiveReasoningModel_ACTV1Carry(new_inner_carry, new_steps, halted, new_current_data), outputs\n",
        "\n",
        "print(\"âœ… TRM Model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Loss Functions (models/losses.py) - Exact copy\n",
        "# ============================================================================\n",
        "def s(x, epsilon=1e-30):\n",
        "    return torch.where(x < 0, 1 / (1 - x + epsilon), x + 1)\n",
        "\n",
        "def log_stablemax(x, dim=-1):\n",
        "    s_x = s(x)\n",
        "    return torch.log(s_x / torch.sum(s_x, dim=dim, keepdim=True))\n",
        "\n",
        "def stablemax_cross_entropy(logits, labels, ignore_index: int = -100, valid_mask=None):\n",
        "    logprobs = log_stablemax(logits.to(torch.float64), dim=-1)\n",
        "    if valid_mask is None:\n",
        "        valid_mask = (labels != ignore_index)\n",
        "    transformed_labels = torch.where(valid_mask, labels, 0)\n",
        "    prediction_logprobs = torch.gather(logprobs, index=transformed_labels.to(torch.long).unsqueeze(-1), dim=-1).squeeze(-1)\n",
        "    return -torch.where(valid_mask, prediction_logprobs, 0)\n",
        "\n",
        "class ACTLossHead(nn.Module):\n",
        "    def __init__(self, model: nn.Module, loss_type: str):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.loss_fn = stablemax_cross_entropy  # globals()[loss_type]\n",
        "\n",
        "    def initial_carry(self, *args, **kwargs):\n",
        "        return self.model.initial_carry(*args, **kwargs)\n",
        "\n",
        "    def forward(self, return_keys: Sequence[str], **model_kwargs):\n",
        "        new_carry, outputs = self.model(**model_kwargs)\n",
        "        labels = new_carry.current_data[\"labels\"]\n",
        "        with torch.no_grad():\n",
        "            outputs[\"preds\"] = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "            mask = (labels != IGNORE_LABEL_ID)\n",
        "            loss_counts = mask.sum(-1)\n",
        "            loss_divisor = loss_counts.clamp_min(1).unsqueeze(-1)\n",
        "            is_correct = mask & (torch.argmax(outputs[\"logits\"], dim=-1) == labels)\n",
        "            seq_is_correct = is_correct.sum(-1) == loss_counts\n",
        "            valid_metrics = new_carry.halted & (loss_counts > 0)\n",
        "            metrics = {\n",
        "                \"count\": valid_metrics.sum(),\n",
        "                \"accuracy\": torch.where(valid_metrics, (is_correct.to(torch.float32) / loss_divisor).sum(-1), 0).sum(),\n",
        "                \"exact_accuracy\": (valid_metrics & seq_is_correct).sum(),\n",
        "                \"q_halt_accuracy\": (valid_metrics & ((outputs[\"q_halt_logits\"] >= 0) == seq_is_correct)).sum(),\n",
        "                \"steps\": torch.where(valid_metrics, new_carry.steps, 0).sum(),\n",
        "            }\n",
        "        lm_loss = (self.loss_fn(outputs[\"logits\"], labels, ignore_index=IGNORE_LABEL_ID, valid_mask=mask) / loss_divisor).sum()\n",
        "        q_halt_loss = F.binary_cross_entropy_with_logits(outputs[\"q_halt_logits\"], seq_is_correct.to(outputs[\"q_halt_logits\"].dtype), reduction=\"sum\")\n",
        "        metrics.update({\"lm_loss\": lm_loss.detach(), \"q_halt_loss\": q_halt_loss.detach()})\n",
        "        q_continue_loss = 0\n",
        "        if \"target_q_continue\" in outputs:\n",
        "            q_continue_loss = F.binary_cross_entropy_with_logits(outputs[\"q_continue_logits\"], outputs[\"target_q_continue\"], reduction=\"sum\")\n",
        "            metrics[\"q_continue_loss\"] = q_continue_loss.detach()\n",
        "        detached_outputs = {k: outputs[k].detach() for k in return_keys if k in outputs}\n",
        "        return new_carry, lm_loss + 0.5 * (q_halt_loss + q_continue_loss), metrics, detached_outputs, new_carry.halted.all()\n",
        "\n",
        "print(\"âœ… Loss functions loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Dataset (puzzle_dataset.py) - Exact copy\n",
        "# ============================================================================\n",
        "class PuzzleDatasetMetadata(pydantic.BaseModel):\n",
        "    pad_id: int; ignore_label_id: Optional[int]; blank_identifier_id: int; vocab_size: int; seq_len: int\n",
        "    num_puzzle_identifiers: int; total_groups: int; mean_puzzle_examples: float; total_puzzles: int; sets: List[str]\n",
        "\n",
        "class PuzzleDatasetConfig(pydantic.BaseModel):\n",
        "    seed: int; dataset_paths: List[str]; global_batch_size: int; test_set_mode: bool; epochs_per_iter: int\n",
        "    rank: int = 0; num_replicas: int = 1; shuffle_eval: bool = False; eval_seed: Optional[int] = None\n",
        "\n",
        "def _sample_batch(rng, group_order, puzzle_indices, group_indices, start_index, global_batch_size):\n",
        "    batch, batch_puzzle_indices, current_size = [], [], 0\n",
        "    while (start_index < group_order.size) and (current_size < global_batch_size):\n",
        "        group_id = group_order[start_index]\n",
        "        puzzle_id = rng.integers(group_indices[group_id], group_indices[group_id + 1])\n",
        "        start_index += 1\n",
        "        puzzle_start = puzzle_indices[puzzle_id]\n",
        "        puzzle_size = int(puzzle_indices[puzzle_id + 1] - puzzle_start)\n",
        "        append_size = min(puzzle_size, global_batch_size - current_size)\n",
        "        batch_puzzle_indices.append(np.full(append_size, puzzle_id, dtype=np.int32))\n",
        "        batch.append(puzzle_start + rng.choice(puzzle_size, append_size, replace=False))\n",
        "        current_size += append_size\n",
        "    return start_index, np.concatenate(batch), np.concatenate(batch_puzzle_indices)\n",
        "\n",
        "class PuzzleDataset(IterableDataset):\n",
        "    def __init__(self, config: PuzzleDatasetConfig, split: str = \"train\"):\n",
        "        super().__init__()\n",
        "        self.config, self.split = config, split\n",
        "        self._load_metadata()\n",
        "        self.local_batch_size = config.global_batch_size // config.num_replicas\n",
        "        self._data, self._iters = None, 0\n",
        "\n",
        "    def _load_metadata(self):\n",
        "        prev_seq_len, mean_puzzle_examples, total_puzzles, total_groups = None, 0, 0, 0\n",
        "        for dataset_path in self.config.dataset_paths:\n",
        "            with open(os.path.join(dataset_path, self.split, \"dataset.json\"), \"r\") as f:\n",
        "                current = PuzzleDatasetMetadata(**json.load(f))\n",
        "            if prev_seq_len is None: self._base = current; prev_seq_len = current.seq_len\n",
        "            mean_puzzle_examples += current.mean_puzzle_examples * current.total_puzzles\n",
        "            total_puzzles += current.total_puzzles; total_groups += current.total_groups\n",
        "        self.metadata = PuzzleDatasetMetadata(seq_len=self._base.seq_len, vocab_size=self._base.vocab_size, pad_id=self._base.pad_id,\n",
        "            ignore_label_id=self._base.ignore_label_id, blank_identifier_id=self._base.blank_identifier_id,\n",
        "            num_puzzle_identifiers=self._base.num_puzzle_identifiers, total_groups=total_groups,\n",
        "            mean_puzzle_examples=mean_puzzle_examples / total_puzzles if total_puzzles else 0, total_puzzles=total_puzzles, sets=self._base.sets)\n",
        "\n",
        "    def _lazy_load(self):\n",
        "        if self._data: return\n",
        "        self._data = {}\n",
        "        for set_name in self.metadata.sets:\n",
        "            for i, dp in enumerate(self.config.dataset_paths):\n",
        "                sn = set_name + str(i) if i else set_name\n",
        "                self._data[sn] = {fn: np.load(os.path.join(dp, self.split, f\"{set_name}__{fn}.npy\"), mmap_mode=\"r\" if fn in [\"inputs\",\"labels\"] else None)\n",
        "                    for fn in [\"inputs\",\"labels\",\"puzzle_identifiers\",\"puzzle_indices\",\"group_indices\"]}\n",
        "\n",
        "    def _collate(self, batch):\n",
        "        batch = {k: v.astype(np.int32) for k, v in batch.items()}\n",
        "        if self.metadata.ignore_label_id is not None:\n",
        "            batch[\"labels\"][batch[\"labels\"] == self.metadata.ignore_label_id] = IGNORE_LABEL_ID\n",
        "        if batch[\"puzzle_identifiers\"].size < self.local_batch_size:\n",
        "            pad_size = self.local_batch_size - batch[\"puzzle_identifiers\"].size\n",
        "            pad_values = {\"inputs\": self.metadata.pad_id, \"labels\": IGNORE_LABEL_ID, \"puzzle_identifiers\": self.metadata.blank_identifier_id}\n",
        "            batch = {k: np.pad(v, ((0, pad_size),) + ((0, 0),) * (v.ndim - 1), constant_values=pad_values[k]) for k, v in batch.items()}\n",
        "        return {k: torch.from_numpy(v) for k, v in batch.items()}\n",
        "\n",
        "    def _iter_test(self):\n",
        "        for set_name, dataset in self._data.items():\n",
        "            total_examples = len(dataset[\"inputs\"])\n",
        "            for start_index in range(0, total_examples, self.config.global_batch_size):\n",
        "                end_index = min(total_examples, start_index + self.config.global_batch_size)\n",
        "                local_start, local_end = start_index, min(start_index + self.local_batch_size, end_index)\n",
        "                puzzle_indices = []; puzzle_index = np.searchsorted(dataset[\"puzzle_indices\"], local_start, side=\"right\") - 1\n",
        "                for i in range(local_start, local_end):\n",
        "                    while puzzle_index + 1 < len(dataset[\"puzzle_indices\"]) and i >= dataset[\"puzzle_indices\"][puzzle_index + 1]: puzzle_index += 1\n",
        "                    puzzle_indices.append(puzzle_index)\n",
        "                yield set_name, self._collate({\"inputs\": dataset[\"inputs\"][local_start:local_end], \"labels\": dataset[\"labels\"][local_start:local_end],\n",
        "                    \"puzzle_identifiers\": dataset[\"puzzle_identifiers\"][puzzle_indices]}), end_index - start_index\n",
        "\n",
        "    def _iter_train(self):\n",
        "        for set_name, dataset in self._data.items():\n",
        "            self._iters += 1\n",
        "            rng = np.random.Generator(np.random.Philox(seed=self.config.seed + self._iters))\n",
        "            group_order = np.concatenate([rng.permutation(dataset[\"group_indices\"].size - 1) for _ in range(self.config.epochs_per_iter)])\n",
        "            start_index = 0\n",
        "            while start_index < group_order.size:\n",
        "                start_index, batch_indices, batch_puzzle_indices = _sample_batch(\n",
        "                    rng, group_order, dataset[\"puzzle_indices\"], dataset[\"group_indices\"], start_index, self.config.global_batch_size)\n",
        "                if batch_puzzle_indices.size < self.config.global_batch_size: break\n",
        "                yield set_name, self._collate({\"inputs\": dataset[\"inputs\"][batch_indices[:self.local_batch_size]],\n",
        "                    \"labels\": dataset[\"labels\"][batch_indices[:self.local_batch_size]],\n",
        "                    \"puzzle_identifiers\": dataset[\"puzzle_identifiers\"][batch_puzzle_indices[:self.local_batch_size]]}), batch_puzzle_indices.size\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._lazy_load()\n",
        "        yield from (self._iter_test() if self.config.test_set_mode else self._iter_train())\n",
        "\n",
        "print(\"âœ… Dataset loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Efficiency Metrics (NEW) - Fixed v2\n",
        "# ============================================================================\n",
        "import gc\n",
        "\n",
        "def get_gpu_memory_gb():\n",
        "    return torch.cuda.memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0.0\n",
        "\n",
        "def get_peak_memory_gb():\n",
        "    return torch.cuda.max_memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0.0\n",
        "\n",
        "def reset_peak_memory():\n",
        "    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "def clear_cuda_cache():\n",
        "    \"\"\"Clear CUDA cache to prevent OOM between experiments\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    gc.collect()\n",
        "\n",
        "def estimate_flops(batch_size, seq_len, hidden_size, num_heads, expansion, H_cycles, L_cycles, L_layers, puzzle_emb_len=16):\n",
        "    \"\"\"Estimate FLOPs per forward pass (single ACT step)\"\"\"\n",
        "    L = seq_len + puzzle_emb_len\n",
        "    D = hidden_size\n",
        "    # Attention: QKV + scores + output\n",
        "    attn_flops = batch_size * L * (3 * D * D + D * L + D * D)\n",
        "    # MLP: SwiGLU\n",
        "    inter = (-(round(expansion * D * 2 / 3) // -256)) * 256\n",
        "    mlp_flops = batch_size * L * (D * inter * 2 + inter * D)\n",
        "    layer_flops = attn_flops + mlp_flops\n",
        "    # Total: H_cycles * (L_cycles + 1) * L_layers\n",
        "    total_layer_calls = H_cycles * (L_cycles + 1) * L_layers\n",
        "    return total_layer_calls * layer_flops\n",
        "\n",
        "# Global sample counter - thread safe version\n",
        "class SampleCounter:\n",
        "    def __init__(self):\n",
        "        self._samples = 0\n",
        "    def reset(self):\n",
        "        self._samples = 0\n",
        "    def update(self, n):\n",
        "        self._samples += n\n",
        "        return self._samples\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self._samples\n",
        "\n",
        "_sample_counter = SampleCounter()\n",
        "def reset_samples(): _sample_counter.reset()\n",
        "def update_samples(n): return _sample_counter.update(n)\n",
        "def get_samples(): return _sample_counter.value\n",
        "\n",
        "print(\"âœ… Efficiency metrics loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Training Framework (pretrain.py) - Faithful reproduction\n",
        "# ============================================================================\n",
        "class PretrainConfig(pydantic.BaseModel):\n",
        "    arch: dict; data_paths: List[str]; data_paths_test: List[str] = []; evaluators: List[dict] = []\n",
        "    global_batch_size: int; epochs: int; lr: float; lr_min_ratio: float; lr_warmup_steps: int\n",
        "    weight_decay: float; beta1: float; beta2: float; puzzle_emb_lr: float; puzzle_emb_weight_decay: float\n",
        "    project_name: Optional[str] = None; run_name: Optional[str] = None; checkpoint_path: Optional[str] = None\n",
        "    seed: int = 0; checkpoint_every_eval: bool = False; eval_interval: Optional[int] = None\n",
        "    min_eval_interval: int = 0; eval_save_outputs: List[str] = []; max_eval_batches: Optional[int] = None\n",
        "    ema: bool = False; ema_rate: float = 0.999; freeze_weights: bool = False\n",
        "\n",
        "@dataclass\n",
        "class TrainState:\n",
        "    model: nn.Module; optimizers: Sequence; optimizer_lrs: Sequence; carry: Any; step: int; total_steps: int\n",
        "\n",
        "def create_dataloader(config, split, **kwargs):\n",
        "    dataset = PuzzleDataset(PuzzleDatasetConfig(seed=config.seed, \n",
        "        dataset_paths=config.data_paths_test if config.data_paths_test and split==\"test\" else config.data_paths, **kwargs), split)\n",
        "    return DataLoader(dataset, batch_size=None, num_workers=1, prefetch_factor=8, pin_memory=True, persistent_workers=True), dataset.metadata\n",
        "\n",
        "def create_model(config, train_metadata):\n",
        "    model_cfg = {**{k: v for k, v in config.arch.items() if k not in ['name', 'loss']},\n",
        "                 'batch_size': config.global_batch_size, 'vocab_size': train_metadata.vocab_size,\n",
        "                 'seq_len': train_metadata.seq_len, 'num_puzzle_identifiers': train_metadata.num_puzzle_identifiers, 'causal': False}\n",
        "    with torch.device(\"cuda\"):\n",
        "        model = TinyRecursiveReasoningModel_ACTV1(model_cfg)\n",
        "        print(model)\n",
        "        model = ACTLossHead(model, config.arch['loss'].get('loss_type', 'stablemax_cross_entropy'))\n",
        "        if \"DISABLE_COMPILE\" not in os.environ:\n",
        "            model = torch.compile(model)\n",
        "    # Optimizers (AdamW instead of AdamAtan2)\n",
        "    if config.arch.get('puzzle_emb_ndim', 0) == 0:\n",
        "        optimizers = [torch.optim.AdamW(model.parameters(), lr=0, weight_decay=config.weight_decay, betas=(config.beta1, config.beta2))]\n",
        "        optimizer_lrs = [config.lr]\n",
        "    elif config.freeze_weights:\n",
        "        optimizers = [CastedSparseEmbeddingSignSGD(model.model.puzzle_emb.buffers(), lr=0, weight_decay=config.puzzle_emb_weight_decay)]\n",
        "        optimizer_lrs = [config.puzzle_emb_lr]\n",
        "    else:\n",
        "        optimizers = [\n",
        "            CastedSparseEmbeddingSignSGD(model.model.puzzle_emb.buffers(), lr=0, weight_decay=config.puzzle_emb_weight_decay),\n",
        "            torch.optim.AdamW(model.parameters(), lr=0, weight_decay=config.weight_decay, betas=(config.beta1, config.beta2))]\n",
        "        optimizer_lrs = [config.puzzle_emb_lr, config.lr]\n",
        "    return model, optimizers, optimizer_lrs\n",
        "\n",
        "def cosine_schedule_with_warmup(current_step, base_lr, num_warmup_steps, num_training_steps, min_ratio=0.0, num_cycles=0.5):\n",
        "    \"\"\"Cosine learning rate schedule with warmup - matches TinyRecursiveModels implementation\"\"\"\n",
        "    if current_step < num_warmup_steps:\n",
        "        return base_lr * float(current_step) / float(max(1, num_warmup_steps))\n",
        "    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "    return base_lr * (min_ratio + max(0.0, (1 - min_ratio) * 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))))\n",
        "\n",
        "def train_batch(config, train_state, batch, global_batch_size, log_efficiency=False):\n",
        "    train_state.step += 1\n",
        "    if train_state.step > train_state.total_steps: return None\n",
        "    \n",
        "    if log_efficiency: start_time = time.time(); reset_peak_memory()\n",
        "    \n",
        "    batch = {k: v.cuda() for k, v in batch.items()}\n",
        "    if train_state.carry is None:\n",
        "        with torch.device(\"cuda\"): train_state.carry = train_state.model.initial_carry(batch)\n",
        "    \n",
        "    if log_efficiency: fwd_start = time.time()\n",
        "    train_state.carry, loss, metrics, _, _ = train_state.model(carry=train_state.carry, batch=batch, return_keys=[])\n",
        "    if log_efficiency: fwd_time = time.time() - fwd_start\n",
        "    \n",
        "    samples_now = update_samples(global_batch_size)\n",
        "    ((1 / global_batch_size) * loss).backward()\n",
        "    \n",
        "    lr_this_step = None\n",
        "    for optim, base_lr in zip(train_state.optimizers, train_state.optimizer_lrs):\n",
        "        lr_this_step = cosine_schedule_with_warmup(train_state.step, base_lr, config.lr_warmup_steps, train_state.total_steps, config.lr_min_ratio)\n",
        "        for pg in optim.param_groups: pg['lr'] = lr_this_step\n",
        "        optim.step(); optim.zero_grad()\n",
        "    \n",
        "    if len(metrics):\n",
        "        metric_keys = sorted(metrics.keys())\n",
        "        metric_values = torch.stack([metrics[k] for k in metric_keys]).cpu().numpy()\n",
        "        reduced_metrics = {k: metric_values[i] for i, k in enumerate(metric_keys)}\n",
        "        count = max(reduced_metrics[\"count\"], 1)\n",
        "        reduced_metrics = {f\"train/{k}\": v / (global_batch_size if k.endswith(\"loss\") else count) for k, v in reduced_metrics.items()}\n",
        "        reduced_metrics[\"train/lr\"] = lr_this_step\n",
        "        reduced_metrics[\"train/samples_seen\"] = samples_now\n",
        "        \n",
        "        if log_efficiency:\n",
        "            batch_time = time.time() - start_time\n",
        "            reduced_metrics[\"train/batch_time\"] = batch_time\n",
        "            reduced_metrics[\"train/forward_time\"] = fwd_time\n",
        "            reduced_metrics[\"train/samples_per_sec\"] = global_batch_size / batch_time\n",
        "            reduced_metrics[\"train/gpu_memory_gb\"] = get_gpu_memory_gb()\n",
        "            reduced_metrics[\"train/peak_memory_gb\"] = get_peak_memory_gb()\n",
        "        return reduced_metrics\n",
        "    return None\n",
        "\n",
        "def evaluate(config, train_state, eval_loader, eval_metadata, max_batches=None):\n",
        "    \"\"\"Evaluate model with proper memory management - limits to max_batches for efficiency\"\"\"\n",
        "    reduced_metrics = None\n",
        "    \n",
        "    # Print evaluation limit\n",
        "    if max_batches:\n",
        "        print(f\"  ðŸ“Š Evaluating on {max_batches} batches (limited for efficiency)\")\n",
        "    else:\n",
        "        print(f\"  ðŸ“Š Evaluating on full dataset\")\n",
        "    \n",
        "    with torch.inference_mode():\n",
        "        set_ids = {k: idx for idx, k in enumerate(eval_metadata.sets)}\n",
        "        metric_keys, metric_values = [], None\n",
        "        processed_batches = 0\n",
        "        \n",
        "        for set_name, batch, global_batch_size in eval_loader:\n",
        "            # Check limit before processing (ensures exactly max_batches are processed)\n",
        "            if max_batches and processed_batches >= max_batches:\n",
        "                break\n",
        "            \n",
        "            processed_batches += 1\n",
        "            \n",
        "            batch = {k: v.cuda() for k, v in batch.items()}\n",
        "            with torch.device(\"cuda\"): carry = train_state.model.initial_carry(batch)\n",
        "            \n",
        "            inference_steps = 0\n",
        "            while True:\n",
        "                carry, loss, metrics, preds, all_finish = train_state.model(carry=carry, batch=batch, return_keys=[])\n",
        "                inference_steps += 1\n",
        "                if all_finish: break\n",
        "            \n",
        "            if processed_batches % 10 == 0 or processed_batches == max_batches:\n",
        "                print(f\"  Batch {processed_batches}/{max_batches if max_batches else 'all'}: {inference_steps} ACT steps\")\n",
        "            \n",
        "            set_id = set_ids.get(set_name, 0)\n",
        "            \n",
        "            if metric_values is None:\n",
        "                metric_keys = sorted(metrics.keys())\n",
        "                metric_values = torch.zeros((len(set_ids), len(metrics)), dtype=torch.float32, device=\"cuda\")\n",
        "            metric_values[set_id] += torch.stack([metrics[k] for k in metric_keys])\n",
        "            \n",
        "            # Clean up to prevent memory buildup\n",
        "            del carry, loss, preds, batch, all_finish, metrics\n",
        "        \n",
        "        if metric_values is not None:\n",
        "            reduced_metrics = metric_values.cpu().numpy()\n",
        "            reduced_metrics = {set_name: {mk: reduced_metrics[si, mi] for mi, mk in enumerate(metric_keys)} for si, set_name in enumerate(set_ids)}\n",
        "            for set_name, m in reduced_metrics.items():\n",
        "                count = max(m.pop(\"count\", 1), 1)  # Ensure count >= 1 to avoid division by zero\n",
        "                reduced_metrics[set_name] = {k: v / count for k, v in m.items()}\n",
        "        \n",
        "        print(f\"  âœ… Evaluation complete: {processed_batches} batches processed\")\n",
        "    \n",
        "    return reduced_metrics\n",
        "\n",
        "print(\"âœ… Training framework loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Dataset Builder\n",
        "# ============================================================================\n",
        "def shuffle_sudoku(board, solution):\n",
        "    digit_map = np.pad(np.random.permutation(np.arange(1, 10)), (1, 0))\n",
        "    transpose_flag = np.random.rand() < 0.5\n",
        "    bands = np.random.permutation(3); row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
        "    stacks = np.random.permutation(3); col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
        "    mapping = np.array([row_perm[i // 9] * 9 + col_perm[i % 9] for i in range(81)])\n",
        "    def apply(x): return digit_map[(x.T if transpose_flag else x).flatten()[mapping].reshape(9, 9)]\n",
        "    return apply(board), apply(solution)\n",
        "\n",
        "def build_dataset(config_name, output_dir=None, force=False):\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    cfg = DATASET_CONFIGS[config_name]\n",
        "    output_dir = output_dir or f\"data/sudoku-{config_name}\"\n",
        "    if os.path.exists(os.path.join(output_dir, \"train\", \"dataset.json\")) and not force:\n",
        "        print(f\"âœ… Dataset exists: {output_dir}\"); return output_dir\n",
        "    if force and os.path.exists(output_dir): shutil.rmtree(output_dir)\n",
        "    print(f\"ðŸ“¦ Building: {config_name}\")\n",
        "    for sn in [\"train\", \"test\"]:\n",
        "        csv_path = hf_hub_download(\"sapientinc/sudoku-extreme\", f\"{sn}.csv\", repo_type=\"dataset\", token=HF_TOKEN)\n",
        "        inputs, labels = [], []\n",
        "        with open(csv_path, newline=\"\") as f:\n",
        "            reader = csv.reader(f); next(reader)\n",
        "            for _, q, a, _ in tqdm(reader, desc=f\"Reading {sn}\"):\n",
        "                inputs.append(np.frombuffer(q.replace('.', '0').encode(), np.uint8).reshape(9, 9) - ord('0'))\n",
        "                labels.append(np.frombuffer(a.encode(), np.uint8).reshape(9, 9) - ord('0'))\n",
        "        if sn == \"train\" and cfg[\"subsample\"] and cfg[\"subsample\"] < len(inputs):\n",
        "            # Set random seed for reproducibility (matches TinyRecursiveModels)\n",
        "            rng_subsample = np.random.Generator(np.random.Philox(seed=42))\n",
        "            idx = rng_subsample.choice(len(inputs), cfg[\"subsample\"], replace=False)\n",
        "            inputs, labels = [inputs[i] for i in idx], [labels[i] for i in idx]\n",
        "        num_aug = cfg[\"augmentation\"] if sn == \"train\" else 0\n",
        "        results = {k: [] for k in [\"inputs\", \"labels\", \"puzzle_identifiers\", \"puzzle_indices\", \"group_indices\"]}\n",
        "        puzzle_id, example_id = 0, 0\n",
        "        results[\"puzzle_indices\"].append(0); results[\"group_indices\"].append(0)\n",
        "        for orig_inp, orig_out in tqdm(zip(inputs, labels), total=len(inputs), desc=f\"Augmenting {sn}\"):\n",
        "            for aug_idx in range(1 + num_aug):\n",
        "                inp, out = (orig_inp, orig_out) if aug_idx == 0 else shuffle_sudoku(orig_inp, orig_out)\n",
        "                results[\"inputs\"].append(inp); results[\"labels\"].append(out); example_id += 1; puzzle_id += 1\n",
        "                results[\"puzzle_indices\"].append(example_id); results[\"puzzle_identifiers\"].append(0)\n",
        "            results[\"group_indices\"].append(puzzle_id)\n",
        "        def to_np(seq): return np.concatenate(seq).reshape(len(seq), -1) + 1\n",
        "        results = {\"inputs\": to_np(results[\"inputs\"]), \"labels\": to_np(results[\"labels\"]),\n",
        "                   \"group_indices\": np.array(results[\"group_indices\"], np.int32),\n",
        "                   \"puzzle_indices\": np.array(results[\"puzzle_indices\"], np.int32),\n",
        "                   \"puzzle_identifiers\": np.array(results[\"puzzle_identifiers\"], np.int32)}\n",
        "        # Calculate mean_puzzle_examples correctly: each puzzle has (1 + num_aug) examples\n",
        "        mean_puzzle_examples = 1.0 + num_aug if sn == \"train\" else 1.0\n",
        "        meta = PuzzleDatasetMetadata(seq_len=81, vocab_size=11, pad_id=0, ignore_label_id=0, blank_identifier_id=0,\n",
        "            num_puzzle_identifiers=1, total_groups=len(results[\"group_indices\"]) - 1, mean_puzzle_examples=mean_puzzle_examples,\n",
        "            total_puzzles=len(results[\"group_indices\"]) - 1, sets=[\"all\"])\n",
        "        save_dir = os.path.join(output_dir, sn); os.makedirs(save_dir, exist_ok=True)\n",
        "        with open(os.path.join(save_dir, \"dataset.json\"), \"w\") as f: json.dump(meta.model_dump(), f)\n",
        "        for k, v in results.items(): np.save(os.path.join(save_dir, f\"all__{k}.npy\"), v)\n",
        "        print(f\"  âœ… {sn}: {results['inputs'].shape[0]:,} examples\")\n",
        "    return output_dir\n",
        "\n",
        "print(\"âœ… Dataset builder loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Launch Function (with EMA support) - Fixed v2\n",
        "# ============================================================================\n",
        "import coolname\n",
        "\n",
        "def launch(config_name, batch_size=768, project_name=\"TRM-DataVS\", use_ema=True):\n",
        "    \"\"\"Launch training experiment with proper cleanup and error handling\"\"\"\n",
        "    \n",
        "    # Clear CUDA cache before starting\n",
        "    clear_cuda_cache()\n",
        "    \n",
        "    cfg_d = DATASET_CONFIGS[config_name]\n",
        "    data_dir = build_dataset(config_name, force=FORCE_REBUILD)\n",
        "    \n",
        "    config = PretrainConfig(\n",
        "        arch={'halt_exploration_prob': 0.1, 'halt_max_steps': 16, 'H_cycles': 3, 'L_cycles': 6,\n",
        "              'H_layers': 0, 'L_layers': 2, 'hidden_size': 512, 'num_heads': 8, 'expansion': 4,\n",
        "              'puzzle_emb_ndim': 512, 'pos_encodings': 'rope', 'forward_dtype': 'bfloat16',\n",
        "              'mlp_t': False, 'puzzle_emb_len': 16, 'no_ACT_continue': True,\n",
        "              'loss': {'loss_type': 'stablemax_cross_entropy'}},\n",
        "        data_paths=[data_dir], global_batch_size=batch_size, epochs=cfg_d['epochs'],\n",
        "        eval_interval=max(cfg_d['epochs'] // 10, 1), lr=1e-4, lr_min_ratio=1.0, lr_warmup_steps=2000,\n",
        "        weight_decay=1.0, beta1=0.9, beta2=0.95, puzzle_emb_lr=1e-4, puzzle_emb_weight_decay=1.0,\n",
        "        project_name=project_name, run_name=f\"sudoku-{config_name}\",\n",
        "        checkpoint_path=f\"checkpoints/{project_name}/sudoku-{config_name}\",\n",
        "        checkpoint_every_eval=True, max_eval_batches=MAX_EVAL_BATCHES, ema=use_ema, ema_rate=0.999)\n",
        "    \n",
        "    torch.random.manual_seed(config.seed)\n",
        "    train_epochs_per_iter = config.eval_interval or config.epochs\n",
        "    train_loader, train_meta = create_dataloader(config, \"train\", test_set_mode=False, epochs_per_iter=train_epochs_per_iter, global_batch_size=config.global_batch_size)\n",
        "    \n",
        "    try: \n",
        "        eval_loader, eval_meta = create_dataloader(config, \"test\", test_set_mode=True, epochs_per_iter=1, global_batch_size=config.global_batch_size)\n",
        "    except Exception as e: \n",
        "        print(f\"âš ï¸ No eval data found: {e}\")\n",
        "        eval_loader, eval_meta = None, None\n",
        "    \n",
        "    model, optimizers, optimizer_lrs = create_model(config, train_meta)\n",
        "    total_steps = int(config.epochs * train_meta.total_groups * train_meta.mean_puzzle_examples / config.global_batch_size)\n",
        "    train_state = TrainState(model=model, optimizers=optimizers, optimizer_lrs=optimizer_lrs, carry=None, step=0, total_steps=total_steps)\n",
        "    \n",
        "    # EMA setup - works with compiled models\n",
        "    ema_helper = None\n",
        "    if config.ema:\n",
        "        print('ðŸ”„ Setup EMA')\n",
        "        ema_helper = EMAHelper(mu=config.ema_rate)\n",
        "        ema_helper.register(train_state.model)\n",
        "    \n",
        "    pbar = tqdm(total=total_steps)\n",
        "    wandb.init(project=config.project_name, name=config.run_name, config=config.model_dump(), settings=wandb.Settings(_disable_stats=True))\n",
        "    \n",
        "    # Log initial metrics\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    flops_estimate = estimate_flops(config.global_batch_size, train_meta.seq_len, 512, 8, 4, 3, 6, 2)\n",
        "    wandb.log({\"num_params\": num_params, \"estimated_flops_per_forward\": flops_estimate}, step=0)\n",
        "    \n",
        "    reset_samples()\n",
        "    print(\"=\"*60)\n",
        "    print(f\"ðŸš€ {config.run_name}\")\n",
        "    print(f\"   Epochs: {config.epochs}, Steps: {total_steps:,}, Params: {num_params:,}\")\n",
        "    print(f\"   EMA: {config.ema}, FLOPs/fwd: {flops_estimate/1e9:.2f}G\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    try:\n",
        "        total_iters = config.epochs // train_epochs_per_iter\n",
        "        for iter_id in range(total_iters):\n",
        "            print(f\"\\nEpoch {iter_id * train_epochs_per_iter}\")\n",
        "            train_state.model.train()\n",
        "            for step_in_iter, (_, batch, gbs) in enumerate(train_loader):\n",
        "                log_eff = (train_state.step % 50 == 0)  # Log efficiency every 50 steps\n",
        "                metrics = train_batch(config, train_state, batch, gbs, log_efficiency=log_eff)\n",
        "                if metrics:\n",
        "                    wandb.log(metrics, step=train_state.step)\n",
        "                    pbar.update(train_state.step - pbar.n)\n",
        "                if config.ema:\n",
        "                    ema_helper.update(train_state.model)\n",
        "            \n",
        "            # Evaluation (with EMA if enabled)\n",
        "            if eval_loader and iter_id >= config.min_eval_interval:\n",
        "                print(\"EVALUATE\")\n",
        "                train_state.model.eval()\n",
        "                \n",
        "                if config.ema:\n",
        "                    print(\"SWITCH TO EMA\")\n",
        "                    # Create eval state without deepcopy (avoids issues with compiled models)\n",
        "                    train_state_eval = TrainState(\n",
        "                        model=ema_helper.ema_copy(train_state.model),\n",
        "                        optimizers=train_state.optimizers,\n",
        "                        optimizer_lrs=train_state.optimizer_lrs,\n",
        "                        carry=None,  # Fresh carry for eval\n",
        "                        step=train_state.step,\n",
        "                        total_steps=train_state.total_steps\n",
        "                    )\n",
        "                else:\n",
        "                    train_state_eval = train_state\n",
        "                \n",
        "                train_state_eval.model.eval()\n",
        "                metrics = evaluate(config, train_state_eval, eval_loader, eval_meta, config.max_eval_batches)\n",
        "                if metrics: wandb.log(metrics, step=train_state.step)\n",
        "                \n",
        "                if config.checkpoint_path and config.checkpoint_every_eval:\n",
        "                    os.makedirs(config.checkpoint_path, exist_ok=True)\n",
        "                    torch.save(train_state_eval.model.state_dict(), os.path.join(config.checkpoint_path, f\"step_{train_state.step}\"))\n",
        "                \n",
        "                if config.ema: \n",
        "                    del train_state_eval\n",
        "                    clear_cuda_cache()  # Clear cache after eval with EMA\n",
        "                    \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nâš ï¸ Training interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Training error: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        wandb.finish()\n",
        "        pbar.close()\n",
        "        final_samples = get_samples()\n",
        "        print(f\"\\nâœ… Done! Samples seen: {final_samples:,}\")\n",
        "        clear_cuda_cache()  # Final cleanup\n",
        "    \n",
        "    return train_state\n",
        "\n",
        "print(\"âœ… Launch function ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ” Verification - Test all components before training\n",
        "# ============================================================================\n",
        "def verify_setup():\n",
        "    \"\"\"Verify all components are working before starting experiments\"\"\"\n",
        "    print(\"ðŸ” Verifying setup...\")\n",
        "    errors = []\n",
        "    \n",
        "    # 1. Check CUDA\n",
        "    if not torch.cuda.is_available():\n",
        "        errors.append(\"âŒ CUDA not available\")\n",
        "    else:\n",
        "        print(f\"âœ… CUDA available: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    \n",
        "    # 2. Check imports\n",
        "    required = ['numpy', 'torch', 'einops', 'pydantic', 'wandb', 'tqdm']\n",
        "    for mod in required:\n",
        "        try:\n",
        "            __import__(mod)\n",
        "            print(f\"âœ… {mod} imported\")\n",
        "        except ImportError as e:\n",
        "            errors.append(f\"âŒ {mod} import failed: {e}\")\n",
        "    \n",
        "    # 3. Test model instantiation (small test)\n",
        "    try:\n",
        "        test_cfg = {\n",
        "            'batch_size': 2, 'seq_len': 81, 'vocab_size': 11,\n",
        "            'num_puzzle_identifiers': 1, 'H_cycles': 1, 'L_cycles': 1,\n",
        "            'H_layers': 0, 'L_layers': 1, 'hidden_size': 64, 'num_heads': 4,\n",
        "            'expansion': 2, 'pos_encodings': 'rope', 'halt_max_steps': 1,\n",
        "            'halt_exploration_prob': 0.0, 'puzzle_emb_ndim': 0\n",
        "        }\n",
        "        with torch.device(\"cuda\"):\n",
        "            test_model = TinyRecursiveReasoningModel_ACTV1(test_cfg)\n",
        "        del test_model\n",
        "        clear_cuda_cache()\n",
        "        print(\"âœ… Model instantiation OK\")\n",
        "    except Exception as e:\n",
        "        errors.append(f\"âŒ Model instantiation failed: {e}\")\n",
        "    \n",
        "    # 4. Test dataset config\n",
        "    try:\n",
        "        _ = PuzzleDatasetMetadata(\n",
        "            seq_len=81, vocab_size=11, pad_id=0, ignore_label_id=0,\n",
        "            blank_identifier_id=0, num_puzzle_identifiers=1,\n",
        "            total_groups=100, mean_puzzle_examples=1, total_puzzles=100, sets=[\"all\"]\n",
        "        )\n",
        "        print(\"âœ… Dataset metadata OK\")\n",
        "    except Exception as e:\n",
        "        errors.append(f\"âŒ Dataset metadata failed: {e}\")\n",
        "    \n",
        "    # 5. Test efficiency metrics\n",
        "    try:\n",
        "        reset_samples()\n",
        "        update_samples(100)\n",
        "        assert get_samples() == 100\n",
        "        reset_samples()\n",
        "        print(\"âœ… Sample counter OK\")\n",
        "    except Exception as e:\n",
        "        errors.append(f\"âŒ Sample counter failed: {e}\")\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    if errors:\n",
        "        print(\"âŒ VERIFICATION FAILED:\")\n",
        "        for err in errors:\n",
        "            print(f\"   {err}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"âœ… ALL CHECKS PASSED - Ready to train!\")\n",
        "        return True\n",
        "\n",
        "# Run verification\n",
        "verify_setup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3ï¸âƒ£ Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸš€ RUN EXPERIMENT\n",
        "# ============================================================================\n",
        "train_state = launch(\n",
        "    config_name=SELECTED_CONFIG,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    project_name=PROJECT_NAME,\n",
        "    use_ema=USE_EMA\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4ï¸âƒ£ Run All Experiments (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸš€ RUN ALL EXPERIMENTS (Uncomment to run)\n",
        "# ============================================================================\n",
        "# CONFIGS_TO_RUN = [\"1k-aug1000\", \"10k-aug100\", \"100k-aug10\", \"1M-aug0\"]\n",
        "\n",
        "# results = []\n",
        "# for cfg_name in CONFIGS_TO_RUN:\n",
        "#     print(f\"\\n{'='*60}\\nðŸš€ Starting: {cfg_name}\\n{'='*60}\")\n",
        "#     try:\n",
        "#         ts = launch(cfg_name, batch_size=BATCH_SIZE, project_name=PROJECT_NAME, use_ema=USE_EMA)\n",
        "#         results.append((cfg_name, \"âœ… SUCCESS\", ts))\n",
        "#     except Exception as e:\n",
        "#         print(f\"âŒ Failed: {cfg_name} - {e}\")\n",
        "#         results.append((cfg_name, f\"âŒ FAILED: {e}\", None))\n",
        "#     finally:\n",
        "#         # Clean up between experiments to prevent OOM\n",
        "#         clear_cuda_cache()\n",
        "#         print(f\"ðŸ§¹ Cache cleared after {cfg_name}\")\n",
        "\n",
        "# print(f\"\\n{'='*60}\\nðŸŽ‰ ALL EXPERIMENTS COMPLETE!\\n{'='*60}\")\n",
        "# for name, status, _ in results: \n",
        "#     print(f\"  {status}: {name}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
