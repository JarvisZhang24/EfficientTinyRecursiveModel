{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f2a3b8a8e66b4c098d0ecd8d5e2361e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbf41585d09c41cf9dd2dcd09bc0b365",
              "IPY_MODEL_be261338ab4548999b45a65ada4de549",
              "IPY_MODEL_6b0ffacbd02d4fe4bc3b7d9f7d7018db"
            ],
            "layout": "IPY_MODEL_bbb2d3f0f04b4ef58707867f15fec2c2"
          }
        },
        "dbf41585d09c41cf9dd2dcd09bc0b365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15979cd931304eeab50e9f6994bb706d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_10af8be617ee447986a39367c51b3135",
            "value": "train.csv:â€‡100%"
          }
        },
        "be261338ab4548999b45a65ada4de549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92d31993a36a45dfabefa7d90944cc7d",
            "max": 718819925,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74d7e0abf41b468e8b1c94650dc2f980",
            "value": 718819925
          }
        },
        "6b0ffacbd02d4fe4bc3b7d9f7d7018db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eac40bca9fd04b69b0341d41581cba35",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0fa8f32fd5d041d1bf0a19702871c0ed",
            "value": "â€‡719M/719Mâ€‡[00:01&lt;00:00,â€‡685MB/s]"
          }
        },
        "bbb2d3f0f04b4ef58707867f15fec2c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15979cd931304eeab50e9f6994bb706d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10af8be617ee447986a39367c51b3135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92d31993a36a45dfabefa7d90944cc7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74d7e0abf41b468e8b1c94650dc2f980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eac40bca9fd04b69b0341d41581cba35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fa8f32fd5d041d1bf0a19702871c0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d71de9eb53243848459a85b2339822d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6757f4f8229442aa963ac79a86efb42d",
              "IPY_MODEL_88f2ba72f0b14d4fb566de0fbedce393",
              "IPY_MODEL_73c2f6d7544a4e519a83abfb47bd898f"
            ],
            "layout": "IPY_MODEL_9a51a14f825a45f6899f8fec3eeaefea"
          }
        },
        "6757f4f8229442aa963ac79a86efb42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d556364debbd43d6aa295e8c10047178",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d660eb1086bf4b3999ab52733ac200b7",
            "value": "test.csv:â€‡100%"
          }
        },
        "88f2ba72f0b14d4fb566de0fbedce393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05c6e2be8ba544cb9fdf8cf20184c7b7",
            "max": 79360390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_117049c79d5f43c987fab67a3ef646fb",
            "value": 79360390
          }
        },
        "73c2f6d7544a4e519a83abfb47bd898f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a357ee71b9740e09359adecb45b4d99",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6f579f8b40944d78aee6a4919b25db3d",
            "value": "â€‡79.4M/79.4Mâ€‡[00:00&lt;00:00,â€‡130MB/s]"
          }
        },
        "9a51a14f825a45f6899f8fec3eeaefea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d556364debbd43d6aa295e8c10047178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d660eb1086bf4b3999ab52733ac200b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05c6e2be8ba544cb9fdf8cf20184c7b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "117049c79d5f43c987fab67a3ef646fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a357ee71b9740e09359adecb45b4d99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f579f8b40944d78aee6a4919b25db3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ENV Setup\n"
      ],
      "metadata": {
        "id": "0So5DPLRh7MF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anTe8pHxhqrs",
        "outputId": "cf2f66a7-fc44-486c-ba7d-8e0335fbe54e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Collecting adam-atan2\n",
            "  Downloading adam_atan2-0.0.3.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting coolname\n",
            "  Downloading coolname-2.2.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Collecting argdantic\n",
            "  Downloading argdantic-1.3.3-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Collecting hydra-core\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (25.0)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools-scm\n",
            "  Using cached setuptools_scm-9.2.2-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: pydantic-core in /usr/local/lib/python3.12/dist-packages (2.41.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (0.60.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: pydantic-settings<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from argdantic) (2.12.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.46.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.12/dist-packages (from numba) (2.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3,>=2.4.0->argdantic) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading coolname-2.2.0-py2.py3-none-any.whl (37 kB)\n",
            "Downloading argdantic-1.3.3-py2.py3-none-any.whl (26 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached setuptools_scm-9.2.2-py3-none-any.whl (62 kB)\n",
            "Building wheels for collected packages: adam-atan2\n",
            "  Building wheel for adam-atan2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adam-atan2: filename=adam_atan2-0.0.3-cp312-cp312-linux_x86_64.whl size=208277 sha256=73bb9413d9dfd4740818df2582587cd3868e426f9e6f3d60699adb570c612e8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/32/31/044d4e2d7dc14d2820c0232d9f41b3ae8cb3d8451451a80dbf\n",
            "Successfully built adam-atan2\n",
            "Installing collected packages: coolname, adam-atan2, setuptools-scm, ninja, hydra-core, argdantic\n",
            "Successfully installed adam-atan2-0.0.3 argdantic-1.3.3 coolname-2.2.0 hydra-core-1.3.2 ninja-1.13.0 setuptools-scm-9.2.2\n"
          ]
        }
      ],
      "source": [
        "! pip install torch adam-atan2 einops tqdm coolname pydantic argdantic wandb omegaconf hydra-core huggingface_hub packaging ninja wheel setuptools setuptools-scm pydantic-core numba triton"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weights & Biases"
      ],
      "metadata": {
        "id": "iZuH0YgAphvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "WANDB_key = userdata.get('WANDB')\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell: Wandb è‡ªåŠ¨ç™»å½•\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ğŸ“Š é…ç½® Weights & Biases\")\n",
        "print(\"=\" * 70)\n",
        "import wandb\n",
        "wandb.login(key=WANDB_key)  # login if you want the logger to sync results to your Weights & Biases (https://wandb.ai/)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwlX3Ly0isld",
        "outputId": "8a583f17-835c-4bcb-dad3-ad811c310b66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjarviszhang\u001b[0m (\u001b[33mjarviszhang-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸ“Š é…ç½® Weights & Biases\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Seed\n"
      ],
      "metadata": {
        "id": "k7X3xUYsknD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from einops import rearrange, repeat\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å¤ç°ç»“æœä¸€è‡´\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIP2Mg3DjJsV",
        "outputId": "ccf189bc-cd9c-4ccc-da97-3ab666cfae16"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Sudoku Dataset"
      ],
      "metadata": {
        "id": "IDD2YN1AjUEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab ç‹¬ç«‹ä»£ç  - ä¸‹è½½å¹¶æ„å»º Sudoku æ•°æ®é›†\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# å®‰è£…å¹¶å¯¼å…¥ huggingface_hub\n",
        "# !pip install huggingface_hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# ============ é…ç½® ============\n",
        "SOURCE_REPO = \"sapientinc/sudoku-extreme\"\n",
        "SUBSAMPLE_SIZE = 1000  # è®­ç»ƒé›†é‡‡æ ·æ•°é‡ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨\n",
        "NUM_AUG = 1         # æ•°æ®å¢å¼ºæ¬¡æ•°\n",
        "\n",
        "OUTPUT_DIR = f\"data/sudoku-extreme-${SUBSAMPLE_SIZE}-aug-${NUM_AUG}\"\n",
        "\n",
        "# ============ æ•°æ®å¢å¼ºå‡½æ•° ============\n",
        "def shuffle_sudoku(board: np.ndarray, solution: np.ndarray):\n",
        "    \"\"\"å¯¹æ•°ç‹¬è¿›è¡Œç­‰ä»·å˜æ¢ï¼ˆä¿æŒæœ‰æ•ˆæ€§ï¼‰\"\"\"\n",
        "    # æ•°å­—æ˜ å°„ï¼šéšæœºç½®æ¢ 1-9\n",
        "    digit_map = np.pad(np.random.permutation(np.arange(1, 10)), (1, 0))\n",
        "\n",
        "    # éšæœºè½¬ç½®\n",
        "    transpose_flag = np.random.rand() < 0.5\n",
        "\n",
        "    # è¡Œç½®æ¢ï¼šå…ˆæ‰“ä¹± 3 ä¸ª bandï¼Œå†æ‰“ä¹±æ¯ä¸ª band å†…çš„ 3 è¡Œ\n",
        "    bands = np.random.permutation(3)\n",
        "    row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
        "\n",
        "    # åˆ—ç½®æ¢ï¼šåŒç†\n",
        "    stacks = np.random.permutation(3)\n",
        "    col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
        "\n",
        "    # æ„å»º 81->81 çš„ä½ç½®æ˜ å°„\n",
        "    mapping = np.array([row_perm[i // 9] * 9 + col_perm[i % 9] for i in range(81)])\n",
        "\n",
        "    def apply_transformation(x: np.ndarray) -> np.ndarray:\n",
        "        if transpose_flag:\n",
        "            x = x.T\n",
        "        new_board = x.flatten()[mapping].reshape(9, 9).copy()\n",
        "        return digit_map[new_board]\n",
        "\n",
        "    return apply_transformation(board), apply_transformation(solution)\n",
        "\n",
        "# ============ å¤„ç†å•ä¸ªå­é›† ============\n",
        "def convert_subset(set_name: str):\n",
        "    print(f\"Processing {set_name} set...\")\n",
        "\n",
        "    # ä» HuggingFace ä¸‹è½½ CSV\n",
        "    csv_path = hf_hub_download(SOURCE_REPO, f\"{set_name}.csv\", repo_type=\"dataset\")\n",
        "\n",
        "    # è¯»å– CSV\n",
        "    inputs, labels = [], []\n",
        "    with open(csv_path, newline=\"\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)  # è·³è¿‡ header\n",
        "        for source, q, a, rating in reader:\n",
        "            inputs.append(np.frombuffer(q.replace('.', '0').encode(), dtype=np.uint8).reshape(9, 9) - ord('0'))\n",
        "            labels.append(np.frombuffer(a.encode(), dtype=np.uint8).reshape(9, 9) - ord('0'))\n",
        "\n",
        "    print(f\"  Loaded {len(inputs)} puzzles\")\n",
        "\n",
        "    # è®­ç»ƒé›†é‡‡æ ·\n",
        "    if set_name == \"train\" and SUBSAMPLE_SIZE is not None and SUBSAMPLE_SIZE < len(inputs):\n",
        "        indices = np.random.choice(len(inputs), size=SUBSAMPLE_SIZE, replace=False)\n",
        "        inputs = [inputs[i] for i in indices]\n",
        "        labels = [labels[i] for i in indices]\n",
        "        print(f\"  Subsampled to {len(inputs)} puzzles\")\n",
        "\n",
        "    # æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒé›†ï¼‰\n",
        "    num_augments = NUM_AUG if set_name == \"train\" else 0\n",
        "\n",
        "    # æ„å»ºç»“æœ\n",
        "    results = {k: [] for k in [\"inputs\", \"labels\", \"puzzle_identifiers\", \"puzzle_indices\", \"group_indices\"]}\n",
        "    puzzle_id = 0\n",
        "    example_id = 0\n",
        "\n",
        "    results[\"puzzle_indices\"].append(0)\n",
        "    results[\"group_indices\"].append(0)\n",
        "\n",
        "    for orig_inp, orig_out in tqdm(zip(inputs, labels), total=len(inputs)):\n",
        "        for aug_idx in range(1 + num_augments):\n",
        "            if aug_idx == 0:\n",
        "                inp, out = orig_inp, orig_out\n",
        "            else:\n",
        "                inp, out = shuffle_sudoku(orig_inp, orig_out)\n",
        "\n",
        "            results[\"inputs\"].append(inp)\n",
        "            results[\"labels\"].append(out)\n",
        "            example_id += 1\n",
        "            puzzle_id += 1\n",
        "\n",
        "            results[\"puzzle_indices\"].append(example_id)\n",
        "            results[\"puzzle_identifiers\"].append(0)\n",
        "\n",
        "        results[\"group_indices\"].append(puzzle_id)\n",
        "\n",
        "    # è½¬æ¢ä¸º NumPy æ•°ç»„\n",
        "    def seq_to_numpy(seq):\n",
        "        arr = np.concatenate(seq).reshape(len(seq), -1)\n",
        "        return arr + 1  # åç§» +1ï¼Œ0 ç•™ç»™ PAD\n",
        "\n",
        "    results = {\n",
        "        \"inputs\": seq_to_numpy(results[\"inputs\"]),\n",
        "        \"labels\": seq_to_numpy(results[\"labels\"]),\n",
        "        \"group_indices\": np.array(results[\"group_indices\"], dtype=np.int32),\n",
        "        \"puzzle_indices\": np.array(results[\"puzzle_indices\"], dtype=np.int32),\n",
        "        \"puzzle_identifiers\": np.array(results[\"puzzle_identifiers\"], dtype=np.int32),\n",
        "    }\n",
        "\n",
        "    # å…ƒæ•°æ®\n",
        "    metadata = {\n",
        "        \"seq_len\": 81,\n",
        "        \"vocab_size\": 11,  # PAD + 0-9\n",
        "        \"pad_id\": 0,\n",
        "        \"ignore_label_id\": 0,\n",
        "        \"blank_identifier_id\": 0,\n",
        "        \"num_puzzle_identifiers\": 1,\n",
        "        \"total_groups\": len(results[\"group_indices\"]) - 1,\n",
        "        \"mean_puzzle_examples\": 1,\n",
        "        \"total_puzzles\": len(results[\"group_indices\"]) - 1,\n",
        "        \"sets\": [\"all\"]\n",
        "    }\n",
        "\n",
        "    # ä¿å­˜\n",
        "    save_dir = os.path.join(OUTPUT_DIR, set_name)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    with open(os.path.join(save_dir, \"dataset.json\"), \"w\") as f:\n",
        "        json.dump(metadata, f)\n",
        "\n",
        "    for k, v in results.items():\n",
        "        np.save(os.path.join(save_dir, f\"all__{k}.npy\"), v)\n",
        "\n",
        "    print(f\"  Saved to {save_dir}\")\n",
        "\n",
        "# ============ ä¸»ç¨‹åº ============\n",
        "if __name__ == \"__main__\":\n",
        "    convert_subset(\"train\")\n",
        "    convert_subset(\"test\")\n",
        "\n",
        "    # ä¿å­˜ identifiers.json\n",
        "    with open(os.path.join(OUTPUT_DIR, \"identifiers.json\"), \"w\") as f:\n",
        "        json.dump([\"<blank>\"], f)\n",
        "\n",
        "    print(f\"\\nDone! Dataset saved to {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "f2a3b8a8e66b4c098d0ecd8d5e2361e6",
            "dbf41585d09c41cf9dd2dcd09bc0b365",
            "be261338ab4548999b45a65ada4de549",
            "6b0ffacbd02d4fe4bc3b7d9f7d7018db",
            "bbb2d3f0f04b4ef58707867f15fec2c2",
            "15979cd931304eeab50e9f6994bb706d",
            "10af8be617ee447986a39367c51b3135",
            "92d31993a36a45dfabefa7d90944cc7d",
            "74d7e0abf41b468e8b1c94650dc2f980",
            "eac40bca9fd04b69b0341d41581cba35",
            "0fa8f32fd5d041d1bf0a19702871c0ed",
            "1d71de9eb53243848459a85b2339822d",
            "6757f4f8229442aa963ac79a86efb42d",
            "88f2ba72f0b14d4fb566de0fbedce393",
            "73c2f6d7544a4e519a83abfb47bd898f",
            "9a51a14f825a45f6899f8fec3eeaefea",
            "d556364debbd43d6aa295e8c10047178",
            "d660eb1086bf4b3999ab52733ac200b7",
            "05c6e2be8ba544cb9fdf8cf20184c7b7",
            "117049c79d5f43c987fab67a3ef646fb",
            "1a357ee71b9740e09359adecb45b4d99",
            "6f579f8b40944d78aee6a4919b25db3d"
          ]
        },
        "id": "UWFatLaGjWki",
        "outputId": "99c382b3-f9f2-4453-f1c0-3ff932414a56"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train.csv:   0%|          | 0.00/719M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2a3b8a8e66b4c098d0ecd8d5e2361e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 3831994 puzzles\n",
            "  Subsampled to 1000 puzzles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 7171.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Saved to data/sudoku-extreme-$1000-aug-$1/train\n",
            "Processing test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test.csv:   0%|          | 0.00/79.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d71de9eb53243848459a85b2339822d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 422786 puzzles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 422786/422786 [00:00<00:00, 1867518.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Saved to data/sudoku-extreme-$1000-aug-$1/test\n",
            "\n",
            "Done! Dataset saved to data/sudoku-extreme-$1000-aug-$1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PuzzleDataset"
      ],
      "metadata": {
        "id": "QJfzJVBtmgY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "class PuzzleDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, split: str = \"train\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir: e.g., \"data/sudoku-extreme-$1000-aug-$1\"\n",
        "            split: \"train\" or \"test\"\n",
        "        \"\"\"\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        # åŠ è½½å…ƒæ•°æ®\n",
        "        with open(os.path.join(split_dir, \"dataset.json\")) as f:\n",
        "            self.metadata = json.load(f)\n",
        "\n",
        "        # åŠ è½½æ•°æ®\n",
        "        self.inputs = np.load(os.path.join(split_dir, \"all__inputs.npy\"))\n",
        "        self.labels = np.load(os.path.join(split_dir, \"all__labels.npy\"))\n",
        "        self.puzzle_identifiers = np.load(os.path.join(split_dir, \"all__puzzle_identifiers.npy\"))\n",
        "\n",
        "        print(f\"Loaded {len(self.inputs)} samples from {split_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"inputs\": torch.tensor(self.inputs[idx], dtype=torch.long),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "            \"puzzle_id\": torch.tensor(self.puzzle_identifiers[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# æµ‹è¯•\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = PuzzleDataset(f\"data/sudoku-extreme-${SUBSAMPLE_SIZE}-aug-${NUM_AUG}\", \"train\")\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    batch = next(iter(loader))\n",
        "    print(f\"inputs shape: {batch['inputs'].shape}\")   # [32, 81]\n",
        "    print(f\"labels shape: {batch['labels'].shape}\")   # [32, 81]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq6K6VaWlkJg",
        "outputId": "9f48de23-da52-4c6a-9f96-30f462edd063"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2000 samples from data/sudoku-extreme-$1000-aug-$1/train\n",
            "inputs shape: torch.Size([32, 81])\n",
            "labels shape: torch.Size([32, 81])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRM æ¨¡å‹æ ¸å¿ƒç»„ä»¶"
      ],
      "metadata": {
        "id": "P4-nEQfum0DQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRM"
      ],
      "metadata": {
        "id": "Da6MtuYOw3Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TinyRecursiveModel (TRM) - å®Œæ•´å¤ç°\n",
        "ä¸åŸé¡¹ç›® models/recursive_reasoning/trm.py é€»è¾‘ä¸€è‡´\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from typing import Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import einops\n",
        "\n",
        "\n",
        "# ============ å·¥å…·å‡½æ•° ============\n",
        "\n",
        "def trunc_normal_init_(tensor: torch.Tensor, std: float = 1.0, lower: float = -2.0, upper: float = 2.0):\n",
        "    \"\"\"æˆªæ–­æ­£æ€åˆå§‹åŒ– (ä¸ JAX ä¸€è‡´)\"\"\"\n",
        "    with torch.no_grad():\n",
        "        if std == 0:\n",
        "            tensor.zero_()\n",
        "        else:\n",
        "            sqrt2 = math.sqrt(2)\n",
        "            a = math.erf(lower / sqrt2)\n",
        "            b = math.erf(upper / sqrt2)\n",
        "            z = (b - a) / 2\n",
        "            c = (2 * math.pi) ** -0.5\n",
        "            pdf_u = c * math.exp(-0.5 * lower ** 2)\n",
        "            pdf_l = c * math.exp(-0.5 * upper ** 2)\n",
        "            comp_std = std / math.sqrt(1 - (upper * pdf_u - lower * pdf_l) / z - ((pdf_u - pdf_l) / z) ** 2)\n",
        "            tensor.uniform_(a, b)\n",
        "            tensor.erfinv_()\n",
        "            tensor.mul_(sqrt2 * comp_std)\n",
        "            tensor.clip_(lower * comp_std, upper * comp_std)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def rms_norm(hidden_states: torch.Tensor, variance_epsilon: float = 1e-5) -> torch.Tensor:\n",
        "    \"\"\"RMS Normalization\"\"\"\n",
        "    input_dtype = hidden_states.dtype\n",
        "    hidden_states = hidden_states.to(torch.float32)\n",
        "    variance = hidden_states.square().mean(-1, keepdim=True)\n",
        "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
        "    return hidden_states.to(input_dtype)\n",
        "\n",
        "\n",
        "def _find_multiple(a, b):\n",
        "    \"\"\"å‘ä¸Šå–æ•´åˆ° b çš„å€æ•°\"\"\"\n",
        "    return (-(a // -b)) * b\n",
        "\n",
        "\n",
        "# ============ åŸºç¡€å±‚ ============\n",
        "\n",
        "class CastedLinear(nn.Module):\n",
        "    \"\"\"å¸¦ç±»å‹è½¬æ¢çš„çº¿æ€§å±‚\"\"\"\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = False):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((out_features, in_features)), std=1.0 / (in_features ** 0.5))\n",
        "        )\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features)) if bias else None\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        weight = self.weight.to(x.dtype)\n",
        "        bias = self.bias.to(x.dtype) if self.bias is not None else None\n",
        "        return F.linear(x, weight, bias)\n",
        "\n",
        "\n",
        "class CastedEmbedding(nn.Module):\n",
        "    \"\"\"å¸¦ç±»å‹è½¬æ¢çš„ Embedding\"\"\"\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, init_std: float, cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "        self.embedding_weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return F.embedding(x, self.embedding_weight.to(self.cast_to))\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU FFN\"\"\"\n",
        "    def __init__(self, hidden_size: int, expansion: float = 4.0):\n",
        "        super().__init__()\n",
        "        inter = _find_multiple(round(expansion * hidden_size * 2 / 3), 256)\n",
        "        self.gate_up_proj = CastedLinear(hidden_size, inter * 2, bias=False)\n",
        "        self.down_proj = CastedLinear(inter, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
        "        return self.down_proj(F.silu(gate) * up)\n",
        "\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    \"\"\"Rotary Position Embedding (RoPE)\"\"\"\n",
        "    def __init__(self, dim: int, max_position_embeddings: int, base: float = 10000.0):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
        "        t = torch.arange(max_position_embeddings, dtype=torch.float32)\n",
        "        freqs = torch.outer(t, inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "\n",
        "def rotate_half(x: torch.Tensor):\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n",
        "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor):\n",
        "    orig_dtype = q.dtype\n",
        "    q, k = q.to(cos.dtype), k.to(cos.dtype)\n",
        "    q_embed = (q * cos.unsqueeze(-2)) + (rotate_half(q) * sin.unsqueeze(-2))\n",
        "    k_embed = (k * cos.unsqueeze(-2)) + (rotate_half(k) * sin.unsqueeze(-2))\n",
        "    return q_embed.to(orig_dtype), k_embed.to(orig_dtype)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"Multi-Head Self-Attention\"\"\"\n",
        "    def __init__(self, hidden_size: int, num_heads: int, causal: bool = False):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "\n",
        "        self.qkv_proj = CastedLinear(hidden_size, 3 * hidden_size, bias=False)\n",
        "        self.o_proj = CastedLinear(hidden_size, hidden_size, bias=False)\n",
        "        self.causal = causal\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, cos_sin: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> torch.Tensor:\n",
        "        B, L, _ = hidden_states.shape\n",
        "        qkv = self.qkv_proj(hidden_states).view(B, L, 3, self.num_heads, self.head_dim)\n",
        "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
        "\n",
        "        if cos_sin is not None:\n",
        "            cos, sin = cos_sin\n",
        "            q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
        "\n",
        "        # [B, L, H, D] -> [B, H, L, D]\n",
        "        q, k, v = map(lambda t: t.transpose(1, 2), (q, k, v))\n",
        "        attn_output = F.scaled_dot_product_attention(q, k, v, is_causal=self.causal)\n",
        "        attn_output = attn_output.transpose(1, 2).reshape(B, L, self.hidden_size)\n",
        "        return self.o_proj(attn_output)\n",
        "\n",
        "\n",
        "# ============ TRM Block ============\n",
        "\n",
        "class TRMBlock(nn.Module):\n",
        "    \"\"\"å•ä¸ª Transformer Block (Post-Norm)\"\"\"\n",
        "    def __init__(self, hidden_size: int, num_heads: int, expansion: float = 4.0, rms_norm_eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.self_attn = Attention(hidden_size, num_heads, causal=False)\n",
        "        self.mlp = SwiGLU(hidden_size, expansion)\n",
        "        self.norm_eps = rms_norm_eps\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, cos_sin=None) -> torch.Tensor:\n",
        "        # Post-Norm: x = norm(x + attn(x))\n",
        "        hidden_states = rms_norm(hidden_states + self.self_attn(hidden_states, cos_sin), self.norm_eps)\n",
        "        hidden_states = rms_norm(hidden_states + self.mlp(hidden_states), self.norm_eps)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class ReasoningModule(nn.Module):\n",
        "    \"\"\"æ¨ç†æ¨¡å—ï¼šå¤šå±‚ TRMBlock + input injection\"\"\"\n",
        "    def __init__(self, num_layers: int, hidden_size: int, num_heads: int, expansion: float = 4.0):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TRMBlock(hidden_size, num_heads, expansion) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_injection: torch.Tensor, cos_sin=None) -> torch.Tensor:\n",
        "        hidden_states = hidden_states + input_injection  # å…³é”®ï¼šæ³¨å…¥è¾“å…¥\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states, cos_sin)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# ============ Carry (çŠ¶æ€) ============\n",
        "\n",
        "@dataclass\n",
        "class TRMInnerCarry:\n",
        "    \"\"\"å†…éƒ¨é€’å½’çŠ¶æ€\"\"\"\n",
        "    z_H: torch.Tensor  # High-level latent\n",
        "    z_L: torch.Tensor  # Low-level latent\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TRMCarry:\n",
        "    \"\"\"å®Œæ•´ ACT çŠ¶æ€\"\"\"\n",
        "    inner_carry: TRMInnerCarry\n",
        "    steps: torch.Tensor\n",
        "    halted: torch.Tensor\n",
        "    current_data: Dict[str, torch.Tensor]\n",
        "\n",
        "\n",
        "# ============ TRM Inner Model ============\n",
        "\n",
        "class TRMInner(nn.Module):\n",
        "    \"\"\"TRM æ ¸å¿ƒæ¨¡å‹ï¼ˆä¸å« ACT wrapperï¼‰\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        seq_len: int,\n",
        "        hidden_size: int = 512,\n",
        "        num_heads: int = 8,\n",
        "        expansion: float = 4.0,\n",
        "        L_layers: int = 2,\n",
        "        H_cycles: int = 3,\n",
        "        L_cycles: int = 6,\n",
        "        pos_encodings: str = \"rope\",\n",
        "        rope_theta: float = 10000.0,\n",
        "        rms_norm_eps: float = 1e-5,\n",
        "        forward_dtype: str = \"bfloat16\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.seq_len = seq_len\n",
        "        self.H_cycles = H_cycles\n",
        "        self.L_cycles = L_cycles\n",
        "        self.forward_dtype = getattr(torch, forward_dtype)\n",
        "        self.rms_norm_eps = rms_norm_eps\n",
        "\n",
        "        # Embedding\n",
        "        self.embed_scale = math.sqrt(hidden_size)\n",
        "        embed_init_std = 1.0 / self.embed_scale\n",
        "        self.embed_tokens = CastedEmbedding(vocab_size, hidden_size, embed_init_std, self.forward_dtype)\n",
        "\n",
        "        # Output heads\n",
        "        self.lm_head = CastedLinear(hidden_size, vocab_size, bias=False)\n",
        "        self.q_head = CastedLinear(hidden_size, 2, bias=True)\n",
        "        # Q head ç‰¹æ®Šåˆå§‹åŒ–\n",
        "        with torch.no_grad():\n",
        "            self.q_head.weight.zero_()\n",
        "            self.q_head.bias.fill_(-5)\n",
        "\n",
        "        # Position encoding\n",
        "        self.pos_encodings = pos_encodings\n",
        "        if pos_encodings == \"rope\":\n",
        "            self.rotary_emb = RotaryEmbedding(hidden_size // num_heads, seq_len, rope_theta)\n",
        "\n",
        "        # Reasoning layers (L_level)\n",
        "        self.L_level = ReasoningModule(L_layers, hidden_size, num_heads, expansion)\n",
        "\n",
        "        # Initial states (å¯å­¦ä¹ )\n",
        "        self.H_init = nn.Parameter(trunc_normal_init_(torch.empty(hidden_size, dtype=self.forward_dtype), std=1))\n",
        "        self.L_init = nn.Parameter(trunc_normal_init_(torch.empty(hidden_size, dtype=self.forward_dtype), std=1))\n",
        "\n",
        "    def _input_embeddings(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        embedding = self.embed_tokens(inputs.to(torch.int32))\n",
        "        return self.embed_scale * embedding\n",
        "\n",
        "    def empty_carry(self, batch_size: int) -> TRMInnerCarry:\n",
        "        return TRMInnerCarry(\n",
        "            z_H=torch.empty(batch_size, self.seq_len, self.hidden_size, dtype=self.forward_dtype, device=self.H_init.device),\n",
        "            z_L=torch.empty(batch_size, self.seq_len, self.hidden_size, dtype=self.forward_dtype, device=self.L_init.device),\n",
        "        )\n",
        "\n",
        "    def reset_carry(self, reset_flag: torch.Tensor, carry: TRMInnerCarry) -> TRMInnerCarry:\n",
        "        \"\"\"é‡ç½®å·² halt çš„åºåˆ—çš„ carry\"\"\"\n",
        "        return TRMInnerCarry(\n",
        "            z_H=torch.where(reset_flag.view(-1, 1, 1), self.H_init, carry.z_H),\n",
        "            z_L=torch.where(reset_flag.view(-1, 1, 1), self.L_init, carry.z_L),\n",
        "        )\n",
        "\n",
        "    def forward(self, carry: TRMInnerCarry, batch: Dict[str, torch.Tensor]):\n",
        "        cos_sin = self.rotary_emb() if hasattr(self, \"rotary_emb\") else None\n",
        "\n",
        "        # Input encoding\n",
        "        input_embeddings = self._input_embeddings(batch[\"inputs\"])\n",
        "\n",
        "        z_H, z_L = carry.z_H, carry.z_L\n",
        "\n",
        "        # ========== æ ¸å¿ƒé€’å½’é€»è¾‘ ==========\n",
        "        # H_cycles - 1 æ¬¡ä¸è®¡ç®—æ¢¯åº¦ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰\n",
        "        with torch.no_grad():\n",
        "            for _ in range(self.H_cycles - 1):\n",
        "                for _ in range(self.L_cycles):\n",
        "                    z_L = self.L_level(z_L, z_H + input_embeddings, cos_sin)\n",
        "                z_H = self.L_level(z_H, z_L, cos_sin)\n",
        "\n",
        "        # æœ€å 1 æ¬¡è®¡ç®—æ¢¯åº¦\n",
        "        for _ in range(self.L_cycles):\n",
        "            z_L = self.L_level(z_L, z_H + input_embeddings, cos_sin)\n",
        "        z_H = self.L_level(z_H, z_L, cos_sin)\n",
        "        # ==================================\n",
        "\n",
        "        # Output\n",
        "        new_carry = TRMInnerCarry(z_H=z_H.detach(), z_L=z_L.detach())\n",
        "        logits = self.lm_head(z_H)\n",
        "        q_logits = self.q_head(z_H[:, 0]).to(torch.float32)  # ç”¨ç¬¬ä¸€ä¸ªä½ç½®é¢„æµ‹ halt\n",
        "\n",
        "        return new_carry, logits, (q_logits[..., 0], q_logits[..., 1])\n",
        "\n",
        "\n",
        "# ============ TRM with ACT ============\n",
        "\n",
        "class TRM(nn.Module):\n",
        "    \"\"\"å®Œæ•´ TRM æ¨¡å‹ï¼ˆå« ACT wrapperï¼‰\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        seq_len: int,\n",
        "        hidden_size: int = 512,\n",
        "        num_heads: int = 8,\n",
        "        expansion: float = 4.0,\n",
        "        L_layers: int = 2,\n",
        "        H_cycles: int = 3,\n",
        "        L_cycles: int = 6,\n",
        "        halt_max_steps: int = 16,\n",
        "        halt_exploration_prob: float = 0.1,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.inner = TRMInner(\n",
        "            vocab_size=vocab_size,\n",
        "            seq_len=seq_len,\n",
        "            hidden_size=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            expansion=expansion,\n",
        "            L_layers=L_layers,\n",
        "            H_cycles=H_cycles,\n",
        "            L_cycles=L_cycles,\n",
        "            **kwargs\n",
        "        )\n",
        "        self.halt_max_steps = halt_max_steps\n",
        "        self.halt_exploration_prob = halt_exploration_prob\n",
        "\n",
        "    def initial_carry(self, batch: Dict[str, torch.Tensor]) -> TRMCarry:\n",
        "        batch_size = batch[\"inputs\"].shape[0]\n",
        "        device = batch[\"inputs\"].device\n",
        "        return TRMCarry(\n",
        "            inner_carry=self.inner.empty_carry(batch_size),\n",
        "            steps=torch.zeros(batch_size, dtype=torch.int32, device=device),\n",
        "            halted=torch.ones(batch_size, dtype=torch.bool, device=device),  # åˆå§‹ä¸º halted\n",
        "            current_data={k: torch.empty_like(v) for k, v in batch.items()}\n",
        "        )\n",
        "\n",
        "    def forward(self, carry: TRMCarry, batch: Dict[str, torch.Tensor]):\n",
        "        # é‡ç½®å·² halt çš„åºåˆ—\n",
        "        new_inner_carry = self.inner.reset_carry(carry.halted, carry.inner_carry)\n",
        "        new_steps = torch.where(carry.halted, 0, carry.steps)\n",
        "        new_current_data = {\n",
        "            k: torch.where(carry.halted.view((-1,) + (1,) * (batch[k].ndim - 1)), batch[k], v)\n",
        "            for k, v in carry.current_data.items()\n",
        "        }\n",
        "\n",
        "        # Forward\n",
        "        new_inner_carry, logits, (q_halt_logits, q_continue_logits) = self.inner(new_inner_carry, new_current_data)\n",
        "\n",
        "        outputs = {\n",
        "            \"logits\": logits,\n",
        "            \"q_halt_logits\": q_halt_logits,\n",
        "            \"q_continue_logits\": q_continue_logits,\n",
        "        }\n",
        "\n",
        "        # ACT halting logic\n",
        "        with torch.no_grad():\n",
        "            new_steps = new_steps + 1\n",
        "            is_last_step = new_steps >= self.halt_max_steps\n",
        "            halted = is_last_step\n",
        "\n",
        "            if self.training and self.halt_max_steps > 1:\n",
        "                halted = halted | (q_halt_logits > 0)\n",
        "                # Exploration\n",
        "                min_halt_steps = (torch.rand_like(q_halt_logits) < self.halt_exploration_prob) * \\\n",
        "                                 torch.randint_like(new_steps, low=2, high=self.halt_max_steps + 1)\n",
        "                halted = halted & (new_steps >= min_halt_steps)\n",
        "\n",
        "        new_carry = TRMCarry(new_inner_carry, new_steps, halted, new_current_data)\n",
        "        return new_carry, outputs\n",
        "\n",
        "\n",
        "# ============ æµ‹è¯• ============\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    model = TRM(\n",
        "        vocab_size=11,  # PAD + 0-9\n",
        "        seq_len=81,     # 9x9 sudoku\n",
        "        hidden_size=512,\n",
        "        num_heads=8,\n",
        "        L_layers=2,\n",
        "        H_cycles=3,\n",
        "        L_cycles=6,\n",
        "    ).to(device)\n",
        "\n",
        "    # æ¨¡æ‹Ÿè¾“å…¥\n",
        "    batch = {\n",
        "        \"inputs\": torch.randint(1, 11, (4, 81), device=device),\n",
        "        \"labels\": torch.randint(1, 11, (4, 81), device=device),\n",
        "    }\n",
        "\n",
        "    carry = model.initial_carry(batch)\n",
        "    carry, outputs = model(carry, batch)\n",
        "\n",
        "    print(f\"logits shape: {outputs['logits'].shape}\")  # [4, 81, 11]\n",
        "    print(f\"q_halt_logits shape: {outputs['q_halt_logits'].shape}\")  # [4]\n",
        "    print(f\"Model params: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQHlz94km3a2",
        "outputId": "c3e55dde-4085-4632-89f5-170bb38f664c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shape: torch.Size([4, 81, 11])\n",
            "q_halt_logits shape: torch.Size([4])\n",
            "Model params: 6.83M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "1t-GexSMpC2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Loss å‡½æ•° + å®Œæ•´è®­ç»ƒå¾ªç¯\n",
        "ä¸åŸé¡¹ç›® models/losses.py å’Œ pretrain.py é€»è¾‘ä¸€è‡´\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.optim import AdamW\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import wandb  # æ–°å¢\n",
        "\n",
        "\n",
        "IGNORE_LABEL_ID = 0  # PAD token\n",
        "\n",
        "\n",
        "# ============ Stablemax Cross Entropy (åŸé¡¹ç›®ä½¿ç”¨) ============\n",
        "\n",
        "def s(x, epsilon=1e-30):\n",
        "    \"\"\"Stablemax çš„ s å‡½æ•°\"\"\"\n",
        "    return torch.where(x < 0, 1 / (1 - x + epsilon), x + 1)\n",
        "\n",
        "\n",
        "def log_stablemax(x, dim=-1):\n",
        "    \"\"\"Stablemax log-probabilities\"\"\"\n",
        "    s_x = s(x)\n",
        "    return torch.log(s_x / torch.sum(s_x, dim=dim, keepdim=True))\n",
        "\n",
        "\n",
        "def stablemax_cross_entropy(logits, labels, ignore_index=0):\n",
        "    \"\"\"\n",
        "    Stablemax Cross Entropy Loss\n",
        "    æ¯” softmax æ›´ç¨³å®šï¼Œé¿å…æ•°å€¼æº¢å‡º\n",
        "    \"\"\"\n",
        "    logprobs = log_stablemax(logits.to(torch.float64), dim=-1)\n",
        "    valid_mask = (labels != ignore_index)\n",
        "    transformed_labels = torch.where(valid_mask, labels, 0)\n",
        "    prediction_logprobs = torch.gather(\n",
        "        logprobs,\n",
        "        index=transformed_labels.to(torch.long).unsqueeze(-1),\n",
        "        dim=-1\n",
        "    ).squeeze(-1)\n",
        "    return -torch.where(valid_mask, prediction_logprobs, 0)\n",
        "\n",
        "\n",
        "# ============ ACT Loss Head ============\n",
        "\n",
        "def compute_loss(outputs, labels, ignore_index=0):\n",
        "    \"\"\"\n",
        "    è®¡ç®—å®Œæ•´ lossï¼šLM loss + Q halt loss\n",
        "    \"\"\"\n",
        "    logits = outputs[\"logits\"]\n",
        "    q_halt_logits = outputs[\"q_halt_logits\"]\n",
        "\n",
        "    # Mask for valid tokens\n",
        "    mask = (labels != ignore_index)\n",
        "    loss_counts = mask.sum(-1)\n",
        "    loss_divisor = loss_counts.clamp_min(1).unsqueeze(-1)\n",
        "\n",
        "    # LM Loss (stablemax cross entropy)\n",
        "    lm_loss = (stablemax_cross_entropy(logits, labels, ignore_index) / loss_divisor).sum()\n",
        "\n",
        "    # Correctness for Q halt target\n",
        "    with torch.no_grad():\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        is_correct = mask & (preds == labels)\n",
        "        seq_is_correct = (is_correct.sum(-1) == loss_counts).float()\n",
        "\n",
        "    # Q Halt Loss (binary cross entropy)\n",
        "    q_halt_loss = F.binary_cross_entropy_with_logits(\n",
        "        q_halt_logits,\n",
        "        seq_is_correct,\n",
        "        reduction=\"sum\"\n",
        "    )\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = lm_loss + 0.5 * q_halt_loss\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        \"lm_loss\": lm_loss.item(),\n",
        "        \"q_halt_loss\": q_halt_loss.item(),\n",
        "        \"token_accuracy\": (is_correct.sum() / mask.sum()).item(),      # æ”¹åæ›´æ¸…æ™°\n",
        "        \"exact_accuracy\": seq_is_correct.mean().item(),\n",
        "    }\n",
        "\n",
        "    return total_loss, metrics\n",
        "\n",
        "\n",
        "# ============ Dataset ============\n",
        "\n",
        "class PuzzleDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, split: str = \"train\"):\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        with open(os.path.join(split_dir, \"dataset.json\")) as f:\n",
        "            self.metadata = json.load(f)\n",
        "\n",
        "        self.inputs = np.load(os.path.join(split_dir, \"all__inputs.npy\"))\n",
        "        self.labels = np.load(os.path.join(split_dir, \"all__labels.npy\"))\n",
        "\n",
        "        print(f\"Loaded {len(self.inputs)} samples from {split_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"inputs\": torch.tensor(self.inputs[idx], dtype=torch.long),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "# ============ Learning Rate Scheduler ============\n",
        "\n",
        "def cosine_schedule_with_warmup(current_step, num_warmup_steps, num_training_steps, base_lr, min_ratio=0.0):\n",
        "    \"\"\"Cosine learning rate schedule with warmup\"\"\"\n",
        "    if current_step < num_warmup_steps:\n",
        "        return base_lr * float(current_step) / float(max(1, num_warmup_steps))\n",
        "\n",
        "    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "    return base_lr * (min_ratio + max(0.0, (1 - min_ratio) * 0.5 * (1.0 + np.cos(np.pi * progress))))\n",
        "\n",
        "\n",
        "# ============ Training Loop ============\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    test_loader=None,\n",
        "    epochs=1000,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1.0,\n",
        "    warmup_steps=200,\n",
        "    eval_interval=100,\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    model = model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.95))\n",
        "\n",
        "    total_steps = epochs * len(train_loader)\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_tok_acc = 0\n",
        "        epoch_exact_acc = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for batch in pbar:\n",
        "            # Move to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Learning rate schedule\n",
        "            current_lr = cosine_schedule_with_warmup(\n",
        "                global_step, warmup_steps, total_steps, lr\n",
        "            )\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = current_lr\n",
        "\n",
        "            # Forward with ACT loop\n",
        "            carry = model.initial_carry(batch)\n",
        "            total_loss = 0\n",
        "\n",
        "            # ACT: å¤šæ¬¡ forward ç›´åˆ°å…¨éƒ¨ halt\n",
        "            while True:\n",
        "                carry, outputs = model(carry, batch)\n",
        "\n",
        "                # åªå¯¹ halted çš„æ ·æœ¬è®¡ç®— loss\n",
        "                if carry.halted.any():\n",
        "                    halted_mask = carry.halted\n",
        "                    halted_outputs = {\n",
        "                        \"logits\": outputs[\"logits\"][halted_mask],\n",
        "                        \"q_halt_logits\": outputs[\"q_halt_logits\"][halted_mask],\n",
        "                    }\n",
        "                    halted_labels = carry.current_data[\"labels\"][halted_mask]\n",
        "\n",
        "                    loss, metrics = compute_loss(halted_outputs, halted_labels)\n",
        "                    total_loss = total_loss + loss\n",
        "\n",
        "                if carry.halted.all():\n",
        "                    break\n",
        "\n",
        "            # Backward\n",
        "            optimizer.zero_grad()\n",
        "            (total_loss / len(batch[\"inputs\"])).backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += metrics[\"lm_loss\"]\n",
        "            epoch_tok_acc += metrics[\"token_accuracy\"]\n",
        "            epoch_exact_acc += metrics[\"exact_accuracy\"]\n",
        "            num_batches += 1\n",
        "            global_step += 1\n",
        "\n",
        "            # æ›´æ–°è¿›åº¦æ¡ï¼šå¢åŠ  token_accuracy\n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{metrics['lm_loss']:.4f}\",\n",
        "                \"tok_acc\": f\"{metrics['token_accuracy']:.2%}\",\n",
        "                \"exact_acc\": f\"{metrics['exact_accuracy']:.2%}\",\n",
        "                \"lr\": f\"{current_lr:.2e}\"\n",
        "            })\n",
        "\n",
        "            # W&B è®°å½•æ¯ä¸ª step\n",
        "            wandb.log({\n",
        "                \"train/lm_loss\": metrics[\"lm_loss\"],\n",
        "                \"train/q_halt_loss\": metrics[\"q_halt_loss\"],\n",
        "                \"train/token_accuracy\": metrics[\"token_accuracy\"],\n",
        "                \"train/exact_accuracy\": metrics[\"exact_accuracy\"],\n",
        "                \"train/lr\": current_lr,\n",
        "            }, step=global_step)\n",
        "\n",
        "        # Epoch ç»“æŸåè®°å½•å¹³å‡å€¼\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train/epoch_lm_loss\": epoch_loss / num_batches,\n",
        "            \"train/epoch_token_accuracy\": epoch_tok_acc / num_batches,\n",
        "            \"train/epoch_exact_accuracy\": epoch_exact_acc / num_batches,\n",
        "        }, step=global_step)\n",
        "\n",
        "        # Evaluation\n",
        "        if test_loader is not None and (epoch + 1) % eval_interval == 0:\n",
        "            eval_metrics = evaluate(model, test_loader, device)\n",
        "            print(f\"\\n[Eval] Epoch {epoch+1}: tok_acc={eval_metrics['token_accuracy']:.2%}, exact_acc={eval_metrics['exact_accuracy']:.2%}\")\n",
        "\n",
        "            # W&B è®°å½• eval\n",
        "            wandb.log({\n",
        "                \"eval/token_accuracy\": eval_metrics[\"token_accuracy\"],\n",
        "                \"eval/exact_accuracy\": eval_metrics[\"exact_accuracy\"],\n",
        "            }, step=global_step)\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    total_token_correct = 0\n",
        "    total_tokens = 0\n",
        "    total_seq_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            carry = model.initial_carry(batch)\n",
        "\n",
        "            # è¯„ä¼°æ—¶è·‘æ»¡ halt_max_steps\n",
        "            for _ in range(model.halt_max_steps):\n",
        "                carry, outputs = model(carry, batch)\n",
        "\n",
        "            preds = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "            labels = batch[\"labels\"]\n",
        "            mask = (labels != IGNORE_LABEL_ID)\n",
        "\n",
        "            is_correct = mask & (preds == labels)\n",
        "            seq_correct = (is_correct.sum(-1) == mask.sum(-1))\n",
        "\n",
        "            total_token_correct += is_correct.sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "            total_seq_correct += seq_correct.sum().item()\n",
        "            total_samples += len(labels)\n",
        "\n",
        "    return {\n",
        "        \"token_accuracy\": total_token_correct / total_tokens,\n",
        "        \"exact_accuracy\": total_seq_correct / total_samples,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============ Main ============\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # é…ç½®\n",
        "    DATA_DIR = f\"data/sudoku-extreme-${SUBSAMPLE_SIZE}-aug-${NUM_AUG}\"\n",
        "    BATCH_SIZE = 256\n",
        "    EPOCHS = 100\n",
        "    LR = 1e-4\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    TEST_SUBSET_SIZE = 1000  # åªå– 1000 æ¡åšè¯„ä¼°\n",
        "\n",
        "    # ========== W&B åˆå§‹åŒ– ==========\n",
        "    wandb.init(\n",
        "        project=\"sudoku-extreme-tiny_trm\",\n",
        "        entity=\"jarviszhang-new-york-university\",\n",
        "        config={\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"lr\": LR,\n",
        "            \"weight_decay\": 1.0,\n",
        "            \"warmup_steps\": 200,\n",
        "            \"hidden_size\": 512,\n",
        "            \"num_heads\": 8,\n",
        "            \"L_layers\": 2,\n",
        "            \"H_cycles\": 3,\n",
        "            \"L_cycles\": 6,\n",
        "            \"halt_max_steps\": 16,\n",
        "            \"test_subset_size\": TEST_SUBSET_SIZE,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # æ•°æ®\n",
        "    train_dataset = PuzzleDataset(DATA_DIR, \"train\")\n",
        "    test_dataset_full = PuzzleDataset(DATA_DIR, \"test\")\n",
        "\n",
        "    # ========== ä» test é‡Œåªå– 1000 æ¡ ==========\n",
        "    np.random.seed(42)\n",
        "    test_indices = np.random.choice(len(test_dataset_full), size=TEST_SUBSET_SIZE, replace=False)\n",
        "    test_dataset = Subset(test_dataset_full, test_indices)\n",
        "    print(f\"Using {len(test_dataset)} samples for evaluation (subset of test set)\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # æ¨¡å‹ (ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„ TRM)\n",
        "    model = TRM(\n",
        "        vocab_size=11,\n",
        "        seq_len=81,\n",
        "        hidden_size=512,\n",
        "        num_heads=8,\n",
        "        L_layers=2,\n",
        "        H_cycles=3,\n",
        "        L_cycles=6,\n",
        "        halt_max_steps=16,\n",
        "    )\n",
        "\n",
        "    print(f\"Model params: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "\n",
        "    # W&B watch æ¨¡å‹ï¼ˆå¯é€‰ï¼Œè®°å½•æ¢¯åº¦ï¼‰\n",
        "    wandb.watch(model, log=\"gradients\", log_freq=100)\n",
        "\n",
        "    # è®­ç»ƒ\n",
        "    train(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        test_loader=test_loader,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        weight_decay=1.0,\n",
        "        warmup_steps=200,\n",
        "        eval_interval=10,\n",
        "        device=DEVICE,\n",
        "    )\n",
        "\n",
        "    # è®­ç»ƒç»“æŸ\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fI8VWL4QpGb0",
        "outputId": "fc5f5b4f-0964-497d-a8bc-aa3ee871d0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–‚â–„â–…â–‡â–ˆ</td></tr><tr><td>train/epoch_exact_accuracy</td><td>â–â–â–â–â–â–</td></tr><tr><td>train/epoch_lm_loss</td><td>â–ˆâ–…â–‚â–â–â–</td></tr><tr><td>train/epoch_token_accuracy</td><td>â–â–…â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/exact_accuracy</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/lm_loss</td><td>â–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/lr</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–„â–„â–„â–„â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/q_halt_loss</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–</td></tr><tr><td>train/token_accuracy</td><td>â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>6</td></tr><tr><td>train/epoch_exact_accuracy</td><td>0</td></tr><tr><td>train/epoch_lm_loss</td><td>43.93404</td></tr><tr><td>train/epoch_token_accuracy</td><td>0.44801</td></tr><tr><td>train/exact_accuracy</td><td>0</td></tr><tr><td>train/lm_loss</td><td>43.88954</td></tr><tr><td>train/lr</td><td>0.0001</td></tr><tr><td>train/q_halt_loss</td><td>0.00699</td></tr><tr><td>train/token_accuracy</td><td>0.45602</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">olive-terrain-1</strong> at: <a href='https://wandb.ai/jarviszhang-new-york-university/sudoku-extreme-tiny_trm/runs/oua2zxlw' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/sudoku-extreme-tiny_trm/runs/oua2zxlw</a><br> View project at: <a href='https://wandb.ai/jarviszhang-new-york-university/sudoku-extreme-tiny_trm' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/sudoku-extreme-tiny_trm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251129_221503-oua2zxlw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251129_222032-353461z4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jarviszhang-new-york-university/sudoku-extreme-tiny_trm/runs/353461z4' target=\"_blank\">fancy-field-2</a></strong> to <a href='https://wandb.ai/jarviszhang-new-york-university/sudoku-extreme-tiny_trm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jarviszhang-new-york-university/sudoku-extreme-tiny_trm' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/sudoku-extreme-tiny_trm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jarviszhang-new-york-university/sudoku-extreme-tiny_trm/runs/353461z4' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/sudoku-extreme-tiny_trm/runs/353461z4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2000 samples from data/sudoku-extreme-$1000-aug-$1/train\n",
            "Loaded 422786 samples from data/sudoku-extreme-$1000-aug-$1/test\n",
            "Using 1000 samples for evaluation (subset of test set)\n",
            "Model params: 6.83M\n",
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.16s/it, loss=531.3897, tok_acc=9.30%, exact_acc=0.00%, lr=3.50e-06]\n",
            "Epoch 2/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.18s/it, loss=516.5179, tok_acc=10.30%, exact_acc=0.00%, lr=7.50e-06]\n",
            "Epoch 3/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.18s/it, loss=497.7590, tok_acc=11.35%, exact_acc=0.00%, lr=1.15e-05]\n",
            "Epoch 4/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.18s/it, loss=475.9999, tok_acc=11.79%, exact_acc=0.00%, lr=1.55e-05]\n",
            "Epoch 5/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.18s/it, loss=458.2374, tok_acc=13.81%, exact_acc=0.00%, lr=1.95e-05]\n",
            "Epoch 6/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.17s/it, loss=455.2453, tok_acc=17.95%, exact_acc=0.00%, lr=2.35e-05]\n",
            "Epoch 7/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.17s/it, loss=453.0454, tok_acc=18.29%, exact_acc=0.00%, lr=2.75e-05]\n",
            "Epoch 8/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.18s/it, loss=449.6562, tok_acc=23.37%, exact_acc=0.00%, lr=3.15e-05]\n",
            "Epoch 9/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.18s/it, loss=433.0121, tok_acc=33.25%, exact_acc=0.00%, lr=3.55e-05]\n",
            "Epoch 10/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.18s/it, loss=407.2858, tok_acc=36.33%, exact_acc=0.00%, lr=3.95e-05]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:31<00:00,  7.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Eval] Epoch 10: tok_acc=37.17%, exact_acc=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.15s/it, loss=381.6966, tok_acc=39.16%, exact_acc=0.00%, lr=4.35e-05]\n",
            "Epoch 12/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.19s/it, loss=373.9076, tok_acc=39.62%, exact_acc=0.00%, lr=4.75e-05]\n",
            "Epoch 13/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.19s/it, loss=358.8188, tok_acc=39.99%, exact_acc=0.00%, lr=5.15e-05]\n",
            "Epoch 14/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.18s/it, loss=349.0357, tok_acc=40.85%, exact_acc=0.00%, lr=5.55e-05]\n",
            "Epoch 15/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.18s/it, loss=339.9418, tok_acc=40.91%, exact_acc=0.00%, lr=5.95e-05]\n",
            "Epoch 16/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.19s/it, loss=330.2303, tok_acc=41.84%, exact_acc=0.00%, lr=6.35e-05]\n",
            "Epoch 17/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.19s/it, loss=325.4058, tok_acc=42.17%, exact_acc=0.00%, lr=6.75e-05]\n",
            "Epoch 18/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.19s/it, loss=316.0702, tok_acc=42.65%, exact_acc=0.00%, lr=7.15e-05]\n",
            "Epoch 19/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.19s/it, loss=310.3760, tok_acc=42.55%, exact_acc=0.00%, lr=7.55e-05]\n",
            "Epoch 20/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.19s/it, loss=305.2570, tok_acc=43.22%, exact_acc=0.00%, lr=7.95e-05]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:31<00:00,  7.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Eval] Epoch 20: tok_acc=42.75%, exact_acc=0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.16s/it, loss=302.0923, tok_acc=43.17%, exact_acc=0.00%, lr=8.35e-05]\n",
            "Epoch 22/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.19s/it, loss=299.1765, tok_acc=43.03%, exact_acc=0.00%, lr=8.75e-05]\n",
            "Epoch 23/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.19s/it, loss=298.5241, tok_acc=43.10%, exact_acc=0.00%, lr=9.15e-05]\n",
            "Epoch 24/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.19s/it, loss=296.6414, tok_acc=43.28%, exact_acc=0.00%, lr=9.55e-05]\n",
            "Epoch 25/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.22s/it, loss=295.4992, tok_acc=43.45%, exact_acc=0.00%, lr=9.95e-05]\n",
            "Epoch 26/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:05<00:00,  8.16s/it, loss=294.8296, tok_acc=43.31%, exact_acc=0.00%, lr=1.00e-04]\n",
            "Epoch 27/100:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:50<00:16,  8.43s/it, loss=358.9924, tok_acc=44.27%, exact_acc=0.00%, lr=9.99e-05]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JS0GzzZJrxOx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}