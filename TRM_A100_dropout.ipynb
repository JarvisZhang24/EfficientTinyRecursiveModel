{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iizK9gdI9pEZ"
      },
      "source": [
        "# üß† TinyRecursiveModel: H100 Implementation\n",
        "\n",
        "This notebook provides a **complete faithful reproduction** of the **TinyRecursiveModels** (TRM) architecture.\n",
        "\n",
        "**Key Features:**\n",
        "- ‚úÖ **Exact reproduction** of the original TinyRecursiveModels codebase\n",
        "- ‚úÖ **A100 optimizations** (torch.compile, TF32, optimized attention)\n",
        "- ‚úÖ **No distributed code** - single GPU implementation\n",
        "- ‚úÖ **Ready to run** on H100 GPUs\n",
        "\n",
        "**Based on:** [TinyRecursiveModels](https://github.com/AlexiaJM/TinyRecursiveModels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuNem3Fy9pEa"
      },
      "source": [
        "# TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbExfPNo9pEb"
      },
      "outputs": [],
      "source": [
        "# Hugging Face Token\n",
        "HF_TOKEN = \"\"\n",
        "# Wandb Token\n",
        "WANDB_API_KEY = \"edf531272fd2779dd3f4e16d520e8b9d7913f20a\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjCOz7Rx9pEb"
      },
      "source": [
        "## Part 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_61NY4wz9pEb",
        "outputId": "c057918b-6e64-4770-a40c-430d1fb25489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dependencies installed!\n",
            "üìù Note: Using PyTorch's built-in AdamW optimizer (no need for adam-atan2-pytorch)\n",
            "üìù Note: Installed nvidia-ml-py for GPU monitoring\n"
          ]
        }
      ],
      "source": [
        "!uv pip install -q torch einops tqdm numpy pydantic wandb coolname ninja wheel triton nvidia-ml-py\n",
        "print(\"‚úÖ Dependencies installed!\")\n",
        "print(\"üìù Note: Using PyTorch's built-in AdamW optimizer (no need for adam-atan2-pytorch)\")\n",
        "print(\"üìù Note: Installed nvidia-ml-py for GPU monitoring\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htYw-Ka59pEc"
      },
      "source": [
        "## Part 2: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH5FRUc19pEc",
        "outputId": "11f3998b-73f3-40f6-ec21-6b889780e3d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "üöÄ H100/A100 detected! TF32 enabled\n",
            "======================================================================\n",
            "üìä Configuring Weights & Biases\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkd3289\u001b[0m (\u001b[33mjarviszhang-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ W&B Project: TRM-A100-Dropout\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "from typing import Optional, Any, Sequence, List\n",
        "from dataclasses import dataclass\n",
        "import os\n",
        "import math\n",
        "import yaml\n",
        "import shutil\n",
        "import copy\n",
        "import importlib\n",
        "import inspect\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import coolname\n",
        "import pydantic\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Using PyTorch's built-in AdamW optimizer instead of AdamAtan2\n",
        "# AdamW is more standard and widely used\n",
        "\n",
        "# H100 optimizations\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    if \"H100\" in gpu_name or \"A100\" in gpu_name:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        print(\"üöÄ H100/A100 detected! TF32 enabled\")\n",
        "    torch.cuda.set_device(0)\n",
        "\n",
        "# ============================================================================\n",
        "# Wandb Configuration (Similar to TRM_Baseline.ipynb)\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"üìä Configuring Weights & Biases\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "\n",
        "# Project configuration\n",
        "WANDB_PROJECT = \"TRM-A100-Dropout\"\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "WANDB_ENTITY = None  # Set to your W&B username/team if needed (e.g., \"jarviszhang-new-york-university\")\n",
        "\n",
        "print(f\"üìÅ W&B Project: {WANDB_PROJECT}\")\n",
        "if WANDB_ENTITY:\n",
        "    print(f\"üë§ W&B Entity: {WANDB_ENTITY}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydiRPiT69pEc"
      },
      "source": [
        "## Part 3: Common Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCQQOm_M9pEc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def trunc_normal_init_(tensor: torch.Tensor, std: float = 1.0, lower: float = -2.0, upper: float = 2.0):\n",
        "    # NOTE: PyTorch nn.init.trunc_normal_ is not mathematically correct, the std dev is not actually the std dev of initialized tensor\n",
        "    # This function is a PyTorch version of jax truncated normal init (default init method in flax)\n",
        "    # https://github.com/jax-ml/jax/blob/main/jax/_src/random.py#L807-L848\n",
        "    # https://github.com/jax-ml/jax/blob/main/jax/_src/nn/initializers.py#L162-L199\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if std == 0:\n",
        "            tensor.zero_()\n",
        "        else:\n",
        "            sqrt2 = math.sqrt(2)\n",
        "            a = math.erf(lower / sqrt2)\n",
        "            b = math.erf(upper / sqrt2)\n",
        "            z = (b - a) / 2\n",
        "\n",
        "            c = (2 * math.pi) ** -0.5\n",
        "            pdf_u = c * math.exp(-0.5 * lower ** 2)\n",
        "            pdf_l = c * math.exp(-0.5 * upper ** 2)\n",
        "            comp_std = std / math.sqrt(1 - (upper * pdf_u - lower * pdf_l) / z - ((pdf_u - pdf_l) / z) ** 2)\n",
        "\n",
        "            tensor.uniform_(a, b)\n",
        "            tensor.erfinv_()\n",
        "            tensor.mul_(sqrt2 * comp_std)\n",
        "            tensor.clip_(lower * comp_std, upper * comp_std)\n",
        "\n",
        "    return tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzcDgLx89pEc"
      },
      "source": [
        "## Part 4: Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V27NsJat9pEc"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import einops\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#try:\n",
        "#    from flash_attn_interface import flash_attn_func  # type: ignore[import]\n",
        "#except ImportError:\n",
        "#    # Fallback to FlashAttention 2\n",
        "#    from flash_attn import flash_attn_func  # type: ignore[import]\n",
        "from torch.nn.functional import scaled_dot_product_attention\n",
        "\n",
        "# trunc_normal_init_ defined in Part 3\n",
        "\n",
        "\n",
        "CosSin = Tuple[torch.Tensor, torch.Tensor]\n",
        "\n",
        "\n",
        "def _find_multiple(a, b):\n",
        "    return (-(a // -b)) * b\n",
        "\n",
        "\n",
        "def rotate_half(x: torch.Tensor):\n",
        "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n",
        "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor):\n",
        "    # q, k: [bs, seq_len, num_heads, head_dim]\n",
        "    # cos, sin: [seq_len, head_dim]\n",
        "    orig_dtype = q.dtype\n",
        "    q = q.to(cos.dtype)\n",
        "    k = k.to(cos.dtype)\n",
        "\n",
        "    q_embed = (q * cos.unsqueeze(-2)) + (rotate_half(q) * sin.unsqueeze(-2))\n",
        "    k_embed = (k * cos.unsqueeze(-2)) + (rotate_half(k) * sin.unsqueeze(-2))\n",
        "\n",
        "    return q_embed.to(orig_dtype), k_embed.to(orig_dtype)\n",
        "\n",
        "\n",
        "class CastedLinear(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 out_features: int,\n",
        "                 bias: bool):\n",
        "        super().__init__()\n",
        "        # Truncated LeCun normal init\n",
        "        self.weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((out_features, in_features)), std=1.0 / (in_features ** 0.5))\n",
        "        )\n",
        "        self.bias = None\n",
        "        if bias:\n",
        "            # Zero init bias\n",
        "            self.bias = nn.Parameter(torch.zeros((out_features, )))\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return F.linear(input, self.weight.to(input.dtype), bias=self.bias.to(input.dtype) if self.bias is not None else None)\n",
        "\n",
        "\n",
        "class CastedEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_embeddings: int,\n",
        "                 embedding_dim: int,\n",
        "                 init_std: float,\n",
        "                 cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "\n",
        "        # Truncated LeCun normal init\n",
        "        self.embedding_weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std)\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return F.embedding(input, self.embedding_weight.to(self.cast_to))\n",
        "\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_position_embeddings, base, device=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # RoPE\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))\n",
        "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
        "        freqs = torch.outer(t, inv_freq)\n",
        "\n",
        "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.cos_cached = nn.Buffer(emb.cos(), persistent=False)\n",
        "        self.sin_cached = nn.Buffer(emb.sin(), persistent=False)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "\n",
        "# class Attention(nn.Module):\n",
        "#     def __init__(self, hidden_size, head_dim, num_heads, num_key_value_heads, causal=False):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.head_dim = head_dim\n",
        "#         self.output_size = head_dim * num_heads\n",
        "#         self.num_heads = num_heads\n",
        "#         self.num_key_value_heads = num_key_value_heads\n",
        "#         self.causal = causal\n",
        "\n",
        "#         self.qkv_proj = CastedLinear(self.hidden_size, (self.num_heads + 2 * self.num_key_value_heads) * self.head_dim, bias=False)\n",
        "#         self.o_proj = CastedLinear(self.output_size, self.hidden_size, bias=False)\n",
        "\n",
        "#     def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "#         batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "#         # hidden_states: [bs, seq_len, num_heads, head_dim]\n",
        "#         qkv = self.qkv_proj(hidden_states)\n",
        "\n",
        "#         # Split head\n",
        "#         qkv = qkv.view(batch_size, seq_len, self.num_heads + 2 * self.num_key_value_heads, self.head_dim)\n",
        "#         query = qkv[:, :, :self.num_heads]\n",
        "#         key = qkv[:, :, self.num_heads: self.num_heads + self.num_key_value_heads]\n",
        "#         value = qkv[:, :, self.num_heads + self.num_key_value_heads:]\n",
        "\n",
        "#         # RoPE\n",
        "#         if cos_sin is not None:\n",
        "#             cos, sin = cos_sin\n",
        "#             query, key = apply_rotary_pos_emb(query, key, cos, sin)\n",
        "\n",
        "#         # flash attn\n",
        "#         query, key, value = map(lambda t: einops.rearrange(t, 'B S H D -> B H S D'), (query, key, value)) # needed for scaled_dot_product_attention but not flash_attn_func\n",
        "#         attn_output = scaled_dot_product_attention(query=query, key=key, value=value, is_causal=self.causal)\n",
        "#         attn_output = einops.rearrange(attn_output, 'B H S D -> B S H D')\n",
        "#         attn_output = attn_output.view(batch_size, seq_len, self.output_size)  # type: ignore\n",
        "#         return self.o_proj(attn_output)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, head_dim, num_heads, num_key_value_heads, causal=False, attention_dropout=0.0, hidden_dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.head_dim = head_dim\n",
        "        self.output_size = head_dim * num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.num_key_value_heads = num_key_value_heads\n",
        "        self.causal = causal\n",
        "\n",
        "        # Dropout\n",
        "        self.attention_dropout_prob = attention_dropout\n",
        "        self.hidden_dropout = nn.Dropout(hidden_dropout)\n",
        "\n",
        "        self.qkv_proj = CastedLinear(self.hidden_size, (self.num_heads + 2 * self.num_key_value_heads) * self.head_dim, bias=False)\n",
        "        self.o_proj = CastedLinear(self.output_size, self.hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "        # hidden_states: [bs, seq_len, num_heads, head_dim]\n",
        "        qkv = self.qkv_proj(hidden_states)\n",
        "\n",
        "        # Split head\n",
        "        qkv = qkv.view(batch_size, seq_len, self.num_heads + 2 * self.num_key_value_heads, self.head_dim)\n",
        "        query = qkv[:, :, :self.num_heads]\n",
        "        key = qkv[:, :, self.num_heads: self.num_heads + self.num_key_value_heads]\n",
        "        value = qkv[:, :, self.num_heads + self.num_key_value_heads:]\n",
        "\n",
        "        # RoPE\n",
        "        if cos_sin is not None:\n",
        "            cos, sin = cos_sin\n",
        "            query, key = apply_rotary_pos_emb(query, key, cos, sin)\n",
        "\n",
        "        # flash attn\n",
        "        query, key, value = map(lambda t: einops.rearrange(t, 'B S H D -> B H S D'), (query, key, value)) # needed for scaled_dot_product_attention but not flash_attn_func\n",
        "        attn_output = scaled_dot_product_attention(\n",
        "            query=query, key=key, value=value, is_causal=self.causal,\n",
        "            dropout_p=self.attention_dropout_prob if self.training else 0.0  # Key Dropout Modification\n",
        "        )\n",
        "        attn_output = einops.rearrange(attn_output, 'B H S D -> B S H D')\n",
        "        attn_output = attn_output.view(batch_size, seq_len, self.output_size)  # type: ignore\n",
        "\n",
        "        output = self.o_proj(attn_output)\n",
        "        output = self.hidden_dropout(output) # Key Dropout Modification\n",
        "        return output\n",
        "\n",
        "class LinearSwish(nn.Module):\n",
        "    def __init__(self, hidden_size: int, reverse=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = CastedLinear(hidden_size, hidden_size, bias=False)\n",
        "        self.reverse = reverse\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.reverse:\n",
        "            return F.silu(self.linear(x))\n",
        "        else:\n",
        "            return self.linear(F.silu(x))\n",
        "\n",
        "\n",
        "# class SwiGLU(nn.Module):\n",
        "#     def __init__(self, hidden_size: int, expansion: float):\n",
        "#         super().__init__()\n",
        "#         inter = _find_multiple(round(expansion * hidden_size * 2 / 3), 256)\n",
        "\n",
        "#         self.gate_up_proj = CastedLinear(hidden_size, inter * 2, bias=False)\n",
        "#         self.down_proj    = CastedLinear(inter, hidden_size, bias=False)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
        "#         return self.down_proj(F.silu(gate) * up)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, hidden_size: int, expansion: float,\n",
        "                 activation_dropout: float = 0.0, hidden_dropout: float = 0.0): # Key Dropout Modification\n",
        "        super().__init__()\n",
        "        inter = _find_multiple(round(expansion * hidden_size * 2 / 3), 256)\n",
        "\n",
        "        self.gate_up_proj = CastedLinear(hidden_size, inter * 2, bias=False)\n",
        "        self.down_proj    = CastedLinear(inter, hidden_size, bias=False)\\\n",
        "\n",
        "        # Dropout\n",
        "        self.activation_dropout = nn.Dropout(activation_dropout)\n",
        "        self.hidden_dropout = nn.Dropout(hidden_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
        "        activated = F.silu(gate) * up\n",
        "        activated = self.activation_dropout(activated)  # Key Dropout Modification\n",
        "        output = self.down_proj(activated)\n",
        "        output = self.hidden_dropout(output)  # Key Dropout Modification\n",
        "        return output\n",
        "\n",
        "def rms_norm(hidden_states: torch.Tensor, variance_epsilon: float) -> torch.Tensor:\n",
        "    input_dtype = hidden_states.dtype\n",
        "    hidden_states = hidden_states.to(torch.float32)\n",
        "\n",
        "    variance = hidden_states.square().mean(-1, keepdim=True)\n",
        "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
        "    return hidden_states.to(input_dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lwd9STh9pEd"
      },
      "source": [
        "## Part 5: Sparse Embedding (Non-distributed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfJWgaCb9pEd",
        "outputId": "b4aee940-a48f-46ee-8567-7c544d731861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Efficiency metrics functions loaded!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Part 12.5: Efficiency Metrics Functions\n",
        "# ============================================================================\n",
        "# These functions are added to track efficiency metrics for comparing different architectures\n",
        "# Note: Using 'Any' type hint to avoid forward reference issues with PretrainConfig\n",
        "\n",
        "def get_gpu_memory_usage() -> float:\n",
        "    \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated() / (1024 ** 3)\n",
        "    return 0.0\n",
        "\n",
        "def estimate_model_flops(batch_size: int, seq_len: int, config: Any) -> float:\n",
        "    \"\"\"\n",
        "    Estimate FLOPs for one forward pass of TRM model.\n",
        "    Rough estimation based on architecture parameters.\n",
        "    \"\"\"\n",
        "    # Extract config values\n",
        "    arch_config = config.arch if hasattr(config, 'arch') else config\n",
        "    if isinstance(arch_config, dict):\n",
        "        hidden_size = arch_config.get('hidden_size', 512)\n",
        "        expansion = arch_config.get('expansion', 4)\n",
        "        H_cycles = arch_config.get('H_cycles', 3)\n",
        "        L_cycles = arch_config.get('L_cycles', 6)\n",
        "        L_layers = arch_config.get('L_layers', 2)\n",
        "    else:\n",
        "        hidden_size = getattr(arch_config, 'hidden_size', 512)\n",
        "        expansion = getattr(arch_config, 'expansion', 4)\n",
        "        H_cycles = getattr(arch_config, 'H_cycles', 3)\n",
        "        L_cycles = getattr(arch_config, 'L_cycles', 6)\n",
        "        L_layers = getattr(arch_config, 'L_layers', 2)\n",
        "\n",
        "    # Base embedding FLOPs\n",
        "    embedding_flops = batch_size * seq_len * hidden_size\n",
        "\n",
        "    # Per transformer block FLOPs\n",
        "    attn_flops = batch_size * seq_len * seq_len * hidden_size * 4  # QKV + attention + output\n",
        "    mlp_flops = batch_size * seq_len * hidden_size * expansion * 2  # gate_up + down\n",
        "\n",
        "    # Per layer FLOPs\n",
        "    layer_flops = attn_flops + mlp_flops\n",
        "\n",
        "    # Total recursive cycles\n",
        "    total_cycles = H_cycles * (L_cycles * L_layers + 1)\n",
        "\n",
        "    # Total FLOPs per forward pass\n",
        "    total_flops = embedding_flops + total_cycles * layer_flops\n",
        "\n",
        "    return total_flops\n",
        "\n",
        "print(\"‚úÖ Efficiency metrics functions loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KMhrnKq9pEe"
      },
      "source": [
        "## Part 12.6: Efficiency Metrics Integration\n",
        "\n",
        "**üìä Added Efficiency Metrics:**\n",
        "\n",
        "### Training Efficiency Metrics (train_batch)\n",
        "- `train/batch_time`: Total time per batch (seconds)\n",
        "- `train/forward_time`: Forward propagation time (seconds)\n",
        "- `train/samples_per_sec`: Training throughput (samples/sec)\n",
        "- `train/tokens_per_sec`: Token processing throughput (tokens/sec)\n",
        "- `train/gpu_memory_gb`: GPU memory usage (GB)\n",
        "- `train/flops_per_sample`: FLOPs per sample\n",
        "- `train/flops_per_sec`: FLOPs processing rate\n",
        "\n",
        "### Inference Efficiency Metrics (evaluate)\n",
        "- `eval/avg_inference_steps`: Average inference steps\n",
        "- `eval/inference_time_per_step`: Inference time per step (seconds)\n",
        "- `eval/samples_per_sec`: Inference rate (samples/sec)\n",
        "- `eval/gpu_memory_gb`: GPU memory usage during inference (GB)\n",
        "\n",
        "### Model Structure Metrics (launch)\n",
        "- `num_params`: Number of model parameters (existing)\n",
        "- `model_size_mb`: Model size (MB)\n",
        "- `estimated_flops_per_forward`: FLOPs per forward pass\n",
        "\n",
        "**üîß Modification Notes:**\n",
        "The code cells below contain the modified `train_batch` and `evaluate` functions. Replace the original functions in the cells of Part 12 with these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi6MVAXD9pEe",
        "outputId": "dc3b075a-5375-4ab5-94d1-cb217e76b367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Modified train_batch function with efficiency metrics loaded!\n",
            "‚ö†Ô∏è  Note: Replace the original train_batch function in Part 12 with train_batch_efficient\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Modified train_batch function with efficiency metrics\n",
        "# ============================================================================\n",
        "# Replace the train_batch function in Part 12 (cell 12) with this version\n",
        "\n",
        "def train_batch_efficient(config: Any, train_state: Any, batch: Any, global_batch_size: int, rank: int = 0, world_size: int = 1):\n",
        "    \"\"\"\n",
        "    Modified train_batch function with efficiency metrics tracking.\n",
        "    Replace the original train_batch function in Part 12 with this version.\n",
        "    \"\"\"\n",
        "    train_state.step += 1\n",
        "    if train_state.step > train_state.total_steps:\n",
        "        return\n",
        "\n",
        "    # ============================================================================\n",
        "    # Efficiency Metrics: Start timing and memory tracking\n",
        "    # ============================================================================\n",
        "    batch_start_time = time.time()\n",
        "    gpu_memory_before = get_gpu_memory_usage()\n",
        "\n",
        "    # To device\n",
        "    batch = {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "    # Get sequence length for FLOPs calculation\n",
        "    seq_len = batch[\"inputs\"].shape[1]\n",
        "\n",
        "    # Init carry if it is None\n",
        "    if train_state.carry is None:\n",
        "        with torch.device(\"cuda\"):\n",
        "            train_state.carry = train_state.model.initial_carry(batch)  # type: ignore\n",
        "\n",
        "    # Forward pass with timing\n",
        "    forward_start_time = time.time()\n",
        "    train_state.carry, loss, metrics, _, _ = train_state.model(carry=train_state.carry, batch=batch, return_keys=[])\n",
        "    forward_time = time.time() - forward_start_time\n",
        "\n",
        "    # Check for NaN or Inf in loss\n",
        "    loss_value = loss.item() if isinstance(loss, torch.Tensor) else loss\n",
        "    if not (torch.isfinite(loss) if isinstance(loss, torch.Tensor) else (math.isfinite(loss_value) if isinstance(loss_value, float) else True)):\n",
        "        print(f\"‚ö†Ô∏è WARNING: Step {train_state.step} - Loss is NaN or Inf: {loss_value}\")\n",
        "        return None\n",
        "\n",
        "    # Backward pass\n",
        "    backward_start_time = time.time()\n",
        "    ((1 / global_batch_size) * loss).backward()\n",
        "    backward_time = time.time() - backward_start_time\n",
        "\n",
        "    # Allreduce (single GPU, skip)\n",
        "    if False:  # Single GPU\n",
        "        pass\n",
        "\n",
        "    # Compute gradient norm for monitoring (no clipping, just recording)\n",
        "    grad_norm = compute_grad_norm(train_state.model)\n",
        "\n",
        "    # Apply optimizer\n",
        "    optimizer_start_time = time.time()\n",
        "    lr_this_step = None\n",
        "    for optim, base_lr in zip(train_state.optimizers, train_state.optimizer_lrs):\n",
        "        lr_this_step = compute_lr(base_lr, config, train_state)\n",
        "        for param_group in optim.param_groups:\n",
        "            param_group['lr'] = lr_this_step\n",
        "\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    optimizer_time = time.time() - optimizer_start_time\n",
        "\n",
        "    # Total batch time\n",
        "    batch_time = time.time() - batch_start_time\n",
        "    gpu_memory_after = get_gpu_memory_usage()\n",
        "\n",
        "    # Calculate efficiency metrics\n",
        "    total_tokens = global_batch_size * seq_len\n",
        "    samples_per_sec = global_batch_size / batch_time if batch_time > 0 else 0\n",
        "    tokens_per_sec = total_tokens / batch_time if batch_time > 0 else 0\n",
        "\n",
        "    # Estimate FLOPs\n",
        "    flops_per_forward = estimate_model_flops(global_batch_size, seq_len, config)\n",
        "    flops_per_sample = flops_per_forward / global_batch_size\n",
        "    flops_per_sec = flops_per_forward / forward_time if forward_time > 0 else 0\n",
        "\n",
        "    # Reduce metrics\n",
        "    if len(metrics):\n",
        "        assert not any(v.requires_grad for v in metrics.values())\n",
        "        metric_keys = list(sorted(metrics.keys()))\n",
        "        metric_values = torch.stack([metrics[k] for k in metric_keys])\n",
        "        if False:  # Single GPU\n",
        "            pass\n",
        "        if True:  # Single GPU, always rank 0\n",
        "            metric_values = metric_values.cpu().numpy()\n",
        "            reduced_metrics = {k: metric_values[i] for i, k in enumerate(metric_keys)}\n",
        "\n",
        "            # Postprocess\n",
        "            count = max(reduced_metrics[\"count\"], 1)\n",
        "            reduced_metrics = {f\"train/{k}\": v / (global_batch_size if k.endswith(\"loss\") else count) for k, v in reduced_metrics.items()}\n",
        "            reduced_metrics[\"train/lr\"] = lr_this_step\n",
        "\n",
        "            # Add gradient norm monitoring\n",
        "            if grad_norm is not None:\n",
        "                reduced_metrics[\"train/grad_norm\"] = float(grad_norm)\n",
        "\n",
        "            # ============================================================================\n",
        "            # Add Efficiency Metrics\n",
        "            # ============================================================================\n",
        "            reduced_metrics[\"train/batch_time\"] = batch_time\n",
        "            reduced_metrics[\"train/forward_time\"] = forward_time\n",
        "            reduced_metrics[\"train/backward_time\"] = backward_time\n",
        "            reduced_metrics[\"train/optimizer_time\"] = optimizer_time\n",
        "            reduced_metrics[\"train/samples_per_sec\"] = samples_per_sec\n",
        "            reduced_metrics[\"train/tokens_per_sec\"] = tokens_per_sec\n",
        "            reduced_metrics[\"train/gpu_memory_gb\"] = gpu_memory_after\n",
        "            reduced_metrics[\"train/flops_per_sample\"] = flops_per_sample\n",
        "            reduced_metrics[\"train/flops_per_sec\"] = flops_per_sec\n",
        "\n",
        "            return reduced_metrics\n",
        "\n",
        "print(\"‚úÖ Modified train_batch function with efficiency metrics loaded!\")\n",
        "print(\"‚ö†Ô∏è  Note: Replace the original train_batch function in Part 12 with train_batch_efficient\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9iJ2slK9pEe",
        "outputId": "ba585445-5762-4e65-ea7c-afadcb22413d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Modified evaluate function with efficiency metrics loaded!\n",
            "‚ö†Ô∏è  Note: Replace the original evaluate function in Part 12 with evaluate_efficient\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Modified evaluate function with efficiency metrics\n",
        "# ============================================================================\n",
        "# Replace the evaluate function in Part 12 (cell 12) with this version\n",
        "\n",
        "def evaluate_efficient(\n",
        "    config: Any,\n",
        "    train_state: Any,\n",
        "    eval_loader: torch.utils.data.DataLoader,\n",
        "    eval_metadata: Any,\n",
        "    evaluators: List[Any],\n",
        "    rank: int = 0,\n",
        "    world_size: int = 1,\n",
        "    cpu_group: Optional[Any] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Modified evaluate function with efficiency metrics tracking.\n",
        "    Replace the original evaluate function in Part 12 with this version.\n",
        "    \"\"\"\n",
        "    reduced_metrics = None\n",
        "\n",
        "    # Efficiency tracking\n",
        "    total_inference_steps = 0\n",
        "    total_inference_time = 0.0\n",
        "    total_samples = 0\n",
        "    eval_start_time = time.time()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        return_keys = set(config.eval_save_outputs)\n",
        "        for evaluator in evaluators:\n",
        "            evaluator.begin_eval()\n",
        "            return_keys.update(evaluator.required_outputs)\n",
        "\n",
        "        # Run evaluation\n",
        "        set_ids = {k: idx for idx, k in enumerate(eval_metadata.sets)}\n",
        "        save_preds = {}\n",
        "        metric_keys = []\n",
        "        metric_values = None\n",
        "        carry = None\n",
        "        processed_batches = 0\n",
        "\n",
        "        for set_name, batch, global_batch_size in eval_loader:\n",
        "            if config.max_eval_batches is not None and processed_batches >= config.max_eval_batches:\n",
        "                break\n",
        "            processed_batches += 1\n",
        "            if rank == 0:\n",
        "                print(f\"Processing batch {processed_batches}: {set_name}\")\n",
        "\n",
        "            # To device\n",
        "            batch = {k: v.cuda() for k, v in batch.items()}\n",
        "            seq_len = batch[\"inputs\"].shape[1]\n",
        "            total_samples += global_batch_size\n",
        "\n",
        "            with torch.device(\"cuda\"):\n",
        "                carry = train_state.model.initial_carry(batch)  # type: ignore\n",
        "\n",
        "            # Forward with timing\n",
        "            inference_steps = 0\n",
        "            batch_inference_start = time.time()\n",
        "            while True:\n",
        "                step_start = time.time()\n",
        "                carry, loss, metrics, preds, all_finish = train_state.model(\n",
        "                    carry=carry, batch=batch, return_keys=return_keys\n",
        "                )\n",
        "                step_time = time.time() - step_start\n",
        "                inference_steps += 1\n",
        "                total_inference_steps += 1\n",
        "                total_inference_time += step_time\n",
        "\n",
        "                if all_finish:\n",
        "                    break\n",
        "            batch_inference_time = time.time() - batch_inference_start\n",
        "\n",
        "            if rank == 0:\n",
        "                print(f\"  Completed inference in {inference_steps} steps (time: {batch_inference_time:.3f}s)\")\n",
        "\n",
        "            for collection in (batch, preds):\n",
        "                for k, v in collection.items():\n",
        "                    if k in config.eval_save_outputs:\n",
        "                        save_preds.setdefault(k, [])\n",
        "                        save_preds[k].append(v.cpu())\n",
        "\n",
        "            for evaluator in evaluators:\n",
        "                evaluator.update_batch(batch, preds)\n",
        "\n",
        "            del carry, loss, preds, batch, all_finish\n",
        "\n",
        "            # Aggregate metrics\n",
        "            set_id = set_ids[set_name]\n",
        "            if metric_values is None:\n",
        "                metric_keys = list(sorted(metrics.keys()))\n",
        "                metric_values = torch.zeros(\n",
        "                    (len(set_ids), len(metrics.values())), dtype=torch.float32, device=\"cuda\"\n",
        "                )\n",
        "            metric_values[set_id] += torch.stack([metrics[k] for k in metric_keys])\n",
        "            del metrics\n",
        "\n",
        "        # Total evaluation time\n",
        "        total_eval_time = time.time() - eval_start_time\n",
        "        gpu_memory_eval = get_gpu_memory_usage()\n",
        "\n",
        "        # Calculate efficiency metrics\n",
        "        avg_inference_steps = total_inference_steps / processed_batches if processed_batches > 0 else 0\n",
        "        avg_inference_time_per_step = total_inference_time / total_inference_steps if total_inference_steps > 0 else 0\n",
        "        eval_samples_per_sec = total_samples / total_eval_time if total_eval_time > 0 else 0\n",
        "\n",
        "        # Concatenate save preds\n",
        "        save_preds = {k: torch.cat(v, dim=0) for k, v in save_preds.items()}\n",
        "\n",
        "        # Save preds\n",
        "        if config.checkpoint_path is not None and len(save_preds):\n",
        "            os.makedirs(os.path.dirname(config.checkpoint_path), exist_ok=True)\n",
        "            torch.save(\n",
        "                save_preds, os.path.join(config.checkpoint_path, f\"step_{train_state.step}_all_preds.{rank}\")\n",
        "            )\n",
        "        del save_preds\n",
        "\n",
        "        # Reduce to rank 0\n",
        "        if metric_values is not None:\n",
        "            if False:  # Single GPU\n",
        "                pass\n",
        "            if True:  # Single GPU, always rank 0\n",
        "                reduced_metrics = metric_values.cpu().numpy()\n",
        "                reduced_metrics = {\n",
        "                    set_name: {\n",
        "                        metric_name: reduced_metrics[set_id, metric_id]\n",
        "                        for metric_id, metric_name in enumerate(metric_keys)\n",
        "                    }\n",
        "                    for set_id, set_name in enumerate(set_ids)\n",
        "                }\n",
        "                # Postprocess\n",
        "                for set_name, m in reduced_metrics.items():\n",
        "                    count = m.pop(\"count\")\n",
        "                    reduced_metrics[set_name] = {k: v / count for k, v in m.items()}\n",
        "\n",
        "                # ============================================================================\n",
        "                # Expose eval accuracy as flat W&B scalars (so it shows up in charts)\n",
        "                # ============================================================================\n",
        "\n",
        "                _set_metrics = {k: v for k, v in reduced_metrics.items() if isinstance(v, dict)}\n",
        "                if len(_set_metrics) == 1:\n",
        "                    _only_set_name, _m = next(iter(_set_metrics.items()))\n",
        "                    # Common single-set case (e.g., sets=[\"all\"])\n",
        "                    if \"accuracy\" in _m:\n",
        "                        reduced_metrics[\"eval/accuracy\"] = _m[\"accuracy\"]\n",
        "                    if \"exact_accuracy\" in _m:\n",
        "                        reduced_metrics[\"eval/exact_accuracy\"] = _m[\"exact_accuracy\"]\n",
        "                else:\n",
        "                    # Multi-set case: log per-set curves\n",
        "                    for _set_name, _m in _set_metrics.items():\n",
        "                        if \"accuracy\" in _m:\n",
        "                            reduced_metrics[f\"eval/{_set_name}/accuracy\"] = _m[\"accuracy\"]\n",
        "                        if \"exact_accuracy\" in _m:\n",
        "                            reduced_metrics[f\"eval/{_set_name}/exact_accuracy\"] = _m[\"exact_accuracy\"]\n",
        "\n",
        "                # ============================================================================\n",
        "                # Add Efficiency Metrics\n",
        "                # ============================================================================\n",
        "                reduced_metrics[\"eval/avg_inference_steps\"] = avg_inference_steps\n",
        "                reduced_metrics[\"eval/inference_time_per_step\"] = avg_inference_time_per_step\n",
        "                reduced_metrics[\"eval/total_eval_time\"] = total_eval_time\n",
        "                reduced_metrics[\"eval/samples_per_sec\"] = eval_samples_per_sec\n",
        "                reduced_metrics[\"eval/gpu_memory_gb\"] = gpu_memory_eval\n",
        "\n",
        "        # Run evaluators\n",
        "        if rank == 0:\n",
        "            print(f\"\\nRunning {len(evaluators)} evaluator(s)...\")\n",
        "        for i, evaluator in enumerate(evaluators):\n",
        "            if rank == 0:\n",
        "                print(f\"Running evaluator {i+1}/{len(evaluators)}: {evaluator.__class__.__name__}\")\n",
        "            evaluator_save_path = None\n",
        "            if config.checkpoint_path is not None:\n",
        "                evaluator_save_path = os.path.join(\n",
        "                    config.checkpoint_path,\n",
        "                    f\"evaluator_{evaluator.__class__.__name__}_step_{train_state.step}\",\n",
        "                )\n",
        "                os.makedirs(evaluator_save_path, exist_ok=True)\n",
        "            metrics = evaluator.result(evaluator_save_path, rank=rank, world_size=world_size, group=cpu_group)\n",
        "            if rank == 0 and metrics is not None:\n",
        "                if reduced_metrics is None:\n",
        "                    reduced_metrics = {}\n",
        "                reduced_metrics.update(metrics)\n",
        "                print(f\"  Completed {evaluator.__class__.__name__}\")\n",
        "        if rank == 0:\n",
        "            print(\"All evaluators completed!\")\n",
        "\n",
        "    return reduced_metrics\n",
        "\n",
        "print(\"‚úÖ Modified evaluate function with efficiency metrics loaded!\")\n",
        "print(\"‚ö†Ô∏è  Note: Replace the original evaluate function in Part 12 with evaluate_efficient\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpJzAJtu9pEe",
        "outputId": "5f0c24d8-ff13-4611-d309-bec3841b9b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model efficiency logging function loaded!\n",
            "‚ö†Ô∏è  Note: Call log_model_efficiency_metrics() in launch() after train_state initialization\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Modified launch function initialization with efficiency metrics\n",
        "# ============================================================================\n",
        "# Add this code block in the launch function after model initialization (around line 1990)\n",
        "\n",
        "# Add this AFTER: train_state = init_train_state(config, train_metadata, rank=RANK, world_size=WORLD_SIZE)\n",
        "# Add this BEFORE: if RANK == 0: progress_bar = tqdm(...)\n",
        "\n",
        "def log_model_efficiency_metrics(config: Any, train_state: Any, train_metadata: Any):\n",
        "    \"\"\"\n",
        "    Log model structure efficiency metrics to wandb.\n",
        "    Call this function in launch() after train_state initialization.\n",
        "    Note: This function should only be called from rank 0 (already checked in launch()).\n",
        "    \"\"\"\n",
        "    # Calculate model size\n",
        "    total_params = sum(x.numel() for x in train_state.model.parameters())\n",
        "    model_size_mb = sum(p.numel() * p.element_size() for p in train_state.model.parameters()) / (1024 ** 2)\n",
        "\n",
        "    # Estimate FLOPs for one forward pass (using average batch size and sequence length)\n",
        "    avg_batch_size = config.global_batch_size\n",
        "    seq_len = train_metadata.seq_len\n",
        "    flops_per_forward = estimate_model_flops(avg_batch_size, seq_len, config)\n",
        "\n",
        "    # Log to wandb\n",
        "    efficiency_metrics = {\n",
        "        \"num_params\": total_params,\n",
        "        \"model_size_mb\": model_size_mb,\n",
        "        \"estimated_flops_per_forward\": flops_per_forward,\n",
        "        \"estimated_flops_per_sample\": flops_per_forward / avg_batch_size,\n",
        "        \"seq_len\": seq_len,\n",
        "        \"batch_size\": avg_batch_size,\n",
        "    }\n",
        "\n",
        "    wandb.log(efficiency_metrics, step=0)\n",
        "    print(\"=\"*70)\n",
        "    print(\"üìä Model Efficiency Metrics:\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"  Parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
        "    print(f\"  Model Size: {model_size_mb:.2f} MB\")\n",
        "    print(f\"  FLOPs per Forward: {flops_per_forward/1e9:.2f} GFLOPs\")\n",
        "    print(f\"  FLOPs per Sample: {flops_per_forward/avg_batch_size/1e9:.2f} GFLOPs\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "print(\"‚úÖ Model efficiency logging function loaded!\")\n",
        "print(\"‚ö†Ô∏è  Note: Call log_model_efficiency_metrics() in launch() after train_state initialization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu8IlFOp9pEe"
      },
      "source": [
        "## üìã Efficiency Metrics Integration Summary\n",
        "\n",
        "### ‚úÖ Added Content\n",
        "\n",
        "#### 1. **Efficiency Utility Functions** (Cell 12.5)\n",
        "- `get_gpu_memory_usage()`: Retrieve GPU memory usage\n",
        "- `estimate_model_flops()`: Estimate model FLOPs\n",
        "\n",
        "#### 2. **Modified Training Functions** (Cell 14)\n",
        "- `train_batch_efficient()`: Training function incorporating efficiency metrics\n",
        "- **New Metrics**:\n",
        "  - `train/batch_time`: Total time per batch\n",
        "  - `train/forward_time`: Forward pass time\n",
        "  - `train/backward_time`: Backward propagation time\n",
        "  - `train/optimizer_time`: Optimizer update time\n",
        "  - `train/samples_per_sec`: Training speed (samples/sec)\n",
        "  - `train/tokens_per_sec`: Token processing speed\n",
        "  - `train/gpu_memory_gb`: GPU memory usage\n",
        "  - `train/flops_per_sample`: FLOPs per sample\n",
        "  - `train/flops_per_sec`: FLOPs processing rate\n",
        "\n",
        "#### 3. **Modified Evaluation Function** (Cell 15)\n",
        "- `evaluate_efficient()`: Evaluation function incorporating efficiency metrics\n",
        "- **New Metrics**:\n",
        "  - `eval/avg_inference_steps`: Average inference steps\n",
        "  - `eval/inference_time_per_step`: Inference time per step\n",
        "  - `eval/total_eval_time`: Total evaluation time\n",
        "  - `eval/samples_per_sec`: Inference throughput\n",
        "  - `eval/gpu_memory_gb`: Inference GPU memory usage\n",
        "\n",
        "#### 4. **Model Structure Metrics Function** (Cell 16)\n",
        "- `log_model_efficiency_metrics()`: Records model structure efficiency metrics\n",
        "- **New Metrics**:\n",
        "  - `num_params`: Number of parameters (existing, enhanced)\n",
        "  - `model_size_mb`: Model size (MB)\n",
        "  - `estimated_flops_per_forward`: FLOPs per forward pass\n",
        "  - `estimated_flops_per_sample`: FLOPs per sample\n",
        "\n",
        "### üîß Usage Guide\n",
        "\n",
        "#### Step 1: Replace the function in Part 12's cell\n",
        "\n",
        "1. **Replace the `train_batch` function**:\n",
        "   - Locate the `def train_batch(...)` function in Part 12 cells\n",
        "   - Replace it with the `train_batch_efficient` function from Cell 14\n",
        "   - Alternatively, rename the function to `train_batch` (Remove the `_efficient` suffix)\n",
        "\n",
        "2. **Replace the `evaluate` function**:\n",
        "   - Locate the `def evaluate(...)` function in Part 12 cells\n",
        "   - Replace it with the `evaluate_efficient` function from Cell 15\n",
        "   - Alternatively, rename the function to `evaluate` (Remove the `_efficient` suffix)\n",
        "\n",
        "#### Step 2: Add model metric logging to the `launch()` function\n",
        "\n",
        "In the `launch()` function, locate this line:\n",
        "```python\n",
        "train_state = init_train_state(config, train_metadata, rank=RANK, world_size=WORLD_SIZE)\n",
        "```\n",
        "\n",
        "After this line, add:\n",
        "```python\n",
        "# Log model efficiency metrics\n",
        "if RANK == 0:\n",
        "    log_model_efficiency_metrics(config, train_state, train_metadata)\n",
        "```\n",
        "\n",
        "### üìä Viewing Metrics in WandB\n",
        "\n",
        "After training begins, you can observe the following in the WandB dashboard:\n",
        "\n",
        "**Training Metrics**:\n",
        "- `train/batch_time`, `train/forward_time`, `train/samples_per_sec`, etc.\n",
        "\n",
        "**Evaluation Metrics**:\n",
        "- `eval/avg_inference_steps`, `eval/inference_time_per_step`, etc.\n",
        "\n",
        "**Model Structure Metrics** (step=0):\n",
        "- `num_params`, `model_size_mb`, `estimated_flops_per_forward`, etc.\n",
        "\n",
        "### üéØ Comparing Efficiency Across Architectures\n",
        "\n",
        "These metrics enable easy comparison of:\n",
        "- **Different architectures** (Attention vs MLP)\n",
        "- **Different configurations** (H_cycles, L_cycles, L_layers)\n",
        "- **Different batch sizes**\n",
        "- **Different data augmentation strategies**\n",
        "\n",
        "All efficiency metrics are automatically logged to WandB for subsequent analysis and visualization!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3k9RqLK9pEe"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "# import torch.distributed as dist  # Removed for single GPU\n",
        "from torch.optim.optimizer import Optimizer, ParamsT\n",
        "\n",
        "# trunc_normal_init_ defined in Part 3\n",
        "\n",
        "\n",
        "class CastedSparseEmbedding(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, batch_size: int, init_std: float, cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "\n",
        "        # Real Weights\n",
        "        # Truncated LeCun normal init\n",
        "        self.weights = nn.Buffer(\n",
        "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std), persistent=True\n",
        "        )\n",
        "\n",
        "        # Local weights and IDs\n",
        "        # Local embeddings, with gradient, not persistent\n",
        "        self.local_weights = nn.Buffer(torch.zeros(batch_size, embedding_dim, requires_grad=True), persistent=False)\n",
        "        # Local embedding IDs, not persistent\n",
        "        self.local_ids = nn.Buffer(torch.zeros(batch_size, dtype=torch.int32), persistent=False)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training:\n",
        "            # Test mode, no gradient\n",
        "            return self.weights[inputs].to(self.cast_to)\n",
        "\n",
        "        # Training mode, fill puzzle embedding from weights\n",
        "        with torch.no_grad():\n",
        "            self.local_weights.copy_(self.weights[inputs])\n",
        "            self.local_ids.copy_(inputs)\n",
        "\n",
        "        return self.local_weights.to(self.cast_to)\n",
        "\n",
        "\n",
        "class CastedSparseEmbeddingSignSGD(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: ParamsT,\n",
        "\n",
        "        lr: Union[float, torch.Tensor] = 1e-3,\n",
        "        weight_decay: float = 1e-2,\n",
        "    ):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            world_size=1  # Single GPU, no distributed training\n",
        "        )\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad\n",
        "    def step(self, closure=None):  # type: ignore\n",
        "        for group in self.param_groups:\n",
        "            # Find the sparse embedding weights\n",
        "            local_weights_grad = None\n",
        "            local_ids = None\n",
        "            weights = None\n",
        "\n",
        "            assert len(group[\"params\"]) == 3\n",
        "            for p in group[\"params\"]:\n",
        "                if p.requires_grad:\n",
        "                    local_weights_grad = p.grad\n",
        "                elif p.ndim == 1:\n",
        "                    local_ids = p\n",
        "                elif p.ndim == 2:\n",
        "                    weights = p\n",
        "                else:\n",
        "                    assert False\n",
        "\n",
        "            assert local_ids is not None\n",
        "            assert weights is not None\n",
        "\n",
        "            # Apply SignSGD\n",
        "            # Adam ‚âà SignSGD if gradient is very sparse\n",
        "            if local_weights_grad is not None:\n",
        "                _sparse_emb_signsgd_dist(\n",
        "                    local_weights_grad,\n",
        "                    local_ids,\n",
        "                    weights,\n",
        "\n",
        "                    lr=group[\"lr\"],\n",
        "                    weight_decay=group[\"weight_decay\"],\n",
        "                    world_size=group[\"world_size\"]\n",
        "                )\n",
        "\n",
        "\n",
        "def _sparse_emb_signsgd_dist(\n",
        "    local_weights_grad: torch.Tensor,\n",
        "    local_ids: torch.Tensor,\n",
        "    weights: torch.Tensor,\n",
        "\n",
        "    lr: float,\n",
        "    weight_decay: float,\n",
        "    world_size: int = 1,  # Single GPU, not used but kept for compatibility\n",
        "    ) -> None:\n",
        "    N, D = local_weights_grad.shape\n",
        "\n",
        "    # All-gather\n",
        "    all_weights_grad = local_weights_grad\n",
        "    all_ids = local_ids\n",
        "\n",
        "    if False:  # Single GPU\n",
        "        all_weights_grad = torch.empty((world_size * N, D), dtype=local_weights_grad.dtype, device=local_weights_grad.device)\n",
        "        all_ids = torch.empty(world_size * N,               dtype=local_ids.dtype,          device=local_ids.device)\n",
        "\n",
        "        # dist.all_gather_into_tensor(all_weights_grad, local_weights_grad)  # Single GPU\n",
        "        dist.all_gather_into_tensor(all_ids,          local_ids)\n",
        "\n",
        "    # Unique\n",
        "    grad_ids, inv = all_ids.unique(return_inverse=True)\n",
        "\n",
        "    grad = torch.zeros((grad_ids.shape[0], D), dtype=all_weights_grad.dtype, device=all_weights_grad.device)\n",
        "    grad.scatter_add_(0, inv.unsqueeze(-1).expand(-1, D), all_weights_grad)\n",
        "\n",
        "    # SignSGD with decoupled weight decay\n",
        "    p = weights[grad_ids]\n",
        "\n",
        "    p.mul_(1.0 - lr * weight_decay).add_(torch.sign(grad), alpha=-lr)\n",
        "\n",
        "    # Write updated slices back\n",
        "    weights[grad_ids] = p\n",
        "\n",
        "\n",
        "# Distributed version (for compatibility with original code, but works for single GPU)\n",
        "class CastedSparseEmbeddingSignSGD_Distributed(Optimizer):\n",
        "    \"\"\"\n",
        "    Distributed version of CastedSparseEmbeddingSignSGD.\n",
        "    For single GPU, this is equivalent to CastedSparseEmbeddingSignSGD.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: ParamsT,\n",
        "        world_size: int,\n",
        "        lr: Union[float, torch.Tensor] = 1e-3,\n",
        "        weight_decay: float = 1e-2,\n",
        "    ):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            world_size=world_size\n",
        "        )\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad\n",
        "    def step(self, closure=None):  # type: ignore\n",
        "        for group in self.param_groups:\n",
        "            # Find the sparse embedding weights\n",
        "            local_weights_grad = None\n",
        "            local_ids = None\n",
        "            weights = None\n",
        "\n",
        "            assert len(group[\"params\"]) == 3\n",
        "            for p in group[\"params\"]:\n",
        "                if p.requires_grad:\n",
        "                    local_weights_grad = p.grad\n",
        "                elif p.ndim == 1:\n",
        "                    local_ids = p\n",
        "                elif p.ndim == 2:\n",
        "                    weights = p\n",
        "                else:\n",
        "                    assert False\n",
        "\n",
        "            assert local_ids is not None\n",
        "            assert weights is not None\n",
        "\n",
        "            # Apply SignSGD\n",
        "            if local_weights_grad is not None:\n",
        "                _sparse_emb_signsgd_dist(\n",
        "                    local_weights_grad,\n",
        "                    local_ids,\n",
        "                    weights,\n",
        "                    lr=group[\"lr\"],\n",
        "                    weight_decay=group[\"weight_decay\"],\n",
        "                    world_size=group[\"world_size\"]\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OihMx4Sg9pEf"
      },
      "source": [
        "## Part 6: Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QO5eJ9X9pEf"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Tuple, Dict, Sequence, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "IGNORE_LABEL_ID = -100\n",
        "\n",
        "\n",
        "def s(x, epsilon=1e-30):\n",
        "    return torch.where(\n",
        "        x<0,\n",
        "        1/(1-x+ epsilon),\n",
        "        x + 1\n",
        "    )\n",
        "\n",
        "\n",
        "def log_stablemax(x, dim=-1):\n",
        "    s_x = s(x)\n",
        "    return torch.log(s_x/torch.sum(s_x, dim=dim, keepdim=True))\n",
        "\n",
        "\n",
        "def stablemax_cross_entropy(logits, labels, ignore_index: int = -100, valid_mask=None):\n",
        "    logprobs = log_stablemax(logits.to(torch.float64), dim=-1)\n",
        "\n",
        "    if valid_mask is None:\n",
        "        valid_mask = (labels != ignore_index)\n",
        "    transformed_labels = torch.where(valid_mask, labels, 0)\n",
        "    prediction_logprobs = torch.gather(logprobs, index=transformed_labels.to(torch.long).unsqueeze(-1), dim=-1).squeeze(-1)\n",
        "\n",
        "    return -torch.where(valid_mask, prediction_logprobs, 0)\n",
        "\n",
        "\n",
        "def softmax_cross_entropy(logits, labels, ignore_index: int = -100):\n",
        "    # Cast logits to f32\n",
        "    # Flatten logits\n",
        "    return F.cross_entropy(logits.to(torch.float32).view(-1, logits.shape[-1]), labels.to(torch.long).view(-1), ignore_index=ignore_index, reduction=\"none\").view(labels.shape)\n",
        "\n",
        "\n",
        "class ACTLossHead(nn.Module):\n",
        "    def __init__(self, model: nn.Module, loss_type: str):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.loss_fn = globals()[loss_type]\n",
        "\n",
        "    def initial_carry(self, *args, **kwargs):\n",
        "        return self.model.initial_carry(*args, **kwargs)  # type: ignore\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        return_keys: Sequence[str],\n",
        "        # Model args\n",
        "        **model_kwargs,\n",
        "    ) -> Tuple[Any, torch.Tensor, Dict[str, torch.Tensor], Optional[Dict[str, torch.Tensor]], torch.Tensor]:\n",
        "        # Model logits\n",
        "        # B x SeqLen x D\n",
        "        new_carry, outputs = self.model(**model_kwargs)\n",
        "        labels = new_carry.current_data[\"labels\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Preds\n",
        "            outputs[\"preds\"] = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "\n",
        "            # Correctness\n",
        "            mask = (labels != IGNORE_LABEL_ID)\n",
        "            loss_counts = mask.sum(-1)\n",
        "            loss_divisor = loss_counts.clamp_min(1).unsqueeze(-1)  # Avoid NaNs in division\n",
        "\n",
        "            is_correct = mask & (torch.argmax(outputs[\"logits\"], dim=-1) == labels)\n",
        "            seq_is_correct = is_correct.sum(-1) == loss_counts\n",
        "\n",
        "            # Metrics (halted)\n",
        "            valid_metrics = new_carry.halted & (loss_counts > 0)\n",
        "            metrics = {\n",
        "                \"count\": valid_metrics.sum(),\n",
        "\n",
        "                \"accuracy\":       torch.where(valid_metrics, (is_correct.to(torch.float32) / loss_divisor).sum(-1), 0).sum(),\n",
        "                \"exact_accuracy\": (valid_metrics & seq_is_correct).sum(),\n",
        "\n",
        "                \"q_halt_accuracy\": (valid_metrics & ((outputs[\"q_halt_logits\"] >= 0) == seq_is_correct)).sum(),\n",
        "                \"steps\":          torch.where(valid_metrics, new_carry.steps, 0).sum(),\n",
        "            }\n",
        "\n",
        "        # Losses\n",
        "\n",
        "        lm_loss = (self.loss_fn(outputs[\"logits\"], labels, ignore_index=IGNORE_LABEL_ID, valid_mask=mask) / loss_divisor).sum()\n",
        "        q_halt_loss = F.binary_cross_entropy_with_logits(outputs[\"q_halt_logits\"], seq_is_correct.to(outputs[\"q_halt_logits\"].dtype), reduction=\"sum\")\n",
        "        metrics.update({\n",
        "            \"lm_loss\": lm_loss.detach(),\n",
        "            \"q_halt_loss\": q_halt_loss.detach(),\n",
        "        })\n",
        "        # Q continue (bootstrapping target loss); Alexia: This fits Q-learning, but seems totally unecessary\n",
        "        q_continue_loss = 0\n",
        "        if \"target_q_continue\" in outputs:\n",
        "            q_continue_loss = F.binary_cross_entropy_with_logits(outputs[\"q_continue_logits\"], outputs[\"target_q_continue\"], reduction=\"sum\")\n",
        "\n",
        "            metrics[\"q_continue_loss\"] = q_continue_loss.detach()\n",
        "        # Filter outputs for return\n",
        "        detached_outputs = {k: outputs[k].detach() for k in return_keys if k in outputs}\n",
        "\n",
        "        return new_carry, lm_loss + 0.5 * (q_halt_loss + q_continue_loss), metrics, detached_outputs, new_carry.halted.all()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuBw23QH9pEf"
      },
      "source": [
        "## Part 7: TRM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SujTqHwe9pEf"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "import torch\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from pydantic import BaseModel\n",
        "import random\n",
        "# trunc_normal_init_ defined in Part 3\n",
        "# All layer classes defined in Part 4\n",
        "# CastedSparseEmbedding defined in Part 5\n",
        "\n",
        "IGNORE_LABEL_ID = -100\n",
        "\n",
        "@dataclass\n",
        "class TinyRecursiveReasoningModel_ACTV1InnerCarry:\n",
        "    z_H: torch.Tensor\n",
        "    z_L: torch.Tensor\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TinyRecursiveReasoningModel_ACTV1Carry:\n",
        "    inner_carry: TinyRecursiveReasoningModel_ACTV1InnerCarry\n",
        "\n",
        "    steps: torch.Tensor\n",
        "    halted: torch.Tensor\n",
        "\n",
        "    current_data: Dict[str, torch.Tensor]\n",
        "\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1Config(BaseModel):\n",
        "    batch_size: int\n",
        "    seq_len: int\n",
        "    puzzle_emb_ndim: int = 0\n",
        "    num_puzzle_identifiers: int\n",
        "    vocab_size: int\n",
        "\n",
        "    H_cycles: int\n",
        "    L_cycles: int\n",
        "\n",
        "    H_layers: int # ignored\n",
        "    L_layers: int\n",
        "\n",
        "    # Transformer config\n",
        "    hidden_size: int\n",
        "    expansion: float\n",
        "    num_heads: int\n",
        "    pos_encodings: str\n",
        "\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    rope_theta: float = 10000.0\n",
        "\n",
        "    # Halting Q-learning config\n",
        "    halt_max_steps: int\n",
        "    halt_exploration_prob: float\n",
        "\n",
        "    forward_dtype: str = \"bfloat16\"\n",
        "\n",
        "    # Alexia: added\n",
        "    mlp_t: bool = False # use mlp on L instead of transformer\n",
        "    puzzle_emb_len: int = 16 # if non-zero, its specified to this value\n",
        "    no_ACT_continue: bool =  True # No continue ACT loss, only use the sigmoid of the halt which makes much more sense\n",
        "\n",
        "    # Dropout parameters\n",
        "    embedding_dropout: float = 0.0\n",
        "    attention_dropout: float = 0.0\n",
        "    hidden_dropout: float = 0.0\n",
        "    activation_dropout: float = 0.0\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1Block(nn.Module):\n",
        "    def __init__(self, config: TinyRecursiveReasoningModel_ACTV1Config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        # Dropout parameters\n",
        "        attention_dropout = getattr(config, 'attention_dropout', 0.0)\n",
        "        hidden_dropout = getattr(config, 'hidden_dropout', 0.0)\n",
        "        activation_dropout = getattr(config, 'activation_dropout', 0.0)\n",
        "\n",
        "        if self.config.mlp_t:\n",
        "            self.puzzle_emb_len = -(self.config.puzzle_emb_ndim // -self.config.hidden_size) if self.config.puzzle_emb_len == 0 else self.config.puzzle_emb_len\n",
        "            self.mlp_t = SwiGLU(\n",
        "                hidden_size=self.config.seq_len + self.puzzle_emb_len, # L\n",
        "                expansion=config.expansion,\n",
        "                activation_dropout=activation_dropout,  # Dropout parameters\n",
        "                hidden_dropout=hidden_dropout,      # Dropout parameters\n",
        "            )\n",
        "        else:\n",
        "            self.self_attn = Attention(\n",
        "                hidden_size=config.hidden_size,\n",
        "                head_dim=config.hidden_size // config.num_heads,\n",
        "                num_heads=config.num_heads,\n",
        "                num_key_value_heads=config.num_heads,\n",
        "                causal=False,\n",
        "                attention_dropout=attention_dropout,  # Dropout\n",
        "                hidden_dropout=hidden_dropout,     # Dropout\n",
        "            )\n",
        "        self.mlp = SwiGLU(\n",
        "            hidden_size=config.hidden_size,\n",
        "            expansion=config.expansion,\n",
        "            activation_dropout=activation_dropout,  # Dropout parameters\n",
        "            hidden_dropout=hidden_dropout,      # Dropout parameters\n",
        "        )\n",
        "        self.norm_eps = config.rms_norm_eps\n",
        "\n",
        "    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        # B, L, D = hidden_states.shape\n",
        "        # Post Norm\n",
        "        if self.config.mlp_t:\n",
        "            hidden_states = hidden_states.transpose(1,2)\n",
        "            out = self.mlp_t(hidden_states)\n",
        "            hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "            hidden_states = hidden_states.transpose(1,2)\n",
        "        else:\n",
        "            # Self Attention\n",
        "            attn_out = self.self_attn(cos_sin=cos_sin, hidden_states=hidden_states)\n",
        "            hidden_states = rms_norm(hidden_states + self.self_attn(cos_sin=cos_sin, hidden_states=hidden_states), variance_epsilon=self.norm_eps)\n",
        "        # Fully Connected\n",
        "        out = self.mlp(hidden_states)\n",
        "        hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "        return hidden_states\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1ReasoningModule(nn.Module):\n",
        "    def __init__(self, layers: List[TinyRecursiveReasoningModel_ACTV1Block]):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_injection: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "        hidden_states = hidden_states + input_injection\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states=hidden_states, **kwargs)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1_Inner(nn.Module):\n",
        "    def __init__(self, config: TinyRecursiveReasoningModel_ACTV1Config) -> None:\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.forward_dtype = getattr(torch, self.config.forward_dtype)\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(getattr(config, 'embedding_dropout', 0.0))  # Embedding dropout\n",
        "\n",
        "        # I/O\n",
        "\n",
        "        self.embed_scale = math.sqrt(self.config.hidden_size)\n",
        "        embed_init_std = 1.0 / self.embed_scale\n",
        "\n",
        "        self.embed_tokens = CastedEmbedding(self.config.vocab_size, self.config.hidden_size, init_std=embed_init_std, cast_to=self.forward_dtype)\n",
        "        self.lm_head      = CastedLinear(self.config.hidden_size, self.config.vocab_size, bias=False)\n",
        "        self.q_head       = CastedLinear(self.config.hidden_size, 2, bias=True)\n",
        "\n",
        "        self.puzzle_emb_len = -(self.config.puzzle_emb_ndim // -self.config.hidden_size)  if self.config.puzzle_emb_len == 0 else self.config.puzzle_emb_len  # ceil div\n",
        "        if self.config.puzzle_emb_ndim > 0:\n",
        "            # Zero init puzzle embeddings\n",
        "            self.puzzle_emb = CastedSparseEmbedding(self.config.num_puzzle_identifiers, self.config.puzzle_emb_ndim,\n",
        "                                                    batch_size=self.config.batch_size, init_std=0, cast_to=self.forward_dtype)\n",
        "\n",
        "        # LM Blocks\n",
        "        if self.config.pos_encodings == \"rope\":\n",
        "            self.rotary_emb = RotaryEmbedding(dim=self.config.hidden_size // self.config.num_heads,\n",
        "                                              max_position_embeddings=self.config.seq_len + self.puzzle_emb_len,\n",
        "                                              base=self.config.rope_theta)\n",
        "        elif self.config.pos_encodings == \"learned\":\n",
        "            self.embed_pos = CastedEmbedding(self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, init_std=embed_init_std, cast_to=self.forward_dtype)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        # Reasoning Layers\n",
        "        self.L_level = TinyRecursiveReasoningModel_ACTV1ReasoningModule(layers=[TinyRecursiveReasoningModel_ACTV1Block(self.config) for _i in range(self.config.L_layers)])\n",
        "\n",
        "        # Initial states\n",
        "        self.H_init = nn.Buffer(trunc_normal_init_(torch.empty(self.config.hidden_size, dtype=self.forward_dtype), std=1), persistent=True)\n",
        "        self.L_init = nn.Buffer(trunc_normal_init_(torch.empty(self.config.hidden_size, dtype=self.forward_dtype), std=1), persistent=True)\n",
        "\n",
        "        # Q head special init\n",
        "        # Init Q to (almost) zero for faster learning during bootstrapping\n",
        "        with torch.no_grad():\n",
        "            self.q_head.weight.zero_()\n",
        "            self.q_head.bias.fill_(-5)  # type: ignore\n",
        "\n",
        "    def _input_embeddings(self, input: torch.Tensor, puzzle_identifiers: torch.Tensor):\n",
        "        # Token embedding\n",
        "        embedding = self.embed_tokens(input.to(torch.int32))\n",
        "\n",
        "        # Puzzle embeddings\n",
        "        if self.config.puzzle_emb_ndim > 0:\n",
        "            puzzle_embedding = self.puzzle_emb(puzzle_identifiers)\n",
        "\n",
        "            pad_count = self.puzzle_emb_len * self.config.hidden_size - puzzle_embedding.shape[-1]\n",
        "            if pad_count > 0:\n",
        "                puzzle_embedding = F.pad(puzzle_embedding, (0, pad_count))\n",
        "\n",
        "            embedding = torch.cat((puzzle_embedding.view(-1, self.puzzle_emb_len, self.config.hidden_size), embedding), dim=-2)\n",
        "\n",
        "        # Position embeddings\n",
        "        if self.config.pos_encodings == \"learned\":\n",
        "            # scale by 1/sqrt(2) to maintain forward variance\n",
        "            embedding = 0.707106781 * (embedding + self.embed_pos.embedding_weight.to(self.forward_dtype))\n",
        "\n",
        "        # Scale\n",
        "        # Apply embedding dropout\n",
        "        embedding = self.embed_scale * embedding\n",
        "        embedding = self.embedding_dropout(embedding)  # [new]\n",
        "        return embedding\n",
        "\n",
        "    def empty_carry(self, batch_size: int):\n",
        "        return TinyRecursiveReasoningModel_ACTV1InnerCarry(\n",
        "            z_H=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype),\n",
        "            z_L=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype),\n",
        "        )\n",
        "\n",
        "    def reset_carry(self, reset_flag: torch.Tensor, carry: TinyRecursiveReasoningModel_ACTV1InnerCarry):\n",
        "        return TinyRecursiveReasoningModel_ACTV1InnerCarry(\n",
        "            z_H=torch.where(reset_flag.view(-1, 1, 1), self.H_init, carry.z_H),\n",
        "            z_L=torch.where(reset_flag.view(-1, 1, 1), self.L_init, carry.z_L),\n",
        "        )\n",
        "\n",
        "    def forward(self, carry: TinyRecursiveReasoningModel_ACTV1InnerCarry, batch: Dict[str, torch.Tensor]) -> Tuple[TinyRecursiveReasoningModel_ACTV1InnerCarry, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        seq_info = dict(\n",
        "            cos_sin=self.rotary_emb() if hasattr(self, \"rotary_emb\") else None,\n",
        "        )\n",
        "\n",
        "        # Input encoding\n",
        "        input_embeddings = self._input_embeddings(batch[\"inputs\"], batch[\"puzzle_identifiers\"])\n",
        "\n",
        "        # Forward iterations\n",
        "        it = 0\n",
        "        z_H, z_L = carry.z_H, carry.z_L\n",
        "        # H_cycles-1 without grad\n",
        "        with torch.no_grad():\n",
        "            for _H_step in range(self.config.H_cycles-1):\n",
        "                for _L_step in range(self.config.L_cycles):\n",
        "                    z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "                z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "        # 1 with grad\n",
        "        for _L_step in range(self.config.L_cycles):\n",
        "            z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "        z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "\n",
        "        # LM Outputs\n",
        "        new_carry = TinyRecursiveReasoningModel_ACTV1InnerCarry(z_H=z_H.detach(), z_L=z_L.detach())  # New carry no grad\n",
        "        output = self.lm_head(z_H)[:, self.puzzle_emb_len:]\n",
        "        q_logits = self.q_head(z_H[:, 0]).to(torch.float32) # Q-head; uses the first puzzle_emb position\n",
        "        return new_carry, output, (q_logits[..., 0], q_logits[..., 1])\n",
        "\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1(nn.Module):\n",
        "    \"\"\"ACT wrapper.\"\"\"\n",
        "\n",
        "    def __init__(self, config_dict: dict):\n",
        "        super().__init__()\n",
        "        self.config = TinyRecursiveReasoningModel_ACTV1Config(**config_dict)\n",
        "        self.inner = TinyRecursiveReasoningModel_ACTV1_Inner(self.config)\n",
        "\n",
        "    @property\n",
        "    def puzzle_emb(self):\n",
        "        return self.inner.puzzle_emb\n",
        "\n",
        "    def initial_carry(self, batch: Dict[str, torch.Tensor]):\n",
        "        batch_size = batch[\"inputs\"].shape[0]\n",
        "\n",
        "        return TinyRecursiveReasoningModel_ACTV1Carry(\n",
        "            inner_carry=self.inner.empty_carry(batch_size),  # Empty is expected, it will be reseted in first pass as all sequences are halted.\n",
        "\n",
        "            steps=torch.zeros((batch_size, ), dtype=torch.int32),\n",
        "            halted=torch.ones((batch_size, ), dtype=torch.bool),  # Default to halted\n",
        "\n",
        "            current_data={k: torch.empty_like(v) for k, v in batch.items()}\n",
        "        )\n",
        "\n",
        "    def forward(self, carry: TinyRecursiveReasoningModel_ACTV1Carry, batch: Dict[str, torch.Tensor]) -> Tuple[TinyRecursiveReasoningModel_ACTV1Carry, Dict[str, torch.Tensor]]:\n",
        "\n",
        "        # Update data, carry (removing halted sequences)\n",
        "        new_inner_carry = self.inner.reset_carry(carry.halted, carry.inner_carry)\n",
        "\n",
        "        new_steps = torch.where(carry.halted, 0, carry.steps)\n",
        "\n",
        "        new_current_data = {k: torch.where(carry.halted.view((-1, ) + (1, ) * (batch[k].ndim - 1)), batch[k], v) for k, v in carry.current_data.items()}\n",
        "\n",
        "        # Forward inner model\n",
        "        new_inner_carry, logits, (q_halt_logits, q_continue_logits) = self.inner(new_inner_carry, new_current_data)\n",
        "\n",
        "        outputs = {\n",
        "            \"logits\": logits,\n",
        "            \"q_halt_logits\": q_halt_logits,\n",
        "            \"q_continue_logits\": q_continue_logits\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Step\n",
        "            new_steps = new_steps + 1\n",
        "            is_last_step = new_steps >= self.config.halt_max_steps\n",
        "\n",
        "            halted = is_last_step\n",
        "\n",
        "            # if training, and ACT is enabled\n",
        "            if self.training and (self.config.halt_max_steps > 1):\n",
        "\n",
        "                # Halt signal\n",
        "                # NOTE: During evaluation, always use max steps, this is to guarantee the same halting steps inside a batch for batching purposes\n",
        "\n",
        "                if self.config.no_ACT_continue:\n",
        "                    halted = halted | (q_halt_logits > 0)\n",
        "                else:\n",
        "                    halted = halted | (q_halt_logits > q_continue_logits)\n",
        "\n",
        "                # Exploration\n",
        "                min_halt_steps = (torch.rand_like(q_halt_logits) < self.config.halt_exploration_prob) * torch.randint_like(new_steps, low=2, high=self.config.halt_max_steps + 1)\n",
        "                halted = halted & (new_steps >= min_halt_steps)\n",
        "\n",
        "                if not self.config.no_ACT_continue:\n",
        "                    # Compute target Q\n",
        "                    # NOTE: No replay buffer and target networks for computing target Q-value.\n",
        "                    # As batch_size is large, there're many parallel envs.\n",
        "                    # Similar concept as PQN https://arxiv.org/abs/2407.04811\n",
        "                    _, _, (next_q_halt_logits, next_q_continue_logits), _, _ = self.inner(new_inner_carry, new_current_data)\n",
        "                    outputs[\"target_q_continue\"] = torch.sigmoid(torch.where(is_last_step, next_q_halt_logits, torch.maximum(next_q_halt_logits, next_q_continue_logits)))\n",
        "\n",
        "        return TinyRecursiveReasoningModel_ACTV1Carry(new_inner_carry, new_steps, halted, new_current_data), outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy8c9mfM9pEf"
      },
      "source": [
        "## Part 8: Dataset Common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tePZqEl39pEf"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "import pydantic\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Global list mapping each dihedral transform id to its inverse.\n",
        "# Index corresponds to the original tid, and the value is its inverse.\n",
        "DIHEDRAL_INVERSE = [0, 3, 2, 1, 4, 5, 6, 7]\n",
        "\n",
        "\n",
        "class PuzzleDatasetMetadata(pydantic.BaseModel):\n",
        "    pad_id: int\n",
        "    ignore_label_id: Optional[int]\n",
        "    blank_identifier_id: int\n",
        "    vocab_size: int\n",
        "    seq_len: int\n",
        "    num_puzzle_identifiers: int\n",
        "    total_groups: int\n",
        "    mean_puzzle_examples: float\n",
        "    total_puzzles: int\n",
        "    sets: List[str]\n",
        "\n",
        "\n",
        "def dihedral_transform(arr: np.ndarray, tid: int) -> np.ndarray:\n",
        "    \"\"\"8 dihedral symmetries by rotate, flip and mirror\"\"\"\n",
        "\n",
        "    if tid == 0:\n",
        "        return arr  # identity\n",
        "    elif tid == 1:\n",
        "        return np.rot90(arr, k=1)\n",
        "    elif tid == 2:\n",
        "        return np.rot90(arr, k=2)\n",
        "    elif tid == 3:\n",
        "        return np.rot90(arr, k=3)\n",
        "    elif tid == 4:\n",
        "        return np.fliplr(arr)       # horizontal flip\n",
        "    elif tid == 5:\n",
        "        return np.flipud(arr)       # vertical flip\n",
        "    elif tid == 6:\n",
        "        return arr.T                # transpose (reflection along main diagonal)\n",
        "    elif tid == 7:\n",
        "        return np.fliplr(np.rot90(arr, k=1))  # anti-diagonal reflection\n",
        "    else:\n",
        "        return arr\n",
        "\n",
        "\n",
        "def inverse_dihedral_transform(arr: np.ndarray, tid: int) -> np.ndarray:\n",
        "    return dihedral_transform(arr, DIHEDRAL_INVERSE[tid])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkigvNnD9pEf"
      },
      "source": [
        "## Part 9: Puzzle Dataset (Non-distributed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1etWIbZI9pEf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import numpy as np\n",
        "import pydantic\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset, get_worker_info\n",
        "\n",
        "# IGNORE_LABEL_ID defined in Part 6\n",
        "# PuzzleDatasetMetadata defined in Part 8\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "def _sample_batch(rng: np.random.Generator, group_order: np.ndarray, puzzle_indices: np.ndarray, group_indices: np.ndarray, start_index: int, global_batch_size: int):\n",
        "    # Pack examples into a full batch\n",
        "    batch = []\n",
        "    batch_puzzle_indices = []\n",
        "    current_size = 0\n",
        "\n",
        "    while (start_index < group_order.size) and (current_size < global_batch_size):\n",
        "        # Pick a group and a puzzle from that group\n",
        "        group_id = group_order[start_index]\n",
        "        puzzle_id = rng.integers(group_indices[group_id], group_indices[group_id + 1])\n",
        "        start_index += 1\n",
        "\n",
        "        # Get range of the puzzle\n",
        "        puzzle_start = puzzle_indices[puzzle_id]\n",
        "        puzzle_size = int(puzzle_indices[puzzle_id + 1] - puzzle_start)\n",
        "\n",
        "        append_size = min(puzzle_size, global_batch_size - current_size)\n",
        "\n",
        "        # Put into batch\n",
        "        batch_puzzle_indices.append(np.full(append_size, puzzle_id, dtype=np.int32))\n",
        "        batch.append(puzzle_start + np.random.choice(puzzle_size, append_size, replace=False))\n",
        "\n",
        "        current_size += append_size\n",
        "\n",
        "    return start_index, np.concatenate(batch), np.concatenate(batch_puzzle_indices)\n",
        "\n",
        "\n",
        "class PuzzleDatasetConfig(pydantic.BaseModel):\n",
        "    seed: int\n",
        "    dataset_paths: List[str]\n",
        "    global_batch_size: int\n",
        "    test_set_mode: bool\n",
        "    epochs_per_iter: int  # Batch X epochs in an iteration to reduce overhead.\n",
        "    rank: int = 0  # Single GPU\n",
        "    num_replicas: int = 1  # Single GPU\n",
        "    shuffle_eval: bool = False  # Whether to shuffle evaluation batches\n",
        "    eval_seed: Optional[int] = None  # Seed for shuffling (changes each eval to get different random samples)\n",
        "class PuzzleDataset(IterableDataset):\n",
        "    def __init__(self, config: PuzzleDatasetConfig, split: str = \"train\"):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.split = split\n",
        "\n",
        "        # Merge multiple metadata\n",
        "        prev_seq_len = None\n",
        "        prev_vocab_size = None\n",
        "        prev_pad_id = None\n",
        "        prev_ignore_label_id = None\n",
        "        prev_blank_identifier_id = None\n",
        "        prev_sets = None\n",
        "        prev_num_identifiers = None\n",
        "        mean_puzzle_examples = 0\n",
        "        total_puzzles = 0\n",
        "        total_groups = 0\n",
        "        num_identifiers = 0\n",
        "        for dataset_path in config.dataset_paths:\n",
        "            current_metadata = self._load_metadata(dataset_path)\n",
        "            if prev_seq_len is None:\n",
        "                prev_seq_len = current_metadata.seq_len\n",
        "                prev_vocab_size = current_metadata.vocab_size\n",
        "                prev_pad_id = current_metadata.pad_id\n",
        "                prev_ignore_label_id = current_metadata.ignore_label_id\n",
        "                prev_blank_identifier_id = current_metadata.blank_identifier_id\n",
        "                prev_sets = current_metadata.sets\n",
        "                prev_num_identifiers = current_metadata.num_puzzle_identifiers\n",
        "            else:\n",
        "                assert prev_seq_len == current_metadata.seq_len\n",
        "                assert prev_vocab_size == current_metadata.vocab_size\n",
        "                assert prev_pad_id == current_metadata.pad_id\n",
        "                assert prev_ignore_label_id == current_metadata.ignore_label_id\n",
        "                assert prev_blank_identifier_id == current_metadata.blank_identifier_id\n",
        "                assert prev_sets == current_metadata.sets\n",
        "                assert prev_num_identifiers == current_metadata.num_puzzle_identifiers\n",
        "            mean_puzzle_examples += current_metadata.mean_puzzle_examples*current_metadata.total_puzzles\n",
        "            total_puzzles += current_metadata.total_puzzles\n",
        "            total_groups += current_metadata.total_groups\n",
        "            num_identifiers += current_metadata.num_puzzle_identifiers\n",
        "        mean_puzzle_examples = mean_puzzle_examples / total_puzzles\n",
        "\n",
        "        self.metadata = PuzzleDatasetMetadata(\n",
        "            seq_len=prev_seq_len,\n",
        "            vocab_size=prev_vocab_size,\n",
        "            pad_id=prev_pad_id,\n",
        "            ignore_label_id=prev_ignore_label_id,\n",
        "            blank_identifier_id=prev_blank_identifier_id,\n",
        "            num_puzzle_identifiers=num_identifiers,\n",
        "            total_groups=total_groups,\n",
        "            mean_puzzle_examples=mean_puzzle_examples,\n",
        "            total_puzzles=total_puzzles,\n",
        "            sets=prev_sets\n",
        "        )\n",
        "\n",
        "        # Checks\n",
        "        assert self.config.global_batch_size % self.config.num_replicas == 0, f\"Global batch size {self.config.global_batch_size} must be multiples of nodes {self.config.num_replicas}.\"\n",
        "        self.local_batch_size = self.config.global_batch_size // self.config.num_replicas\n",
        "\n",
        "        # State\n",
        "        self._data = None\n",
        "        self._iters = 0\n",
        "\n",
        "    def _load_metadata(self, dataset_path) -> PuzzleDatasetMetadata:\n",
        "        with open(os.path.join(dataset_path, self.split, \"dataset.json\"), \"r\") as f:\n",
        "            return PuzzleDatasetMetadata(**json.load(f))\n",
        "\n",
        "    def _lazy_load_dataset(self):\n",
        "        if self._data is not None:\n",
        "            return\n",
        "\n",
        "        field_mmap_modes = {\n",
        "            \"inputs\": \"r\",\n",
        "            \"labels\": \"r\",\n",
        "\n",
        "            # Keep indices in memory\n",
        "            \"puzzle_identifiers\": None,\n",
        "            \"puzzle_indices\": None,\n",
        "            \"group_indices\": None\n",
        "        }\n",
        "\n",
        "        # Load data\n",
        "        self._data = {}\n",
        "        for set_name in self.metadata.sets: # Load subset\n",
        "            for i, dataset_path in enumerate(self.config.dataset_paths):\n",
        "                if i > 0:\n",
        "                    set_name_ = set_name + str(i)\n",
        "                else:\n",
        "                    set_name_ = set_name\n",
        "                self._data[set_name_] = {\n",
        "                    field_name: np.load(os.path.join(dataset_path, self.split, f\"{set_name}__{field_name}.npy\"), mmap_mode=mmap_mode)\n",
        "                    for field_name, mmap_mode in field_mmap_modes.items()\n",
        "                }\n",
        "\n",
        "\n",
        "    def _collate_batch(self, batch):\n",
        "        # Convert dtype\n",
        "        batch = {k: v.astype(np.int32) for k, v in batch.items()}\n",
        "\n",
        "        # Convert ignore label IDs\n",
        "        if self.metadata.ignore_label_id is not None:\n",
        "            batch[\"labels\"][batch[\"labels\"] == self.metadata.ignore_label_id] = IGNORE_LABEL_ID\n",
        "\n",
        "        # Pad\n",
        "        if batch[\"puzzle_identifiers\"].size < self.local_batch_size:\n",
        "            pad_size = self.local_batch_size - batch[\"puzzle_identifiers\"].size\n",
        "            pad_values = {\n",
        "                \"inputs\": self.metadata.pad_id,\n",
        "                \"labels\": IGNORE_LABEL_ID,\n",
        "                \"puzzle_identifiers\": self.metadata.blank_identifier_id\n",
        "            }\n",
        "            batch = {k: np.pad(v, ((0, pad_size), ) + ((0, 0), ) * (v.ndim - 1), constant_values=pad_values[k]) for k, v in batch.items()}\n",
        "\n",
        "        # To tensor\n",
        "        return {k: torch.from_numpy(v) for k, v in batch.items()}\n",
        "\n",
        "    def _iter_test(self):\n",
        "        for set_i, (set_name, dataset) in enumerate(self._data.items()):  # type: ignore\n",
        "            total_examples = len(dataset[\"inputs\"])\n",
        "\n",
        "            # Calculate all batch start indices\n",
        "            batch_starts = list(range(0, total_examples, self.config.global_batch_size))\n",
        "\n",
        "            # If shuffle_eval is enabled, randomly shuffle batch order\n",
        "            if self.config.shuffle_eval:\n",
        "                # Use eval_seed if provided, otherwise use config seed + iters\n",
        "                eval_seed = self.config.eval_seed if self.config.eval_seed is not None else (self.config.seed + self._iters)\n",
        "                rng = np.random.Generator(np.random.Philox(seed=eval_seed))\n",
        "                rng.shuffle(batch_starts)\n",
        "                self._iters += 1  # Increment so next eval uses different seed\n",
        "\n",
        "            # Iterate over batches (in shuffled or original order)\n",
        "            for start_index in batch_starts:\n",
        "                # Compute indices\n",
        "                end_index = min(total_examples, start_index + self.config.global_batch_size)\n",
        "\n",
        "                local_start = start_index + 0  # Single GPU\n",
        "                local_end   = min(start_index + self.local_batch_size, end_index)  # Single GPU\n",
        "\n",
        "                # Get batch of examples, and also puzzle IDs\n",
        "                puzzle_indices = []\n",
        "                puzzle_index = np.searchsorted(dataset[\"puzzle_indices\"], local_start, side=\"right\") - 1\n",
        "                for i in range(local_start, local_end):\n",
        "                    while puzzle_index + 1 < len(dataset[\"puzzle_indices\"]) and i >= dataset[\"puzzle_indices\"][puzzle_index + 1]:\n",
        "                        puzzle_index += 1\n",
        "\n",
        "                    puzzle_indices.append(puzzle_index)\n",
        "\n",
        "                batch = self._collate_batch({\n",
        "                    \"inputs\": dataset[\"inputs\"][local_start: local_end],\n",
        "                    \"labels\": dataset[\"labels\"][local_start: local_end],\n",
        "                    \"puzzle_identifiers\": dataset[\"puzzle_identifiers\"][puzzle_indices]\n",
        "                })\n",
        "\n",
        "                yield set_name, batch, end_index - start_index\n",
        "\n",
        "    def _iter_train(self):\n",
        "        for set_name, dataset in self._data.items():  # type: ignore\n",
        "            # Increase epoch count\n",
        "            self._iters += 1\n",
        "\n",
        "            # Randomly shuffle groups\n",
        "            rng = np.random.Generator(np.random.Philox(seed=self.config.seed + self._iters))\n",
        "\n",
        "            group_order = np.concatenate([rng.permutation(dataset[\"group_indices\"].size - 1) for _i in range(self.config.epochs_per_iter)])\n",
        "            start_index = 0\n",
        "\n",
        "            while start_index < group_order.size:\n",
        "                start_index, batch_indices, batch_puzzle_indices = _sample_batch(\n",
        "                    rng,\n",
        "                    group_order=group_order,\n",
        "                    puzzle_indices=dataset[\"puzzle_indices\"],\n",
        "                    group_indices=dataset[\"group_indices\"],\n",
        "                    start_index=start_index,\n",
        "                    global_batch_size=self.config.global_batch_size,\n",
        "                )\n",
        "\n",
        "                # Select current rank and collate\n",
        "                global_effective_batch_size = batch_puzzle_indices.size  # Global effective batch size, excluding pads\n",
        "\n",
        "                # Drop last batch\n",
        "                if global_effective_batch_size < self.config.global_batch_size:\n",
        "                    break\n",
        "\n",
        "                batch_indices        = batch_indices       [0: self.local_batch_size]  # Single GPU\n",
        "                batch_puzzle_indices = batch_puzzle_indices[0: self.local_batch_size]  # Single GPU\n",
        "                batch = self._collate_batch({\n",
        "                    \"inputs\": dataset[\"inputs\"][batch_indices],\n",
        "                    \"labels\": dataset[\"labels\"][batch_indices],\n",
        "                    \"puzzle_identifiers\": dataset[\"puzzle_identifiers\"][batch_puzzle_indices]\n",
        "                })\n",
        "\n",
        "                yield set_name, batch, global_effective_batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        worker_info = get_worker_info()\n",
        "        assert worker_info is None or worker_info.num_workers == 1, \"Multithreaded data loading is not currently supported.\"\n",
        "\n",
        "        self._lazy_load_dataset()\n",
        "\n",
        "        # Iterate using specified mode\n",
        "        if self.config.test_set_mode:\n",
        "            yield from self._iter_test()\n",
        "        else:\n",
        "            yield from self._iter_train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_PPKEwr9pEf"
      },
      "source": [
        "## Part 10: EMA Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EHRAAAN9pEf"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch.nn as nn\n",
        "\n",
        "class EMAHelper(object):\n",
        "    def __init__(self, mu=0.999):\n",
        "        self.mu = mu\n",
        "        self.shadow = {}\n",
        "\n",
        "    def register(self, module):\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self, module):\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name].data = (1. - self.mu) * param.data + self.mu * self.shadow[name].data\n",
        "\n",
        "    def ema(self, module):\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data.copy_(self.shadow[name].data)\n",
        "\n",
        "    def ema_copy(self, module):\n",
        "        module_copy = copy.deepcopy(module)\n",
        "        self.ema(module_copy)\n",
        "        return module_copy\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.shadow\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.shadow = state_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX3svdQg9pEf"
      },
      "source": [
        "## Part 11: Utils Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf3JKyif9pEf"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "import inspect\n",
        "\n",
        "\n",
        "# Class registry for notebook environment\n",
        "_MODEL_CLASS_REGISTRY = {}\n",
        "\n",
        "def register_model_class(identifier: str, cls):\n",
        "    \"\"\"Register a model class for notebook environment.\"\"\"\n",
        "    _MODEL_CLASS_REGISTRY[identifier] = cls\n",
        "\n",
        "def load_model_class(identifier: str, prefix: str = \"models.\"):\n",
        "    \"\"\"Load model class from identifier. Works in notebook environment.\"\"\"\n",
        "    # Check registry first\n",
        "    if identifier in _MODEL_CLASS_REGISTRY:\n",
        "        return _MODEL_CLASS_REGISTRY[identifier]\n",
        "\n",
        "    module_path, class_name = identifier.split('@')\n",
        "\n",
        "    # Map common identifiers to class names\n",
        "    class_name_map = {\n",
        "        'TinyRecursiveReasoningModel_ACTV1': 'TinyRecursiveReasoningModel_ACTV1',\n",
        "        'ACTLossHead': 'ACTLossHead',\n",
        "    }\n",
        "\n",
        "    # Get from global namespace (notebook environment)\n",
        "    import sys\n",
        "    frame = sys._getframe(1)\n",
        "    globals_dict = frame.f_globals\n",
        "\n",
        "    # Try to find class in globals\n",
        "    if class_name in globals_dict:\n",
        "        cls = globals_dict[class_name]\n",
        "        if isinstance(cls, type):\n",
        "            return cls\n",
        "\n",
        "    # Fallback: try importing (for evaluators)\n",
        "    try:\n",
        "        if prefix.startswith('evaluators.'):\n",
        "            # For evaluators, try to import\n",
        "            module = importlib.import_module(prefix + module_path)\n",
        "            return getattr(module, class_name)\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    raise ValueError(f'Class {class_name} not found. Make sure all cells are executed in order.')\n",
        "\n",
        "def get_model_source_path(identifier: str, prefix: str = \"models.\"):\n",
        "    \"\"\"Get source path. In notebook, return None.\"\"\"\n",
        "    # In notebook environment, we don't have source files\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRLfHWvP9pEg"
      },
      "source": [
        "## Part 12: Training Framework (Non-distributed, H100 Optimized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD0N119K9pEg"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Part 12: Training Framework (Non-distributed, H100 Optimized)\n",
        "# ============================================================================\n",
        "\n",
        "# Configuration classes\n",
        "class LossConfig(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra='allow')\n",
        "    name: str\n",
        "\n",
        "class ArchConfig(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra='allow')\n",
        "    name: str\n",
        "    loss: LossConfig\n",
        "\n",
        "class EvaluatorConfig(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra=\"allow\")\n",
        "    name: str\n",
        "\n",
        "class PretrainConfig(pydantic.BaseModel):\n",
        "    # Config\n",
        "    arch: ArchConfig\n",
        "    # Data\n",
        "    data_paths: List[str]\n",
        "    data_paths_test: List[str] = []\n",
        "    # Evaluators\n",
        "    evaluators: List[EvaluatorConfig] = []\n",
        "\n",
        "    # Hyperparams\n",
        "    global_batch_size: int\n",
        "    epochs: int\n",
        "\n",
        "    lr: float\n",
        "    lr_min_ratio: float\n",
        "    lr_warmup_steps: int\n",
        "\n",
        "    weight_decay: float\n",
        "    beta1: float\n",
        "    beta2: float\n",
        "\n",
        "    # Puzzle embedding\n",
        "    puzzle_emb_lr: float\n",
        "    puzzle_emb_weight_decay: float\n",
        "\n",
        "    # Names\n",
        "    project_name: Optional[str] = None\n",
        "    run_name: Optional[str] = None\n",
        "    load_checkpoint: Optional[str] = None\n",
        "    checkpoint_path: Optional[str] = None\n",
        "\n",
        "    # Extras\n",
        "    seed: int = 0\n",
        "    checkpoint_every_eval: bool = False\n",
        "    eval_interval: Optional[int] = None\n",
        "    min_eval_interval: Optional[int] = 0\n",
        "    eval_save_outputs: List[str] = []\n",
        "    max_eval_batches: Optional[int] = None\n",
        "    shuffle_eval: bool = True  # Whether to randomly sample eval batches (default: True for random sampling)\n",
        "\n",
        "    ema: bool = False\n",
        "    ema_rate: float = 0.999\n",
        "    freeze_weights: bool = False\n",
        "\n",
        "@dataclass\n",
        "class TrainState:\n",
        "    model: nn.Module\n",
        "    optimizers: Sequence[torch.optim.Optimizer]\n",
        "    optimizer_lrs: Sequence[float]\n",
        "    carry: Any\n",
        "\n",
        "    step: int\n",
        "    total_steps: int\n",
        "\n",
        "def create_dataloader(config: PretrainConfig, split: str, rank: int = 0, world_size: int = 1, **kwargs):\n",
        "    dataset = PuzzleDataset(PuzzleDatasetConfig(\n",
        "        seed=config.seed,\n",
        "        dataset_paths=config.data_paths_test if len(config.data_paths_test)>0 and split==\"test\" else config.data_paths,\n",
        "        rank=rank,\n",
        "        num_replicas=world_size,\n",
        "        **kwargs\n",
        "    ), split=split)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=None,\n",
        "        num_workers=1,\n",
        "        prefetch_factor=8,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "    return dataloader, dataset.metadata\n",
        "\n",
        "def create_model(config: PretrainConfig, train_metadata: PuzzleDatasetMetadata, rank: int = 0, world_size: int = 1):\n",
        "    model_cfg = dict(\n",
        "        **config.arch.__pydantic_extra__,  # type: ignore\n",
        "        batch_size=config.global_batch_size // world_size,\n",
        "        vocab_size=train_metadata.vocab_size,\n",
        "        seq_len=train_metadata.seq_len,\n",
        "        num_puzzle_identifiers=train_metadata.num_puzzle_identifiers,\n",
        "        causal=False\n",
        "    )\n",
        "\n",
        "    # Instantiate model with loss head\n",
        "    model_cls = load_model_class(config.arch.name)\n",
        "    loss_head_cls = load_model_class(config.arch.loss.name)\n",
        "\n",
        "    with torch.device(\"cuda\"):\n",
        "        model: nn.Module = model_cls(model_cfg)\n",
        "        print(model)\n",
        "        model = loss_head_cls(model, **config.arch.loss.__pydantic_extra__)  # type: ignore\n",
        "        if \"DISABLE_COMPILE\" not in os.environ:\n",
        "            model = torch.compile(model)  # type: ignore\n",
        "\n",
        "        # Load checkpoint\n",
        "        if rank == 0:\n",
        "            load_checkpoint(model, config)\n",
        "\n",
        "    # Optimizers and lr (using AdamW instead of AdamAtan2)\n",
        "    if config.arch.puzzle_emb_ndim == 0:\n",
        "        optimizers = [\n",
        "            torch.optim.AdamW(\n",
        "                model.parameters(),\n",
        "                lr=0,  # Needs to be set by scheduler\n",
        "                weight_decay=config.weight_decay,\n",
        "                betas=(config.beta1, config.beta2),\n",
        "                eps=1e-8\n",
        "            )\n",
        "        ]\n",
        "        optimizer_lrs = [config.lr]\n",
        "    elif config.freeze_weights:\n",
        "        # For frozen weights, we still need an optimizer for puzzle_emb\n",
        "        # CastedSparseEmbeddingSignSGD_Distributed is defined in Part 5 (same cell)\n",
        "        optimizers = [\n",
        "            CastedSparseEmbeddingSignSGD_Distributed(\n",
        "                model.model.puzzle_emb.buffers(),  # type: ignore\n",
        "                lr=0,\n",
        "                weight_decay=config.puzzle_emb_weight_decay,\n",
        "                world_size=world_size\n",
        "            )\n",
        "        ]\n",
        "        optimizer_lrs = [config.puzzle_emb_lr]\n",
        "    else:\n",
        "        # CastedSparseEmbeddingSignSGD_Distributed is defined in Part 5 (same cell)\n",
        "        optimizers = [\n",
        "            CastedSparseEmbeddingSignSGD_Distributed(\n",
        "                model.model.puzzle_emb.buffers(),  # type: ignore\n",
        "                lr=0,\n",
        "                weight_decay=config.puzzle_emb_weight_decay,\n",
        "                world_size=world_size\n",
        "            ),\n",
        "            torch.optim.AdamW(\n",
        "                model.parameters(),\n",
        "                lr=0,\n",
        "                weight_decay=config.weight_decay,\n",
        "                betas=(config.beta1, config.beta2),\n",
        "                eps=1e-8\n",
        "            )\n",
        "        ]\n",
        "        optimizer_lrs = [config.puzzle_emb_lr, config.lr]\n",
        "\n",
        "    return model, optimizers, optimizer_lrs\n",
        "\n",
        "def cosine_schedule_with_warmup_lr_lambda(\n",
        "    current_step: int, *, base_lr: float, num_warmup_steps: int, num_training_steps: int, min_ratio: float = 0.0, num_cycles: float = 0.5\n",
        "):\n",
        "    if current_step < num_warmup_steps:\n",
        "        return base_lr * float(current_step) / float(max(1, num_warmup_steps))\n",
        "\n",
        "    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "    return base_lr * (min_ratio + max(0.0, (1 - min_ratio) * 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))))\n",
        "\n",
        "def init_train_state(config: PretrainConfig, train_metadata: PuzzleDatasetMetadata, rank: int = 0, world_size: int = 1):\n",
        "    # Estimated total training steps\n",
        "    total_steps = int(config.epochs * train_metadata.total_groups * train_metadata.mean_puzzle_examples / config.global_batch_size)\n",
        "\n",
        "    # Model\n",
        "    model, optimizers, optimizer_lrs = create_model(config, train_metadata, rank=rank, world_size=world_size)\n",
        "\n",
        "    return TrainState(\n",
        "        step=0,\n",
        "        total_steps=total_steps,\n",
        "        model=model,\n",
        "        optimizers=optimizers,\n",
        "        optimizer_lrs=optimizer_lrs,\n",
        "        carry=None\n",
        "    )\n",
        "\n",
        "def save_train_state(config: PretrainConfig, train_state: TrainState):\n",
        "    if config.checkpoint_path is None:\n",
        "        return\n",
        "\n",
        "    os.makedirs(config.checkpoint_path, exist_ok=True)\n",
        "    torch.save(train_state.model.state_dict(), os.path.join(config.checkpoint_path, f\"step_{train_state.step}\"))\n",
        "\n",
        "def load_checkpoint(model: nn.Module, config: PretrainConfig):\n",
        "    if config.load_checkpoint is not None:\n",
        "        print(f\"Loading checkpoint {config.load_checkpoint}\")\n",
        "        state_dict = torch.load(config.load_checkpoint, map_location=\"cuda\")\n",
        "\n",
        "        # Resize and reset puzzle emb if needed\n",
        "        puzzle_emb_name = \"_orig_mod.model.inner.puzzle_emb.weights\"\n",
        "        expected_shape: torch.Size = model.model.puzzle_emb.weights.shape  # type: ignore\n",
        "        if puzzle_emb_name in state_dict:\n",
        "            puzzle_emb = state_dict[puzzle_emb_name]\n",
        "            if puzzle_emb.shape != expected_shape:\n",
        "                print(f\"Resetting puzzle embedding as shape is different. Found {puzzle_emb.shape}, Expected {expected_shape}\")\n",
        "                state_dict[puzzle_emb_name] = (\n",
        "                    torch.mean(puzzle_emb, dim=0, keepdim=True).expand(expected_shape).contiguous()\n",
        "                )\n",
        "        model.load_state_dict(state_dict, assign=True)\n",
        "\n",
        "def compute_lr(base_lr: float, config: PretrainConfig, train_state: TrainState):\n",
        "    return cosine_schedule_with_warmup_lr_lambda(\n",
        "        current_step=train_state.step,\n",
        "        base_lr=base_lr,\n",
        "        num_warmup_steps=round(config.lr_warmup_steps),\n",
        "        num_training_steps=train_state.total_steps,\n",
        "        min_ratio=config.lr_min_ratio\n",
        "    )\n",
        "\n",
        "def compute_grad_norm(model: nn.Module) -> float:\n",
        "    \"\"\"\n",
        "    Compute the total gradient norm across all parameters.\n",
        "    Returns the L2 norm of all gradients.\n",
        "    \"\"\"\n",
        "    total_norm = 0.0\n",
        "    param_count = 0\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            param_norm = param.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "            param_count += 1\n",
        "    total_norm = total_norm ** (1. / 2)\n",
        "    return total_norm if param_count > 0 else 0.0\n",
        "\n",
        "def train_batch(config: PretrainConfig, train_state: TrainState, batch: Any, global_batch_size: int, rank: int = 0, world_size: int = 1):\n",
        "    train_state.step += 1\n",
        "    if train_state.step > train_state.total_steps:\n",
        "        return\n",
        "\n",
        "    # To device\n",
        "    batch = {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "    # Init carry if it is None\n",
        "    if train_state.carry is None:\n",
        "        with torch.device(\"cuda\"):\n",
        "            train_state.carry = train_state.model.initial_carry(batch)  # type: ignore\n",
        "\n",
        "    # Forward\n",
        "    train_state.carry, loss, metrics, _, _ = train_state.model(carry=train_state.carry, batch=batch, return_keys=[])\n",
        "\n",
        "    # Check for NaN or Inf in loss\n",
        "    loss_value = loss.item() if isinstance(loss, torch.Tensor) else loss\n",
        "    if not (torch.isfinite(loss) if isinstance(loss, torch.Tensor) else (math.isfinite(loss_value) if isinstance(loss_value, float) else True)):\n",
        "        print(f\"‚ö†Ô∏è WARNING: Step {train_state.step} - Loss is NaN or Inf: {loss_value}\")\n",
        "        return None\n",
        "\n",
        "    ((1 / global_batch_size) * loss).backward()\n",
        "\n",
        "    # Allreduce (single GPU, skip)\n",
        "    if False:  # Single GPU\n",
        "        pass\n",
        "\n",
        "    # Compute gradient norm for monitoring (no clipping, just recording)\n",
        "    grad_norm = compute_grad_norm(train_state.model)\n",
        "\n",
        "    # Apply optimizer\n",
        "    lr_this_step = None\n",
        "    for optim, base_lr in zip(train_state.optimizers, train_state.optimizer_lrs):\n",
        "        lr_this_step = compute_lr(base_lr, config, train_state)\n",
        "        for param_group in optim.param_groups:\n",
        "            param_group['lr'] = lr_this_step\n",
        "\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "    # Reduce metrics\n",
        "    if len(metrics):\n",
        "        assert not any(v.requires_grad for v in metrics.values())\n",
        "        metric_keys = list(sorted(metrics.keys()))\n",
        "        metric_values = torch.stack([metrics[k] for k in metric_keys])\n",
        "        if False:  # Single GPU\n",
        "            pass\n",
        "        if True:  # Single GPU, always rank 0\n",
        "            metric_values = metric_values.cpu().numpy()\n",
        "            reduced_metrics = {k: metric_values[i] for i, k in enumerate(metric_keys)}\n",
        "\n",
        "            # Postprocess\n",
        "            count = max(reduced_metrics[\"count\"], 1)\n",
        "            reduced_metrics = {f\"train/{k}\": v / (global_batch_size if k.endswith(\"loss\") else count) for k, v in reduced_metrics.items()}\n",
        "            reduced_metrics[\"train/lr\"] = lr_this_step\n",
        "\n",
        "            # Add gradient norm monitoring\n",
        "            if grad_norm is not None:\n",
        "                reduced_metrics[\"train/grad_norm\"] = float(grad_norm)\n",
        "\n",
        "            # Note: GPU/system stats are automatically logged by wandb (_disable_stats=False)\n",
        "            # No need to manually record them\n",
        "\n",
        "            return reduced_metrics\n",
        "\n",
        "def create_evaluators(config: PretrainConfig, eval_metadata: PuzzleDatasetMetadata) -> List[Any]:\n",
        "    data_paths = config.data_paths_test if len(config.data_paths_test)>0 else config.data_paths\n",
        "    evaluators = []\n",
        "    for cfg in config.evaluators:\n",
        "        for data_path in data_paths:\n",
        "            cls = load_model_class(cfg.name, \"evaluators.\")(\n",
        "                data_path=data_path, eval_metadata=eval_metadata, **cfg.__pydantic_extra__\n",
        "            )  # type: ignore\n",
        "            evaluators.append(cls)\n",
        "    return evaluators\n",
        "\n",
        "def evaluate(\n",
        "    config: PretrainConfig,\n",
        "    train_state: TrainState,\n",
        "    eval_loader: torch.utils.data.DataLoader,\n",
        "    eval_metadata: PuzzleDatasetMetadata,\n",
        "    evaluators: List[Any],\n",
        "    rank: int = 0,\n",
        "    world_size: int = 1,\n",
        "    cpu_group: Optional[Any] = None,\n",
        "):\n",
        "    reduced_metrics = None\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        return_keys = set(config.eval_save_outputs)\n",
        "        for evaluator in evaluators:\n",
        "            evaluator.begin_eval()\n",
        "            return_keys.update(evaluator.required_outputs)\n",
        "\n",
        "        # Run evaluation\n",
        "        set_ids = {k: idx for idx, k in enumerate(eval_metadata.sets)}\n",
        "        save_preds = {}\n",
        "        metric_keys = []\n",
        "        metric_values = None\n",
        "        carry = None\n",
        "        processed_batches = 0\n",
        "\n",
        "        for set_name, batch, global_batch_size in eval_loader:\n",
        "            if config.max_eval_batches is not None and processed_batches >= config.max_eval_batches:\n",
        "                break\n",
        "            processed_batches += 1\n",
        "            if rank == 0:\n",
        "                print(f\"Processing batch {processed_batches}: {set_name}\")\n",
        "\n",
        "            # To device\n",
        "            batch = {k: v.cuda() for k, v in batch.items()}\n",
        "            with torch.device(\"cuda\"):\n",
        "                carry = train_state.model.initial_carry(batch)  # type: ignore\n",
        "\n",
        "            # Forward\n",
        "            inference_steps = 0\n",
        "            while True:\n",
        "                carry, loss, metrics, preds, all_finish = train_state.model(\n",
        "                    carry=carry, batch=batch, return_keys=return_keys\n",
        "                )\n",
        "                inference_steps += 1\n",
        "                if all_finish:\n",
        "                    break\n",
        "\n",
        "            if rank == 0:\n",
        "                print(f\"  Completed inference in {inference_steps} steps\")\n",
        "\n",
        "            for collection in (batch, preds):\n",
        "                for k, v in collection.items():\n",
        "                    if k in config.eval_save_outputs:\n",
        "                        save_preds.setdefault(k, [])\n",
        "                        save_preds[k].append(v.cpu())\n",
        "\n",
        "            for evaluator in evaluators:\n",
        "                evaluator.update_batch(batch, preds)\n",
        "\n",
        "            del carry, loss, preds, batch, all_finish\n",
        "\n",
        "            # Aggregate metrics\n",
        "            set_id = set_ids[set_name]\n",
        "            if metric_values is None:\n",
        "                metric_keys = list(sorted(metrics.keys()))\n",
        "                metric_values = torch.zeros(\n",
        "                    (len(set_ids), len(metrics.values())), dtype=torch.float32, device=\"cuda\"\n",
        "                )\n",
        "            metric_values[set_id] += torch.stack([metrics[k] for k in metric_keys])\n",
        "            del metrics\n",
        "\n",
        "        # Concatenate save preds\n",
        "        save_preds = {k: torch.cat(v, dim=0) for k, v in save_preds.items()}\n",
        "\n",
        "        # Save preds\n",
        "        if config.checkpoint_path is not None and len(save_preds):\n",
        "            os.makedirs(os.path.dirname(config.checkpoint_path), exist_ok=True)\n",
        "            torch.save(\n",
        "                save_preds, os.path.join(config.checkpoint_path, f\"step_{train_state.step}_all_preds.{rank}\")\n",
        "            )\n",
        "        del save_preds\n",
        "\n",
        "        # Reduce to rank 0\n",
        "        if metric_values is not None:\n",
        "            if False:  # Single GPU\n",
        "                pass\n",
        "            if True:  # Single GPU, always rank 0\n",
        "                reduced_metrics = metric_values.cpu().numpy()\n",
        "                reduced_metrics = {\n",
        "                    set_name: {\n",
        "                        metric_name: reduced_metrics[set_id, metric_id]\n",
        "                        for metric_id, metric_name in enumerate(metric_keys)\n",
        "                    }\n",
        "                    for set_id, set_name in enumerate(set_ids)\n",
        "                }\n",
        "                # Postprocess\n",
        "                for set_name, m in reduced_metrics.items():\n",
        "                    count = m.pop(\"count\")\n",
        "                    reduced_metrics[set_name] = {k: v / count for k, v in m.items()}\n",
        "\n",
        "        # Run evaluators\n",
        "        if rank == 0:\n",
        "            print(f\"\\nRunning {len(evaluators)} evaluator(s)...\")\n",
        "        for i, evaluator in enumerate(evaluators):\n",
        "            if rank == 0:\n",
        "                print(f\"Running evaluator {i+1}/{len(evaluators)}: {evaluator.__class__.__name__}\")\n",
        "            evaluator_save_path = None\n",
        "            if config.checkpoint_path is not None:\n",
        "                evaluator_save_path = os.path.join(\n",
        "                    config.checkpoint_path,\n",
        "                    f\"evaluator_{evaluator.__class__.__name__}_step_{train_state.step}\",\n",
        "                )\n",
        "                os.makedirs(evaluator_save_path, exist_ok=True)\n",
        "            metrics = evaluator.result(evaluator_save_path, rank=rank, world_size=world_size, group=cpu_group)\n",
        "            if rank == 0 and metrics is not None:\n",
        "                if reduced_metrics is None:\n",
        "                    reduced_metrics = {}\n",
        "                reduced_metrics.update(metrics)\n",
        "                print(f\"  Completed {evaluator.__class__.__name__}\")\n",
        "        if rank == 0:\n",
        "            print(\"All evaluators completed!\")\n",
        "\n",
        "    return reduced_metrics\n",
        "\n",
        "def launch(config_dict: dict):\n",
        "    \"\"\"\n",
        "    Launch training with a configuration dictionary.\n",
        "    Single GPU, non-distributed version.\n",
        "    \"\"\"\n",
        "    RANK = 0\n",
        "    WORLD_SIZE = 1\n",
        "    CPU_PROCESS_GROUP = None\n",
        "\n",
        "    # Load config\n",
        "    config = PretrainConfig(**config_dict)\n",
        "\n",
        "    # Naming\n",
        "    if config.project_name is None:\n",
        "        config.project_name = \"TRM-A100-Sudoku\"\n",
        "    if config.run_name is None:\n",
        "        config.run_name = f\"{config.arch.name.split('@')[-1]} {coolname.generate_slug(2)}\"\n",
        "    if config.checkpoint_path is None:\n",
        "        config.checkpoint_path = os.path.join(\"checkpoints\", config.project_name, config.run_name)\n",
        "\n",
        "    # Seed RNGs\n",
        "    torch.random.manual_seed(config.seed + RANK)\n",
        "\n",
        "    # Dataset\n",
        "    train_epochs_per_iter = config.eval_interval if config.eval_interval is not None else config.epochs\n",
        "    total_iters = config.epochs // train_epochs_per_iter\n",
        "    assert config.epochs % train_epochs_per_iter == 0, \"Eval interval must be a divisor of total epochs.\"\n",
        "\n",
        "    train_loader, train_metadata = create_dataloader(\n",
        "        config, \"train\", test_set_mode=False, epochs_per_iter=train_epochs_per_iter,\n",
        "        global_batch_size=config.global_batch_size, rank=RANK, world_size=WORLD_SIZE\n",
        "    )\n",
        "    try:\n",
        "        eval_loader, eval_metadata = create_dataloader(\n",
        "            config, \"test\", test_set_mode=True, epochs_per_iter=1,\n",
        "            global_batch_size=config.global_batch_size, rank=RANK, world_size=WORLD_SIZE,\n",
        "            shuffle_eval=config.shuffle_eval  # Enable random batch sampling for evaluation\n",
        "        )\n",
        "    except:\n",
        "        print(\"NO EVAL DATA FOUND\")\n",
        "        eval_loader = eval_metadata = None\n",
        "\n",
        "    try:\n",
        "        evaluators = create_evaluators(config, eval_metadata)\n",
        "    except:\n",
        "        print(\"No evaluator found\")\n",
        "        evaluators = []\n",
        "\n",
        "    # Train state\n",
        "    train_state = init_train_state(config, train_metadata, rank=RANK, world_size=WORLD_SIZE)\n",
        "\n",
        "    # Progress bar and logger\n",
        "    progress_bar = None\n",
        "    ema_helper = None\n",
        "    if RANK == 0:\n",
        "        progress_bar = tqdm(total=train_state.total_steps)\n",
        "        wandb.init(\n",
        "            project=config.project_name,\n",
        "            name=config.run_name,\n",
        "            config=config.model_dump(),\n",
        "            settings=wandb.Settings(_disable_stats=False)  # Enable automatic system stats\n",
        "        )\n",
        "        wandb.log({\"num_params\": sum(x.numel() for x in train_state.model.parameters())}, step=0)\n",
        "    if config.ema:\n",
        "        print('Setup EMA')\n",
        "        # EMAHelper is defined in Part 10 (cell 10) of this notebook\n",
        "        # No need to import from models.ema - it's already in the global namespace\n",
        "        ema_helper = EMAHelper(mu=config.ema_rate)\n",
        "        ema_helper.register(train_state.model)\n",
        "\n",
        "    # ============================================================================\n",
        "    # Log Model Efficiency Metrics at Start\n",
        "    # ============================================================================\n",
        "    if RANK == 0:\n",
        "        log_model_efficiency_metrics(config, train_state, train_metadata)\n",
        "\n",
        "    # Training Loop\n",
        "    for _iter_id in range(total_iters):\n",
        "        print(f\"Epoch {_iter_id * train_epochs_per_iter}\")\n",
        "\n",
        "        # Train Iter\n",
        "        if RANK == 0:\n",
        "            print(\"TRAIN\")\n",
        "        train_state.model.train()\n",
        "        for set_name, batch, global_batch_size in train_loader:\n",
        "            # Use efficiency-tracked train_batch function\n",
        "            metrics = train_batch_efficient(config, train_state, batch, global_batch_size, rank=RANK, world_size=WORLD_SIZE)\n",
        "            if RANK == 0 and metrics is not None:\n",
        "                wandb.log(metrics, step=train_state.step)\n",
        "                progress_bar.update(train_state.step - progress_bar.n)  # type: ignore\n",
        "            if config.ema:\n",
        "                ema_helper.update(train_state.model)\n",
        "\n",
        "        if _iter_id >= config.min_eval_interval:\n",
        "            # Evaluation\n",
        "            if RANK == 0:\n",
        "                print(\"EVALUATE\")\n",
        "            if config.ema:\n",
        "                print(\"SWITCH TO EMA\")\n",
        "                train_state_eval = copy.deepcopy(train_state)\n",
        "                train_state_eval.model = ema_helper.ema_copy(train_state_eval.model)\n",
        "            else:\n",
        "                train_state_eval = train_state\n",
        "            train_state_eval.model.eval()\n",
        "            # Use efficiency-tracked evaluate function\n",
        "            metrics = evaluate_efficient(\n",
        "                config, train_state_eval, eval_loader, eval_metadata, evaluators,\n",
        "                rank=RANK, world_size=WORLD_SIZE, cpu_group=CPU_PROCESS_GROUP\n",
        "            )\n",
        "            if RANK == 0 and metrics is not None:\n",
        "                wandb.log(metrics, step=train_state.step)\n",
        "\n",
        "            # Checkpointing\n",
        "            if RANK == 0:\n",
        "                print(\"SAVE CHECKPOINT\")\n",
        "            if RANK == 0 and (config.checkpoint_every_eval or (_iter_id == total_iters - 1)):\n",
        "                save_train_state(config, train_state_eval)\n",
        "            if config.ema:\n",
        "                del train_state_eval\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1FnKYMO9pEg"
      },
      "source": [
        "## Part 13: Example Usage & Dataset Preparation\n",
        "\n",
        "### üì¶ Dataset Preparation\n",
        "\n",
        "Before training, you need to prepare your dataset. For **Sudoku-Extreme** dataset:\n",
        "\n",
        "```bash\n",
        "# Build Sudoku dataset (1000 examples, 1000 augmentations)\n",
        "python TinyRecursiveModels/dataset/build_sudoku_dataset.py \\\n",
        "  --output-dir data/sudoku-extreme-1k-aug-1000 \\\n",
        "  --subsample-size 1000 \\\n",
        "  --num-aug 1000\n",
        "```\n",
        "\n",
        "This will create the dataset in `data/sudoku-extreme-1k-aug-1000/` directory.\n",
        "\n",
        "**Note:** Make sure you have the `TinyRecursiveModels` folder in your workspace, or adjust the path accordingly.\n",
        "\n",
        "### ‚öôÔ∏è Configuration Options\n",
        "\n",
        "Below are three pre-configured setups ready to use:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joG-yjZE9pEg"
      },
      "source": [
        "## Configuration 4: Sudoku-Extreme Minimal Baseline\n",
        " Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWdD6LBA9pEg",
        "outputId": "97d42d27-5826-46dc-87d4-0d29a502bea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "‚úÖ Current configuration: Sudoku (Attention)\n",
            "üìÅ Data path: ['data/sudoku-extreme-1k-aug-1000']\n",
            "üîÑ Epochs: 1000, Eval interval: 100\n",
            "üìä Batch size: 256\n",
            "üéØ Architecture: Attention (Transformer)\n",
            "üíæ EMA: Enabled\n",
            "üöÄ To start training, call: launch(example_config)\n",
            "  Or use: launch(sudoku_att_config), launch(sudoku_mlp_config), or launch(sudoku_test_config)\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------------------\n",
        "# Configuration 4: Sudoku-Extreme Minimal Test Configuration üß™\n",
        "# For quick testing - reduced epochs and batch size\n",
        "# ----------------------------------------------------------------------------\n",
        "sudoku_config_Attn_A100 = {\n",
        "    'arch': {\n",
        "        'name': 'recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1',\n",
        "        'loss': {\n",
        "            'name': 'losses@ACTLossHead',\n",
        "            'loss_type': 'stablemax_cross_entropy'\n",
        "        },\n",
        "        'halt_exploration_prob': 0.1,\n",
        "        'halt_max_steps': 16,\n",
        "        'H_cycles': 3,\n",
        "        'L_cycles': 6,\n",
        "        'H_layers': 0,\n",
        "        'L_layers': 2,\n",
        "        'hidden_size': 512,\n",
        "        'num_heads': 8,\n",
        "        'expansion': 4,\n",
        "        'puzzle_emb_ndim': 512,\n",
        "        'pos_encodings': 'rope',\n",
        "        'forward_dtype': 'bfloat16',\n",
        "        'mlp_t': False,\n",
        "        'puzzle_emb_len': 1,\n",
        "        'no_ACT_continue': True,\n",
        "\n",
        "        'embedding_dropout': 0.0,\n",
        "        'attention_dropout': 0.0,\n",
        "        'hidden_dropout': 0.0,\n",
        "        'activation_dropout': 0.0,\n",
        "    },\n",
        "    'data_paths': ['data/sudoku-extreme-1k-aug-1000'],\n",
        "    'data_paths_test': [],\n",
        "    'evaluators': [],\n",
        "    'global_batch_size': 256,  # Reduced for testing\n",
        "    'epochs': 1000,  # Minimal epochs for quick test\n",
        "    'eval_interval': 100,  # Evaluate every 100 epochs\n",
        "    'checkpoint_every_eval': True,\n",
        "    'lr': 1e-4,\n",
        "    'lr_min_ratio': 1.0,\n",
        "    'lr_warmup_steps': 100,  # Reduced warmup\n",
        "    'beta1': 0.9,\n",
        "    'beta2': 0.95,\n",
        "    'weight_decay': 1,\n",
        "    'puzzle_emb_weight_decay': 1,\n",
        "    'puzzle_emb_lr': 1e-4,\n",
        "    'seed': 0,\n",
        "    'min_eval_interval': 0,\n",
        "    'max_eval_batches': 200,  # Limit to 50 batches for faster evaluation (~12,800 examples with batch_size=256)\n",
        "    'shuffle_eval': True,\n",
        "\t'ema': True,\n",
        "    'ema_rate': 0.999,\n",
        "    'freeze_weights': False,\n",
        "    'project_name': 'TRM-A100-Dropout-Sensitivity-eval' , # Wandb project name\n",
        "\t'run_name': 'Sudoku-Extreme-Baseline' # Wandb run name\n",
        "}\n",
        "\n",
        "\n",
        "# Default to Sudoku Attention version\n",
        "example_config = sudoku_config_Attn_A100\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "config_name = 'Sudoku (Attention)'\n",
        "print(f\"‚úÖ Current configuration: {config_name}\")\n",
        "print(f\"üìÅ Data path: {example_config['data_paths']}\")\n",
        "print(f\"üîÑ Epochs: {example_config['epochs']}, Eval interval: {example_config['eval_interval']}\")\n",
        "print(f\"üìä Batch size: {example_config['global_batch_size']}\")\n",
        "print(f\"üéØ Architecture: {'MLP' if example_config['arch']['mlp_t'] else 'Attention (Transformer)'}\")\n",
        "print(f\"üíæ EMA: {'Enabled' if example_config['ema'] else 'Disabled'}\")\n",
        "print(\"üöÄ To start training, call: launch(example_config)\")\n",
        "print(\"  Or use: launch(sudoku_att_config), launch(sudoku_mlp_config), or launch(sudoku_test_config)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0XII_VX9pEh"
      },
      "source": [
        "## build dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726,
          "referenced_widgets": [
            "ccd4de31909b455b8667eef0e8e33916",
            "7a9981eff1464bcf856eb0a353f597a6",
            "7f79fe0c96d84d6cac026c3d53e95075",
            "b8143ed3e3df45f389166ea31eda5ab9",
            "1198bdbd8f7f4733b846de7b4e1b08aa",
            "301baf207ca64be880f74ef8d79555eb",
            "a1f0bccc9d1f4306ae9d2e0b6cf07f7a",
            "5211e729514f445db45bcd7f3976752b",
            "ff5ad1923e7341acaf044548e181856d",
            "8be8228bdece49cc83a6615d78a2ee55",
            "b855873aaeac4bc1b7fdeff97ed86c15",
            "8aef680f119e4fbba5f62af4638b9bcc",
            "14695b7240794c43aee23e84ab6a8184",
            "2e6da288e4cd4ea5ae402aab7c0a6d3a",
            "0fc83881bfd448dd9d22f7e7e5781729",
            "71338241f3f846d29cceda6ca43aab1d",
            "7326fdbc18a54fabb4ae2b562b4247a6",
            "bebb0503536945d195df4659f2431eba",
            "701e5d3b96864f7fb2d355ea7ef23cc2",
            "be1683a24c3d45bd9e6801d1799b13bb",
            "6f0fcdbfb57f49cabed6c17d3647f8f7",
            "ce57a7185c18445ebfa045fd9e667206"
          ]
        },
        "id": "iwfzh6YK9pEh",
        "outputId": "6c7f2bf8-8923-418e-ad4f-7fba2e267480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Warning: Could not login to Hugging Face Hub: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `hf auth login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.\n",
            "   Continuing with token in environment variable...\n",
            "======================================================================\n",
            "üì¶ Building Sudoku Dataset\n",
            "======================================================================\n",
            "Source: sapientinc/sudoku-extreme\n",
            "Output: data/sudoku-extreme-1k-aug-1000\n",
            "Train subsample: 1000\n",
            "Augmentation: 1000\n",
            "======================================================================\n",
            "\n",
            "üì• Processing train set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train.csv:   0%|          | 0.00/719M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccd4de31909b455b8667eef0e8e33916"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading CSV: 3831994it [00:34, 112297.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 3831994 puzzles\n",
            "  Subsampled to 1000 puzzles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Augmenting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [02:19<00:00,  7.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úÖ Saved to data/sudoku-extreme-1k-aug-1000/train\n",
            "  üìä Total examples: 1001000\n",
            "\n",
            "üì• Processing test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test.csv:   0%|          | 0.00/79.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8aef680f119e4fbba5f62af4638b9bcc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading CSV: 422786it [00:03, 115982.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded 422786 puzzles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Augmenting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 422786/422786 [00:00<00:00, 1817095.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úÖ Saved to data/sudoku-extreme-1k-aug-1000/test\n",
            "  üìä Total examples: 422786\n",
            "\n",
            "======================================================================\n",
            "‚úÖ Dataset build complete!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Check and build dataset if needed\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm  import tqdm\n",
        "from huggingface_hub import hf_hub_download, login\n",
        "import warnings\n",
        "\n",
        "# Set Hugging Face Token for authentication\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "# Login to Hugging Face Hub\n",
        "try:\n",
        "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "    print(\"‚úÖ Successfully authenticated with Hugging Face Hub\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Warning: Could not login to Hugging Face Hub: {e}\")\n",
        "    print(\"   Continuing with token in environment variable...\")\n",
        "\n",
        "DATASET_DIR = \"data/sudoku-extreme-1k-aug-1000\"\n",
        "TRAIN_SUBSAMPLE_SIZE = 1000\n",
        "NUM_AUG = 1000  # Augmentation count\n",
        "MIN_DIFFICULTY = None  # Optional: filter by minimum difficulty rating\n",
        "\n",
        "def shuffle_sudoku(board: np.ndarray, solution: np.ndarray):\n",
        "    \"\"\"Apply equivalent transformations to Sudoku (preserves validity)\"\"\"\n",
        "    # Digit mapping: random permutation of 1-9\n",
        "    digit_map = np.pad(np.random.permutation(np.arange(1, 10)), (1, 0))\n",
        "\n",
        "    # Random transpose\n",
        "    transpose_flag = np.random.rand() < 0.5\n",
        "\n",
        "    # Row permutation: shuffle 3 bands, then shuffle rows within each band\n",
        "    bands = np.random.permutation(3)\n",
        "    row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
        "\n",
        "    # Column permutation: same for columns\n",
        "    stacks = np.random.permutation(3)\n",
        "    col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
        "\n",
        "    # Build 81->81 position mapping\n",
        "    mapping = np.array([row_perm[i // 9] * 9 + col_perm[i % 9] for i in range(81)])\n",
        "\n",
        "    def apply_transformation(x: np.ndarray) -> np.ndarray:\n",
        "        if transpose_flag:\n",
        "            x = x.T\n",
        "        new_board = x.flatten()[mapping].reshape(9, 9).copy()\n",
        "        return digit_map[new_board]\n",
        "\n",
        "    return apply_transformation(board), apply_transformation(solution)\n",
        "\n",
        "def convert_subset(set_name: str):\n",
        "    \"\"\"Process train or test set\"\"\"\n",
        "    print(f\"\\nüì• Processing {set_name} set...\")\n",
        "\n",
        "    # Download CSV from HuggingFace\n",
        "    # Use HF_TOKEN from environment variable for authentication\n",
        "    csv_path = hf_hub_download(\"sapientinc/sudoku-extreme\", f\"{set_name}.csv\", repo_type=\"dataset\", token=HF_TOKEN)\n",
        "\n",
        "    # Read CSV\n",
        "    inputs, labels = [], []\n",
        "    with open(csv_path, newline=\"\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)  # Skip header\n",
        "        for source, q, a, rating in tqdm(reader, desc=\"Reading CSV\"):\n",
        "            # Filter by difficulty if specified (matching original implementation)\n",
        "            if (MIN_DIFFICULTY is None) or (int(rating) >= MIN_DIFFICULTY):\n",
        "                assert len(q) == 81 and len(a) == 81\n",
        "                inputs.append(np.frombuffer(q.replace('.', '0').encode(), dtype=np.uint8).reshape(9, 9) - ord('0'))\n",
        "                labels.append(np.frombuffer(a.encode(), dtype=np.uint8).reshape(9, 9) - ord('0'))\n",
        "\n",
        "    print(f\"  Loaded {len(inputs)} puzzles\")\n",
        "\n",
        "    # Dataset subsampling (only for train set)\n",
        "    if set_name == \"train\" and TRAIN_SUBSAMPLE_SIZE is not None and TRAIN_SUBSAMPLE_SIZE < len(inputs):\n",
        "        indices = np.random.choice(len(inputs), size=TRAIN_SUBSAMPLE_SIZE, replace=False)\n",
        "        inputs = [inputs[i] for i in indices]\n",
        "        labels = [labels[i] for i in indices]\n",
        "        print(f\"  Subsampled to {len(inputs)} puzzles\")\n",
        "\n",
        "    # Data augmentation (only for train set)\n",
        "    num_augments = NUM_AUG if set_name == \"train\" else 0\n",
        "\n",
        "    # Build results\n",
        "    results = {k: [] for k in [\"inputs\", \"labels\", \"puzzle_identifiers\", \"puzzle_indices\", \"group_indices\"]}\n",
        "    puzzle_id = 0\n",
        "    example_id = 0\n",
        "\n",
        "    results[\"puzzle_indices\"].append(0)\n",
        "    results[\"group_indices\"].append(0)\n",
        "\n",
        "    for orig_inp, orig_out in tqdm(zip(inputs, labels), total=len(inputs), desc=\"Augmenting\"):\n",
        "        for aug_idx in range(1 + num_augments):\n",
        "            if aug_idx == 0:\n",
        "                inp, out = orig_inp, orig_out\n",
        "            else:\n",
        "                inp, out = shuffle_sudoku(orig_inp, orig_out)\n",
        "\n",
        "            results[\"inputs\"].append(inp)\n",
        "            results[\"labels\"].append(out)\n",
        "            example_id += 1\n",
        "            puzzle_id += 1\n",
        "\n",
        "            results[\"puzzle_indices\"].append(example_id)\n",
        "            results[\"puzzle_identifiers\"].append(0)\n",
        "\n",
        "        results[\"group_indices\"].append(puzzle_id)\n",
        "\n",
        "    # Convert to NumPy arrays\n",
        "    def seq_to_numpy(seq):\n",
        "        arr = np.concatenate(seq).reshape(len(seq), -1)\n",
        "        assert np.all((arr >= 0) & (arr <= 9))\n",
        "        return arr + 1  # Offset +1, 0 reserved for PAD\n",
        "\n",
        "    results = {\n",
        "        \"inputs\": seq_to_numpy(results[\"inputs\"]),\n",
        "        \"labels\": seq_to_numpy(results[\"labels\"]),\n",
        "        \"group_indices\": np.array(results[\"group_indices\"], dtype=np.int32),\n",
        "        \"puzzle_indices\": np.array(results[\"puzzle_indices\"], dtype=np.int32),\n",
        "        \"puzzle_identifiers\": np.array(results[\"puzzle_identifiers\"], dtype=np.int32),\n",
        "    }\n",
        "\n",
        "    # Metadata (matching original implementation exactly)\n",
        "    metadata = PuzzleDatasetMetadata(\n",
        "        seq_len=81,\n",
        "        vocab_size=10 + 1,  # PAD + \"0\" ... \"9\" (matching original)\n",
        "        pad_id=0,\n",
        "        ignore_label_id=0,\n",
        "        blank_identifier_id=0,\n",
        "        num_puzzle_identifiers=1,\n",
        "        total_groups=len(results[\"group_indices\"]) - 1,\n",
        "        mean_puzzle_examples=1,  # Fixed value as in original (even with augmentation)\n",
        "        total_puzzles=len(results[\"group_indices\"]) - 1,\n",
        "        sets=[\"all\"]\n",
        "    )\n",
        "\n",
        "    # Save\n",
        "    save_dir = os.path.join(DATASET_DIR, set_name)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    with open(os.path.join(save_dir, \"dataset.json\"), \"w\") as f:\n",
        "        json.dump(metadata.model_dump(), f)  # No indent to match original\n",
        "\n",
        "    for k, v in results.items():\n",
        "        np.save(os.path.join(save_dir, f\"all__{k}.npy\"), v)\n",
        "\n",
        "    print(f\"  ‚úÖ Saved to {save_dir}\")\n",
        "    print(f\"  üìä Total examples: {results['inputs'].shape[0]}\")\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# Check if dataset exists\n",
        "train_metadata_path = os.path.join(DATASET_DIR, \"train\", \"dataset.json\")\n",
        "test_metadata_path = os.path.join(DATASET_DIR, \"test\", \"dataset.json\")\n",
        "\n",
        "if os.path.exists(train_metadata_path) and os.path.exists(test_metadata_path):\n",
        "    print(\"=\"*70)\n",
        "    print(\"‚úÖ Dataset already exists!\")\n",
        "    print(f\"üìÅ Path: {DATASET_DIR}\")\n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"=\"*70)\n",
        "    print(\"üì¶ Building Sudoku Dataset\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Source: sapientinc/sudoku-extreme\")\n",
        "    print(f\"Output: {DATASET_DIR}\")\n",
        "    print(f\"Train subsample: {TRAIN_SUBSAMPLE_SIZE}\")\n",
        "    print(f\"Augmentation: {NUM_AUG}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Build train set\n",
        "    train_metadata = convert_subset(\"train\")\n",
        "\n",
        "    # Build test set\n",
        "    test_metadata = convert_subset(\"test\")\n",
        "\n",
        "    # Save identifiers.json\n",
        "    with open(os.path.join(DATASET_DIR, \"identifiers.json\"), \"w\") as f:\n",
        "        json.dump([\"<blank>\"], f)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ Dataset build complete!\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2E5Wu9e9pEp"
      },
      "outputs": [],
      "source": [
        "DROPOUT_TYPES = ['embedding_dropout', 'attention_dropout', 'hidden_dropout', 'activation_dropout']\n",
        "\n",
        "def make_dropout_config(dropout_type: str, dropout_value: float):\n",
        "    \"\"\"Create a single dropout configuration\"\"\"\n",
        "    config = copy.deepcopy(sudoku_config_Attn_A100)\n",
        "    for dt in DROPOUT_TYPES:\n",
        "        config['arch'][dt] = 0.0\n",
        "    config['arch'][dropout_type] = dropout_value\n",
        "    config['run_name'] = f\"{dropout_type.replace('_dropout', '')}_{dropout_value}\"\n",
        "    return config\n",
        "\n",
        "def run_dropout_sensitivity_test():\n",
        "    \"\"\"Run a complete dropout sensitivity test\"\"\"\n",
        "\n",
        "    # Experimental Setup: (dropout_type, values_to_test)\n",
        "    experiments = [\n",
        "\n",
        "        # Single-factor test\n",
        "        ('embedding_dropout', [0.1, 0.2, 0.3]),\n",
        "        ('attention_dropout', [0.1, 0.2, 0.3]),\n",
        "        ('hidden_dropout', [0.1, 0.2, 0.3]),\n",
        "        ('activation_dropout', [0.1, 0.2, 0.3]),\n",
        "    ]\n",
        "\n",
        "    total = sum(len(values) for _, values in experiments)\n",
        "    current = 0\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üß™ Dropout Sensitivity Analysis\")\n",
        "    print(f\"   Total experiments: {total}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for dropout_type, values in experiments:\n",
        "        for value in values:\n",
        "            current += 1\n",
        "            config = make_dropout_config(dropout_type, value)\n",
        "\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"üìä [{current}/{total}] {dropout_type} = {value}\")\n",
        "            print(\"=\" * 70)\n",
        "\n",
        "            try:\n",
        "                launch(config)\n",
        "                print(f\"‚úÖ Completed!\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] {dropout_type}={value} failed: {e}\")\n",
        "            finally:\n",
        "                # Key: Always finish each session to prevent the notebook from getting stuck during repeated initializations.\n",
        "                try:\n",
        "                    wandb.finish()\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üéâ All experiments completed!\")\n",
        "    print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline\n",
        "# Run the minimal test configuration for quick testing\n",
        "print(\"üöÄ Starting minimal Baseline configuration...\")\n",
        "print(\"=\"*70)\n",
        "print(\"üìä Test Configuration:\")\n",
        "print(f\"  - Epochs: {sudoku_config_Attn_A100['epochs']} (quick test)\")\n",
        "print(f\"  - Batch size: {sudoku_config_Attn_A100['global_batch_size']}\")\n",
        "print(f\"  - Eval interval: {sudoku_config_Attn_A100['eval_interval']}\")\n",
        "print(f\"  - Architecture: Attention (Transformer)\")\n",
        "print(f\"  - EMA: {'Enabled' if sudoku_config_Attn_A100['ema'] else 'Disabled'}\")\n",
        "\n",
        "try:\n",
        "    launch(sudoku_config_Attn_A100)\n",
        "finally:\n",
        "    try:\n",
        "        wandb.finish()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ============================================================================\n",
        "# üöÄ Start Testing\n",
        "# ============================================================================\n",
        "# Uncomment the following line to run the full test:\n",
        "# run_dropout_sensitivity_test()\n",
        "\n",
        "# # Or test a specific dropout type individually:\n",
        "# # config = make_dropout_config(‚Äòembedding_dropout‚Äô, 0.1)\n",
        "# # launch(config)\n",
        "\n",
        "# print(‚Äú‚úÖ Dropout sensitivity test functions loaded!‚Äù)\n",
        "# print(‚Äú\\nüìã Available functions:‚Äù)\n",
        "# print(‚Äú  - run_dropout_sensitivity_test()  # Run full test‚Äù)\n",
        "# print(‚Äú  - make_dropout_config(type, value) # Create single configuration‚Äù)\n",
        "# print(‚Äú\\nüìù Usage examples:‚Äù)\n",
        "# print(‚Äú  run_dropout_sensitivity_test()  # Run all experiments‚Äù)\n",
        "# print(‚Äú  launch(make_dropout_config(‚Äòembedding_dropout‚Äô, 0.1))  # Single experiment‚Äù)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LfX1SdCn5lBS",
        "outputId": "0a0f6b80-20c8-4dbf-97d7-b20cdbe679e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting minimal Baseline configuration...\n",
            "======================================================================\n",
            "üìä Test Configuration:\n",
            "  - Epochs: 1000 (quick test)\n",
            "  - Batch size: 256\n",
            "  - Eval interval: 100\n",
            "  - Architecture: Attention (Transformer)\n",
            "  - EMA: Enabled\n",
            "TinyRecursiveReasoningModel_ACTV1(\n",
            "  (inner): TinyRecursiveReasoningModel_ACTV1_Inner(\n",
            "    (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (embed_tokens): CastedEmbedding()\n",
            "    (lm_head): CastedLinear()\n",
            "    (q_head): CastedLinear()\n",
            "    (puzzle_emb): CastedSparseEmbedding()\n",
            "    (rotary_emb): RotaryEmbedding()\n",
            "    (L_level): TinyRecursiveReasoningModel_ACTV1ReasoningModule(\n",
            "      (layers): ModuleList(\n",
            "        (0-1): 2 x TinyRecursiveReasoningModel_ACTV1Block(\n",
            "          (self_attn): Attention(\n",
            "            (hidden_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (qkv_proj): CastedLinear()\n",
            "            (o_proj): CastedLinear()\n",
            "          )\n",
            "          (mlp): SwiGLU(\n",
            "            (gate_up_proj): CastedLinear()\n",
            "            (down_proj): CastedLinear()\n",
            "            (activation_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (hidden_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3906 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251216_005021-hy27cjhw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Dropout-Sensitivity-eval/runs/hy27cjhw' target=\"_blank\">Sudoku-Extreme-Baseline</a></strong> to <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Dropout-Sensitivity-eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Dropout-Sensitivity-eval' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Dropout-Sensitivity-eval</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Dropout-Sensitivity-eval/runs/hy27cjhw' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Dropout-Sensitivity-eval/runs/hy27cjhw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup EMA\n",
            "======================================================================\n",
            "üìä Model Efficiency Metrics:\n",
            "======================================================================\n",
            "  Parameters: 6,828,034 (6.83M)\n",
            "  Model Size: 26.05 MB\n",
            "  FLOPs per Forward: 137.48 GFLOPs\n",
            "  FLOPs per Sample: 0.54 GFLOPs\n",
            "======================================================================\n",
            "Epoch 0\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|‚ñâ         | 389/3906 [01:29<05:05, 11.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|‚ñâ         | 390/3906 [01:43<05:04, 11.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 40.131s)\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps (time: 0.761s)\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps (time: 0.761s)\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps (time: 0.762s)\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps (time: 0.762s)\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps (time: 0.763s)\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps (time: 0.763s)\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps (time: 0.763s)\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps (time: 0.763s)\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps (time: 0.765s)\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps (time: 0.765s)\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps (time: 0.765s)\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps (time: 0.766s)\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps (time: 0.767s)\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps (time: 0.768s)\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 101: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 102: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 103: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 104: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 105: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 106: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 107: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 108: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 109: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 110: all\n",
            "  Completed inference in 16 steps (time: 0.769s)\n",
            "Processing batch 111: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 112: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 113: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 114: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 115: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 116: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 117: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 118: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 119: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 120: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 121: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 122: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 123: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 124: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 125: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 126: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 127: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 128: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 129: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 130: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 131: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 132: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 133: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 134: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 135: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 136: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 137: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 138: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 139: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 140: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 141: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 142: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 143: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 144: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 145: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 146: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 147: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 148: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 149: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 150: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 151: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 152: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 153: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 154: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 155: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 156: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 157: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 158: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 159: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 160: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 161: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 162: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 163: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 164: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 165: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 166: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 167: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 168: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 169: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 170: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 171: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 172: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 173: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 174: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 175: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 176: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 177: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 178: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 179: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 180: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 181: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 182: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 183: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 184: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 185: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 186: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 187: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 188: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 189: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 190: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 191: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 192: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 193: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 194: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 195: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 196: all\n",
            "  Completed inference in 16 steps (time: 0.770s)\n",
            "Processing batch 197: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 198: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 199: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 200: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|‚ñà         | 391/3906 [04:43<28:30:43, 29.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAVE CHECKPOINT\n",
            "Epoch 100\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|‚ñà‚ñâ        | 779/3906 [05:17<04:30, 11.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 21: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|‚ñà‚ñâ        | 780/3906 [05:33<04:30, 11.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 101: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 102: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 103: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 104: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 105: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 106: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 107: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 108: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 109: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 110: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 111: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 112: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 113: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 114: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 115: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 116: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 117: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 118: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 119: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 120: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 121: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 122: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 123: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 124: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 125: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 126: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 127: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 128: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 129: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 130: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 131: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 132: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 133: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 134: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 135: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 136: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 137: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 138: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 139: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 140: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 141: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 142: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 143: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 144: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 145: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 146: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 147: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 148: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 149: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 150: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 151: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 152: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 153: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 154: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 155: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 156: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 157: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 158: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 159: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 160: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 161: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 162: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 163: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 164: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 165: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 166: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 167: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 168: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 169: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 170: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 171: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 172: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 173: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 174: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 175: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 176: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 177: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 178: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 179: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 180: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 181: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 182: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 183: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 184: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 185: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 186: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 187: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 188: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 189: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 190: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 191: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 192: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 193: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 194: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 195: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 196: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 197: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 198: all\n",
            "  Completed inference in 16 steps (time: 0.771s)\n",
            "Processing batch 199: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 200: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|‚ñà‚ñâ        | 781/3906 [07:52<20:14:54, 23.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 200\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|‚ñà‚ñà‚ñâ       | 1169/3906 [08:25<03:55, 11.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 14: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|‚ñà‚ñà‚ñâ       | 1170/3906 [08:36<03:55, 11.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 101: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 102: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 103: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 104: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 105: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 106: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 107: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 108: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 109: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 110: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 111: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 112: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 113: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 114: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 115: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 116: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 117: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 118: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 119: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 120: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 121: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 122: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 123: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 124: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 125: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 126: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 127: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 128: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 129: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 130: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 131: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 132: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 133: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 134: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 135: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 136: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 137: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 138: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 139: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 140: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 141: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 142: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 143: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 144: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 145: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 146: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 147: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 148: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 149: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 150: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 151: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 152: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 153: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 154: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 155: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 156: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 157: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 158: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 159: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 160: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 161: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 162: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 163: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 164: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 165: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 166: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 167: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 168: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 169: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 170: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 171: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 172: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 173: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 174: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 175: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 176: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 177: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 178: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 179: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 180: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 181: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 182: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 183: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 184: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 185: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 186: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 187: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 188: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 189: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 190: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 191: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 192: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 193: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 194: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 195: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 196: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 197: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 198: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 199: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 200: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|‚ñà‚ñà‚ñâ       | 1171/3906 [11:01<17:45:23, 23.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 300\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|‚ñà‚ñà‚ñà‚ñâ      | 1559/3906 [11:34<03:23, 11.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 15: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|‚ñà‚ñà‚ñà‚ñâ      | 1560/3906 [11:46<03:23, 11.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 101: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 102: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 103: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 104: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 105: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 106: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 107: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 108: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 109: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 110: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 111: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 112: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 113: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 114: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 115: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 116: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 117: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 118: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 119: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 120: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 121: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 122: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 123: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 124: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 125: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 126: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 127: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 128: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 129: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 130: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 131: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 132: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 133: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 134: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 135: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 136: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 137: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 138: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 139: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 140: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 141: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 142: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 143: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 144: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 145: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 146: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 147: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 148: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 149: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 150: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 151: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 152: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 153: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 154: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 155: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 156: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 157: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 158: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 159: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 160: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 161: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 162: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 163: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 164: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 165: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 166: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 167: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 168: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 169: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 170: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 171: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 172: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 173: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 174: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 175: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 176: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 177: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 178: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 179: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 180: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 181: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 182: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 183: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 184: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 185: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 186: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 187: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 188: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 189: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 190: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 191: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 192: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 193: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 194: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 195: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 196: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 197: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 198: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 199: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 200: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|‚ñà‚ñà‚ñà‚ñâ      | 1561/3906 [14:10<15:14:16, 23.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 400\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1949/3906 [14:43<02:48, 11.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 16: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1950/3906 [14:56<02:48, 11.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 101: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 102: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 103: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 104: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 105: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 106: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 107: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 108: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 109: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 110: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 111: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 112: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 113: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 114: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 115: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 116: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 117: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 118: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 119: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 120: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 121: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 122: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 123: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 124: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 125: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 126: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 127: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 128: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 129: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 130: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 131: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 132: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 133: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 134: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 135: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 136: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 137: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 138: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 139: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 140: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 141: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 142: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 143: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 144: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 145: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 146: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 147: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 148: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 149: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 150: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 151: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 152: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 153: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 154: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 155: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 156: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 157: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 158: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 159: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 160: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 161: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 162: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 163: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 164: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 165: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 166: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 167: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 168: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 169: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 170: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 171: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 172: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 173: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 174: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 175: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 176: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 177: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 178: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 179: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 180: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 181: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 182: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 183: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 184: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 185: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 186: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 187: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 188: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 189: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 190: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 191: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 192: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 193: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 194: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 195: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 196: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 197: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 198: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 199: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 200: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1951/3906 [17:19<12:42:39, 23.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 500\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 2339/3906 [17:53<02:15, 11.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 14: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 2340/3906 [18:03<02:15, 11.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 101: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 102: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 103: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 104: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 105: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 106: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 107: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 108: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 109: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 110: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 111: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 112: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 113: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 114: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 115: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 116: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 117: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 118: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 119: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 120: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 121: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 122: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 123: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 124: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 125: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 126: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 127: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 128: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 129: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 130: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 131: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 132: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 133: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 134: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 135: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 136: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 137: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 138: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 139: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 140: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 141: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 142: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 143: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 144: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 145: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 146: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 147: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 148: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 149: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 150: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 151: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 152: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 153: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 154: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 155: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 156: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 157: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 158: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 159: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 160: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 161: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 162: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 163: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 164: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 165: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 166: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 167: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 168: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 169: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 170: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 171: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 172: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 173: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 174: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 175: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 176: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 177: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 178: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 179: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 180: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 181: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 182: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 183: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 184: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 185: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 186: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 187: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 188: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 189: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 190: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 191: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 192: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 193: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 194: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 195: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 196: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 197: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 198: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 199: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 200: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 2341/3906 [20:28<10:10:51, 23.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 600\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2729/3906 [21:02<01:41, 11.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 15: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2730/3906 [21:13<01:41, 11.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 101: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 102: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 103: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 104: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 105: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 106: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 107: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 108: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 109: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 110: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 111: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 112: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 113: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 114: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 115: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 116: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 117: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 118: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 119: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 120: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 121: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 122: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 123: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 124: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 125: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 126: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 127: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 128: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 129: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 130: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 131: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 132: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 133: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 134: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 135: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 136: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 137: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 138: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 139: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 140: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 141: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 142: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 143: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 144: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 145: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 146: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 147: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 148: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 149: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 150: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 151: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 152: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 153: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 154: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 155: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 156: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 157: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 158: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 159: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 160: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 161: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 162: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 163: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 164: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 165: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 166: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 167: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 168: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 169: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 170: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 171: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 172: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 173: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 174: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 175: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 176: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 177: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 178: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 179: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 180: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 181: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 182: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 183: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 184: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 185: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 186: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 187: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 188: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 189: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 190: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 191: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 192: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 193: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 194: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 195: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 196: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 197: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 198: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 199: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 200: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2731/3906 [23:38<7:38:35, 23.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 700\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3119/3906 [24:11<01:07, 11.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 16: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3120/3906 [24:23<01:07, 11.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 101: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 102: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 103: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 104: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 105: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 106: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 107: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 108: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 109: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 110: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 111: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 112: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 113: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 114: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 115: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 116: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 117: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 118: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 119: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 120: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 121: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 122: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 123: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 124: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 125: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 126: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 127: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 128: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 129: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 130: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 131: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 132: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 133: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 134: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 135: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 136: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 137: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 138: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 139: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 140: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 141: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 142: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 143: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 144: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 145: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 146: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 147: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 148: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 149: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 150: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 151: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 152: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 153: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 154: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 155: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 156: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 157: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 158: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 159: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 160: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 161: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 162: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 163: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 164: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 165: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 166: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 167: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 168: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 169: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 170: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 171: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 172: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 173: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 174: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 175: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 176: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 177: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 178: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 179: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 180: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 181: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 182: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 183: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 184: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 185: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 186: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 187: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 188: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 189: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 190: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 191: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 192: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 193: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 194: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 195: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 196: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 197: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 198: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 199: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 200: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3121/3906 [26:47<5:06:26, 23.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 800\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3509/3906 [27:20<00:34, 11.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps (time: 0.772s)\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 17: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3510/3906 [27:33<00:34, 11.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps (time: 0.778s)\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 101: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 102: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 103: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 104: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 105: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 106: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 107: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 108: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 109: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 110: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 111: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 112: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 113: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 114: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 115: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 116: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 117: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 118: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 119: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 120: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 121: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 122: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 123: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 124: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 125: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 126: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 127: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 128: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 129: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 130: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 131: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 132: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 133: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 134: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 135: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 136: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 137: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 138: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 139: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 140: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 141: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 142: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 143: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 144: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 145: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 146: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 147: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 148: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 149: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 150: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 151: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 152: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 153: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 154: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 155: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 156: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 157: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 158: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 159: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 160: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 161: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 162: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 163: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 164: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 165: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 166: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 167: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 168: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 169: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 170: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 171: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 172: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 173: all\n",
            "  Completed inference in 16 steps (time: 0.778s)\n",
            "Processing batch 174: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 175: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 176: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 177: all\n",
            "  Completed inference in 16 steps (time: 0.779s)\n",
            "Processing batch 178: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 179: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 180: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 181: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 182: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 183: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 184: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 185: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 186: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 187: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 188: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 189: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 190: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 191: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 192: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 193: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 194: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 195: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 196: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 197: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 198: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 199: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 200: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3511/3906 [29:56<2:34:16, 23.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 900\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3899/3906 [30:30<00:00, 11.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps (time: 0.773s)\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 17: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3900/3906 [30:43<00:00, 11.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps (time: 0.778s)\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 101: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 102: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 103: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 104: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 105: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 106: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 107: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 108: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 109: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 110: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 111: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 112: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 113: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 114: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 115: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 116: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 117: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 118: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 119: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 120: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 121: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 122: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 123: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 124: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 125: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 126: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 127: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 128: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 129: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 130: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 131: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 132: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 133: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 134: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 135: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 136: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 137: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 138: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 139: all\n",
            "  Completed inference in 16 steps (time: 0.777s)\n",
            "Processing batch 140: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 141: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 142: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 143: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 144: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 145: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 146: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 147: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 148: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 149: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 150: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 151: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 152: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 153: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 154: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 155: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 156: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 157: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 158: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 159: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 160: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 161: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 162: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 163: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 164: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 165: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 166: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 167: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 168: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 169: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 170: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 171: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 172: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 173: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 174: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 175: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 176: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 177: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 178: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 179: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 180: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 181: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 182: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 183: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 184: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 185: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 186: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 187: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 188: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 189: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 190: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 191: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 192: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 193: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 194: all\n",
            "  Completed inference in 16 steps (time: 0.774s)\n",
            "Processing batch 195: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 196: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 197: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 198: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "Processing batch 199: all\n",
            "  Completed inference in 16 steps (time: 0.775s)\n",
            "Processing batch 200: all\n",
            "  Completed inference in 16 steps (time: 0.776s)\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>‚ñÅ</td></tr><tr><td>estimated_flops_per_forward</td><td>‚ñÅ</td></tr><tr><td>estimated_flops_per_sample</td><td>‚ñÅ</td></tr><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/avg_inference_steps</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/exact_accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñà</td></tr><tr><td>eval/gpu_memory_gb</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/inference_time_per_step</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/samples_per_sec</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/total_eval_time</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>+21</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>256</td></tr><tr><td>estimated_flops_per_forward</td><td>137477357568</td></tr><tr><td>estimated_flops_per_sample</td><td>537020928.0</td></tr><tr><td>eval/accuracy</td><td>0.64751</td></tr><tr><td>eval/avg_inference_steps</td><td>16</td></tr><tr><td>eval/exact_accuracy</td><td>0.01012</td></tr><tr><td>eval/gpu_memory_gb</td><td>0.3079</td></tr><tr><td>eval/inference_time_per_step</td><td>0.01172</td></tr><tr><td>eval/samples_per_sec</td><td>329.12794</td></tr><tr><td>eval/total_eval_time</td><td>155.5626</td></tr><tr><td>+21</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Sudoku-Extreme-Baseline</strong> at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Dropout-Sensitivity-eval/runs/hy27cjhw' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Dropout-Sensitivity-eval/runs/hy27cjhw</a><br> View project at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Dropout-Sensitivity-eval' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Dropout-Sensitivity-eval</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251216_005021-hy27cjhw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3900/3906 [33:06<00:03,  1.96it/s]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ccd4de31909b455b8667eef0e8e33916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a9981eff1464bcf856eb0a353f597a6",
              "IPY_MODEL_7f79fe0c96d84d6cac026c3d53e95075",
              "IPY_MODEL_b8143ed3e3df45f389166ea31eda5ab9"
            ],
            "layout": "IPY_MODEL_1198bdbd8f7f4733b846de7b4e1b08aa"
          }
        },
        "7a9981eff1464bcf856eb0a353f597a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_301baf207ca64be880f74ef8d79555eb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a1f0bccc9d1f4306ae9d2e0b6cf07f7a",
            "value": "train.csv:‚Äá100%"
          }
        },
        "7f79fe0c96d84d6cac026c3d53e95075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5211e729514f445db45bcd7f3976752b",
            "max": 718819925,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff5ad1923e7341acaf044548e181856d",
            "value": 718819925
          }
        },
        "b8143ed3e3df45f389166ea31eda5ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8be8228bdece49cc83a6615d78a2ee55",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b855873aaeac4bc1b7fdeff97ed86c15",
            "value": "‚Äá719M/719M‚Äá[00:03&lt;00:00,‚Äá385MB/s]"
          }
        },
        "1198bdbd8f7f4733b846de7b4e1b08aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301baf207ca64be880f74ef8d79555eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1f0bccc9d1f4306ae9d2e0b6cf07f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5211e729514f445db45bcd7f3976752b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff5ad1923e7341acaf044548e181856d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8be8228bdece49cc83a6615d78a2ee55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b855873aaeac4bc1b7fdeff97ed86c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8aef680f119e4fbba5f62af4638b9bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14695b7240794c43aee23e84ab6a8184",
              "IPY_MODEL_2e6da288e4cd4ea5ae402aab7c0a6d3a",
              "IPY_MODEL_0fc83881bfd448dd9d22f7e7e5781729"
            ],
            "layout": "IPY_MODEL_71338241f3f846d29cceda6ca43aab1d"
          }
        },
        "14695b7240794c43aee23e84ab6a8184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7326fdbc18a54fabb4ae2b562b4247a6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bebb0503536945d195df4659f2431eba",
            "value": "test.csv:‚Äá100%"
          }
        },
        "2e6da288e4cd4ea5ae402aab7c0a6d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_701e5d3b96864f7fb2d355ea7ef23cc2",
            "max": 79360390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be1683a24c3d45bd9e6801d1799b13bb",
            "value": 79360390
          }
        },
        "0fc83881bfd448dd9d22f7e7e5781729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f0fcdbfb57f49cabed6c17d3647f8f7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ce57a7185c18445ebfa045fd9e667206",
            "value": "‚Äá79.4M/79.4M‚Äá[00:01&lt;00:00,‚Äá53.1MB/s]"
          }
        },
        "71338241f3f846d29cceda6ca43aab1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7326fdbc18a54fabb4ae2b562b4247a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bebb0503536945d195df4659f2431eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "701e5d3b96864f7fb2d355ea7ef23cc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be1683a24c3d45bd9e6801d1799b13bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f0fcdbfb57f49cabed6c17d3647f8f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce57a7185c18445ebfa045fd9e667206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}