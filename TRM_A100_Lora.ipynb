{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† TRM LoRA Efficiency Study\n",
        "\n",
        "**One-Click Runnable** notebook for LoRA experiments on TinyRecursiveModels.\n",
        "\n",
        "## ‚úÖ Alignment with Trelis Implementation\n",
        "This notebook is aligned with `TinyRecursiveModels-Trelis` (trm.yaml + cfg_pretrain_lora.yaml):\n",
        "\n",
        "| Parameter | This Notebook | Trelis | Status |\n",
        "|-----------|---------------|--------|--------|\n",
        "| `L_cycles` | 4 | 4 | ‚úÖ Aligned |\n",
        "| `puzzle_emb_len` | 1 | 1 | ‚úÖ Aligned |\n",
        "| `lr_warmup_steps` | 2000 | 2000 | ‚úÖ Aligned |\n",
        "| `global_batch_size` | 768 | 768 | ‚úÖ Aligned |\n",
        "| `H_cycles` | 3 | 3 | ‚úÖ Aligned |\n",
        "| `L_layers` | 2 | 2 | ‚úÖ Aligned |\n",
        "| `hidden_size` | 512 | 512 | ‚úÖ Aligned |\n",
        "| `num_heads` | 8 | 8 | ‚úÖ Aligned |\n",
        "| `expansion` | 4 | 4 | ‚úÖ Aligned |\n",
        "| `halt_max_steps` | 16 | 16 | ‚úÖ Aligned |\n",
        "| `halt_exploration_prob` | 0.1 | 0.1 | ‚úÖ Aligned |\n",
        "| `lr` | 1e-4 | 1e-4 | ‚úÖ Aligned |\n",
        "| `weight_decay` | 0.1 | 0.1 | ‚úÖ Aligned |\n",
        "| `ema_rate` | 0.999 | 0.999 | ‚úÖ Aligned |\n",
        "\n",
        "**Differences (intentional):**\n",
        "- Single GPU (Trelis uses multi-GPU)\n",
        "- AdamW optimizer (Trelis uses Muon)\n",
        "\n",
        "## LoRA Experiments\n",
        "| Config | LoRA Rank | Alpha | Train Base |\n",
        "|--------|-----------|-------|------------|\n",
        "| baseline | 0 | - | True |\n",
        "| lora-r1 | 1 | 16 | False |\n",
        "| lora-r4 | 4 | 16 | False |\n",
        "| lora-r16 | 16 | 32 | False |\n",
        "| lora-r4-full | 4 | 16 | True |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected: lora-r1, Epochs: 10000, BatchSize: 1024 (Aligned with Trelis)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 1: Configuration - EDIT THIS\n",
        "# ============================================================================\n",
        "import os\n",
        "\n",
        "# API Keys\n",
        "WANDB_API_KEY = '' # Or set directly\n",
        "HF_TOKEN = \"\"\n",
        "\n",
        "# Experiment Selection\n",
        "SELECTED_CONFIG = 'lora-r1'  # Options: 'baseline', 'lora-r1', 'lora-r4', 'lora-r16', 'lora-r4-full'\n",
        "\n",
        "# Training Settings (Aligned with Trelis cfg_pretrain_lora.yaml)\n",
        "TRAIN_EPOCHS = 10000\n",
        "BATCH_SIZE = 1024  # Aligned: Trelis cfg_pretrain_lora.yaml\n",
        "EVAL_INTERVAL = 1000\n",
        "\n",
        "# Dataset Settings\n",
        "TRAIN_SUBSAMPLE = 1000\n",
        "NUM_AUGMENT = 1000\n",
        "FORCE_REBUILD = False\n",
        "\n",
        "print(f'Selected: {SELECTED_CONFIG}, Epochs: {TRAIN_EPOCHS}, BatchSize: {BATCH_SIZE} (Aligned with Trelis)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 2: Install Dependencies\n",
        "# ============================================================================\n",
        "!pip install -q torch einops tqdm numpy pydantic wandb coolname datasets\n",
        "print('Dependencies installed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "TF32 enabled\n",
            "Wandb logged in\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 3: Imports & GPU Setup\n",
        "# ============================================================================\n",
        "from typing import Optional, Any, Sequence, List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "import os, math, json, shutil, copy, time\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, IterableDataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import coolname\n",
        "import pydantic\n",
        "from pydantic import BaseModel\n",
        "import einops\n",
        "from torch.nn.functional import scaled_dot_product_attention\n",
        "\n",
        "IGNORE_LABEL_ID = -100\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu = torch.cuda.get_device_name(0)\n",
        "    print(f'GPU: {gpu}')\n",
        "    if 'H100' in gpu or 'A100' in gpu:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        print('TF32 enabled')\n",
        "    torch.cuda.set_device(0)\n",
        "\n",
        "if WANDB_API_KEY:\n",
        "    wandb.login(key=WANDB_API_KEY)\n",
        "    print('Wandb logged in')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utilities loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 4: Common Utilities\n",
        "# ============================================================================\n",
        "def trunc_normal_init_(tensor, std=1.0, lower=-2.0, upper=2.0):\n",
        "    with torch.no_grad():\n",
        "        if std == 0:\n",
        "            tensor.zero_()\n",
        "        else:\n",
        "            sqrt2 = math.sqrt(2)\n",
        "            a, b = math.erf(lower/sqrt2), math.erf(upper/sqrt2)\n",
        "            z = (b-a)/2\n",
        "            c = (2*math.pi)**-0.5\n",
        "            pdf_u, pdf_l = c*math.exp(-0.5*lower**2), c*math.exp(-0.5*upper**2)\n",
        "            comp_std = std/math.sqrt(1-(upper*pdf_u-lower*pdf_l)/z-((pdf_u-pdf_l)/z)**2)\n",
        "            tensor.uniform_(a, b).erfinv_().mul_(sqrt2*comp_std).clip_(lower*comp_std, upper*comp_std)\n",
        "    return tensor\n",
        "\n",
        "CosSin = Tuple[torch.Tensor, torch.Tensor]\n",
        "def _find_multiple(a, b): return (-(a//-b))*b\n",
        "def rotate_half(x): return torch.cat((-x[..., x.shape[-1]//2:], x[..., :x.shape[-1]//2]), dim=-1)\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    orig = q.dtype; q, k = q.to(cos.dtype), k.to(cos.dtype)\n",
        "    return ((q*cos.unsqueeze(-2))+(rotate_half(q)*sin.unsqueeze(-2))).to(orig), ((k*cos.unsqueeze(-2))+(rotate_half(k)*sin.unsqueeze(-2))).to(orig)\n",
        "def rms_norm(x, eps):\n",
        "    dt = x.dtype; x = x.float(); return (x*torch.rsqrt(x.square().mean(-1,keepdim=True)+eps)).to(dt)\n",
        "print('Utilities loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 5: Layers with LoRA Support\n",
        "# ============================================================================\n",
        "class CastedLinear(nn.Module):\n",
        "    def __init__(self, in_f, out_f, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(trunc_normal_init_(torch.empty((out_f, in_f)), std=1.0/(in_f**0.5)))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_f)) if bias else None\n",
        "        self._lora_rank = 0; self._lora_alpha = 1.0; self._lora_scaling = 1.0\n",
        "        self._lora_dropout = None; self._lora_A = None; self._lora_B = None\n",
        "    def forward(self, x):\n",
        "        out = F.linear(x, self.weight.to(x.dtype), self.bias.to(x.dtype) if self.bias is not None else None)\n",
        "        if self._lora_rank > 0 and self._lora_A is not None:\n",
        "            lx = self._lora_dropout(x) if self._lora_dropout else x\n",
        "            out = out + F.linear(F.linear(lx.to(self._lora_A.dtype), self._lora_A), self._lora_B).to(out.dtype) * self._lora_scaling\n",
        "        return out\n",
        "    def enable_lora(self, rank, alpha=None, dropout=0.0, train_base=False, train_bias=False):\n",
        "        if rank <= 0: return\n",
        "        if self._lora_rank > 0: raise RuntimeError('LoRA already enabled')\n",
        "        self._lora_rank = rank\n",
        "        self._lora_alpha = float(alpha if alpha else rank)\n",
        "        self._lora_scaling = self._lora_alpha / rank\n",
        "        self._lora_dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "        self._lora_A = nn.Parameter(torch.zeros(rank, self.weight.shape[1]))\n",
        "        self._lora_B = nn.Parameter(torch.zeros(self.weight.shape[0], rank))\n",
        "        nn.init.kaiming_uniform_(self._lora_A, a=math.sqrt(5)); nn.init.zeros_(self._lora_B)\n",
        "        if not train_base: self.weight.requires_grad = False\n",
        "        if self.bias is not None and not train_bias: self.bias.requires_grad = False\n",
        "\n",
        "class CastedEmbedding(nn.Module):\n",
        "    def __init__(self, n, d, std, dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = dtype\n",
        "        self.embedding_weight = nn.Parameter(trunc_normal_init_(torch.empty(n, d), std=std))\n",
        "    def forward(self, x): return F.embedding(x, self.embedding_weight.to(self.cast_to))\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_pos, base, device=None):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0/(base**(torch.arange(0, dim, 2, dtype=torch.float32, device=device)/dim))\n",
        "        freqs = torch.outer(torch.arange(max_pos, dtype=torch.float32, device=device), inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.cos_cached = nn.Buffer(emb.cos(), persistent=False)\n",
        "        self.sin_cached = nn.Buffer(emb.sin(), persistent=False)\n",
        "    def forward(self): return self.cos_cached, self.sin_cached\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden, head_dim, heads, kv_heads, causal=False):\n",
        "        super().__init__()\n",
        "        self.hidden, self.head_dim, self.heads, self.kv_heads, self.causal = hidden, head_dim, heads, kv_heads, causal\n",
        "        self.out_size = head_dim * heads\n",
        "        self.qkv_proj = CastedLinear(hidden, (heads + 2*kv_heads)*head_dim, False)\n",
        "        self.o_proj = CastedLinear(self.out_size, hidden, False)\n",
        "    def forward(self, cos_sin, x):\n",
        "        B, L, _ = x.shape\n",
        "        qkv = self.qkv_proj(x).view(B, L, self.heads + 2*self.kv_heads, self.head_dim)\n",
        "        q, k, v = qkv[:,:,:self.heads], qkv[:,:,self.heads:self.heads+self.kv_heads], qkv[:,:,self.heads+self.kv_heads:]\n",
        "        if cos_sin: q, k = apply_rotary_pos_emb(q, k, *cos_sin)\n",
        "        q, k, v = (einops.rearrange(t, 'B S H D -> B H S D') for t in (q, k, v))\n",
        "        out = scaled_dot_product_attention(q, k, v, is_causal=self.causal)\n",
        "        return self.o_proj(einops.rearrange(out, 'B H S D -> B S H D').reshape(B, L, self.out_size))\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, hidden, expansion):\n",
        "        super().__init__()\n",
        "        inter = _find_multiple(round(expansion*hidden*2/3), 256)\n",
        "        self.gate_up = CastedLinear(hidden, inter*2, False)\n",
        "        self.down = CastedLinear(inter, hidden, False)\n",
        "    def forward(self, x):\n",
        "        g, u = self.gate_up(x).chunk(2, dim=-1)\n",
        "        return self.down(F.silu(g) * u)\n",
        "\n",
        "def enable_lora_for_model(model, rank, alpha=None, dropout=0.0, train_base=False, train_bias=False):\n",
        "    if rank <= 0: return 0\n",
        "    cnt = 0\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, CastedLinear):\n",
        "            m.enable_lora(rank, alpha, dropout, train_base, train_bias); cnt += 1\n",
        "    return cnt\n",
        "\n",
        "def count_parameters(model):\n",
        "    total = trainable = lora = 0\n",
        "    for n, p in model.named_parameters():\n",
        "        total += p.numel()\n",
        "        if p.requires_grad: trainable += p.numel()\n",
        "        if '_lora_' in n: lora += p.numel()\n",
        "    return {'total': total, 'trainable': trainable, 'lora': lora, 'ratio': trainable/total if total else 0}\n",
        "\n",
        "print('Layers loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRM Model loaded (aligned with Trelis)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 6: TRM Model (Aligned with TinyRecursiveModels-Trelis)\n",
        "# ============================================================================\n",
        "@dataclass\n",
        "class TRMInnerCarry:\n",
        "    z_H: torch.Tensor\n",
        "    z_L: torch.Tensor\n",
        "\n",
        "@dataclass\n",
        "class TRMCarry:\n",
        "    inner: TRMInnerCarry\n",
        "    steps: torch.Tensor\n",
        "    halted: torch.Tensor\n",
        "    data: Dict[str, torch.Tensor]\n",
        "\n",
        "class TRMConfig(BaseModel):\n",
        "    \"\"\"Config aligned with Trelis TinyRecursiveModels trm.yaml\"\"\"\n",
        "    batch_size: int; seq_len: int; vocab_size: int; num_puzzle_identifiers: int\n",
        "    H_cycles: int; L_cycles: int; H_layers: int; L_layers: int\n",
        "    hidden_size: int; expansion: float; num_heads: int; pos_encodings: str\n",
        "    rms_norm_eps: float = 1e-5; rope_theta: float = 10000.0\n",
        "    halt_max_steps: int; halt_exploration_prob: float\n",
        "    halt_max_steps_eval: Optional[int] = None\n",
        "    forward_dtype: str = 'bfloat16'\n",
        "    puzzle_emb_ndim: int = 0; puzzle_emb_len: int = 1  # Aligned: Trelis uses 1\n",
        "    mlp_t: bool = False; no_ACT_continue: bool = True\n",
        "    puzzle_emb_dropout: float = 0.0; grid_token_dropout: float = 0.0\n",
        "    lora_rank: int = 0; lora_alpha: Optional[float] = 1.0  # Optional to handle baseline (None)\n",
        "    lora_dropout: float = 0.0; lora_train_base: bool = False; lora_train_bias: bool = False\n",
        "\n",
        "class TRMBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        if cfg.mlp_t:\n",
        "            self.mlp_t = SwiGLU(cfg.seq_len + cfg.puzzle_emb_len, cfg.expansion)\n",
        "        else:\n",
        "            self.attn = Attention(cfg.hidden_size, cfg.hidden_size//cfg.num_heads, cfg.num_heads, cfg.num_heads, False)\n",
        "        self.mlp = SwiGLU(cfg.hidden_size, cfg.expansion)\n",
        "        self.eps = cfg.rms_norm_eps\n",
        "    def forward(self, cos_sin, x):\n",
        "        if self.cfg.mlp_t:\n",
        "            x = x.transpose(1,2); x = rms_norm(x + self.mlp_t(x), self.eps); x = x.transpose(1,2)\n",
        "        else:\n",
        "            x = rms_norm(x + self.attn(cos_sin, x), self.eps)\n",
        "        return rms_norm(x + self.mlp(x), self.eps)\n",
        "\n",
        "class TRMReasoning(nn.Module):\n",
        "    def __init__(self, layers): super().__init__(); self.layers = nn.ModuleList(layers)\n",
        "    def forward(self, x, inj, **kw):\n",
        "        x = x + inj\n",
        "        for l in self.layers: x = l(hidden_states=x, **kw) if hasattr(l, 'hidden_states') else l(kw.get('cos_sin'), x)\n",
        "        return x\n",
        "\n",
        "class TRMInner(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.dtype = getattr(torch, cfg.forward_dtype)\n",
        "        self.puzzle_emb_len = cfg.puzzle_emb_len\n",
        "        \n",
        "        # Embedding scale (aligned with original)\n",
        "        self.embed_scale = math.sqrt(cfg.hidden_size)\n",
        "        embed_init_std = 1.0 / self.embed_scale\n",
        "        \n",
        "        self.embedding = CastedEmbedding(cfg.vocab_size, cfg.hidden_size, embed_init_std, self.dtype)\n",
        "        \n",
        "        if cfg.pos_encodings == 'rope':\n",
        "            self.rotary = RotaryEmbedding(cfg.hidden_size//cfg.num_heads, cfg.seq_len + self.puzzle_emb_len, cfg.rope_theta)\n",
        "        else:\n",
        "            self.pos_emb = CastedEmbedding(cfg.seq_len + self.puzzle_emb_len, cfg.hidden_size, embed_init_std, self.dtype)\n",
        "        \n",
        "        self.L_level = TRMReasoning([TRMBlock(cfg) for _ in range(cfg.L_layers)])\n",
        "        self.lm_head = CastedLinear(cfg.hidden_size, cfg.vocab_size, False)\n",
        "        self.q_head = CastedLinear(cfg.hidden_size, 2, True)  # bias=True (aligned)\n",
        "        \n",
        "        # Initial states as Buffer (aligned with original)\n",
        "        self.H_init = nn.Buffer(trunc_normal_init_(torch.empty(cfg.hidden_size, dtype=self.dtype), std=1), persistent=True)\n",
        "        self.L_init = nn.Buffer(trunc_normal_init_(torch.empty(cfg.hidden_size, dtype=self.dtype), std=1), persistent=True)\n",
        "        \n",
        "        # Q head special init (aligned with original)\n",
        "        with torch.no_grad():\n",
        "            self.q_head.weight.zero_()\n",
        "            self.q_head.bias.fill_(-5)\n",
        "    \n",
        "    def _input_embeddings(self, inputs):\n",
        "        \"\"\"Token embedding with scale.\"\"\"\n",
        "        emb = self.embedding(inputs.to(torch.int32))\n",
        "        # Position embeddings (learned)\n",
        "        if self.cfg.pos_encodings == 'learned':\n",
        "            emb = 0.707106781 * (emb + self.pos_emb.embedding_weight.to(self.dtype))\n",
        "        return self.embed_scale * emb\n",
        "    \n",
        "    def empty_carry(self, batch_size):\n",
        "        return TRMInnerCarry(\n",
        "            z_H=torch.empty(batch_size, self.cfg.seq_len + self.puzzle_emb_len, self.cfg.hidden_size, dtype=self.dtype),\n",
        "            z_L=torch.empty(batch_size, self.cfg.seq_len + self.puzzle_emb_len, self.cfg.hidden_size, dtype=self.dtype),\n",
        "        )\n",
        "    \n",
        "    def reset_carry(self, reset_flag, carry):\n",
        "        return TRMInnerCarry(\n",
        "            z_H=torch.where(reset_flag.view(-1, 1, 1), self.H_init, carry.z_H),\n",
        "            z_L=torch.where(reset_flag.view(-1, 1, 1), self.L_init, carry.z_L),\n",
        "        )\n",
        "    \n",
        "    def forward(self, carry, batch):\n",
        "        cs = self.rotary() if hasattr(self, 'rotary') else None\n",
        "        \n",
        "        # Input encoding (no puzzle embedding for simplicity in Sudoku)\n",
        "        # Pad zeros for puzzle_emb_len positions at the beginning\n",
        "        input_emb = self._input_embeddings(batch['inputs'])\n",
        "        B = input_emb.shape[0]\n",
        "        pad = torch.zeros(B, self.puzzle_emb_len, self.cfg.hidden_size, dtype=self.dtype, device=input_emb.device)\n",
        "        input_embeddings = torch.cat([pad, input_emb], dim=1)\n",
        "        \n",
        "        z_H, z_L = carry.z_H, carry.z_L\n",
        "        \n",
        "        # Forward iterations (aligned with original: H_cycles-1 no_grad, 1 with grad)\n",
        "        with torch.no_grad():\n",
        "            for _ in range(self.cfg.H_cycles - 1):\n",
        "                for _ in range(self.cfg.L_cycles):\n",
        "                    z_L = self.L_level(z_L, z_H + input_embeddings, cos_sin=cs)\n",
        "                z_H = self.L_level(z_H, z_L, cos_sin=cs)\n",
        "        \n",
        "        # Last H cycle with grad\n",
        "        for _ in range(self.cfg.L_cycles):\n",
        "            z_L = self.L_level(z_L, z_H + input_embeddings, cos_sin=cs)\n",
        "        z_H = self.L_level(z_H, z_L, cos_sin=cs)\n",
        "        \n",
        "        # Output from z_H (aligned with original)\n",
        "        new_carry = TRMInnerCarry(z_H=z_H.detach(), z_L=z_L.detach())\n",
        "        logits = self.lm_head(z_H)[:, self.puzzle_emb_len:]  # Remove puzzle_emb positions\n",
        "        q_logits = self.q_head(z_H[:, 0]).to(torch.float32)  # Use first position (aligned)\n",
        "        \n",
        "        return new_carry, {'logits': logits, 'q_halt_logits': q_logits[:, 0], 'q_continue_logits': q_logits[:, 1]}\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1(nn.Module):\n",
        "    def __init__(self, cfg_dict):\n",
        "        super().__init__()\n",
        "        self.config = TRMConfig(**cfg_dict)\n",
        "        self.inner = TRMInner(self.config)\n",
        "    \n",
        "    def initial_carry(self, batch):\n",
        "        B = batch['inputs'].shape[0]\n",
        "        return TRMCarry(\n",
        "            inner=self.inner.empty_carry(B),\n",
        "            steps=torch.zeros(B, dtype=torch.int32, device=batch['inputs'].device),\n",
        "            halted=torch.ones(B, dtype=torch.bool, device=batch['inputs'].device),  # Default halted (aligned)\n",
        "            data={k: torch.empty_like(v) for k, v in batch.items()}\n",
        "        )\n",
        "    \n",
        "    def forward(self, carry, batch, **kw):\n",
        "        # Reset carry for halted sequences (aligned with original)\n",
        "        new_inner = self.inner.reset_carry(carry.halted, carry.inner)\n",
        "        new_steps = torch.where(carry.halted, torch.zeros_like(carry.steps), carry.steps)\n",
        "        new_data = {k: torch.where(carry.halted.view((-1,) + (1,)*(v.ndim-1)), batch[k], v) for k, v in carry.data.items()}\n",
        "        \n",
        "        # Forward inner model\n",
        "        new_inner, out = self.inner(new_inner, new_data)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            new_steps = new_steps + 1\n",
        "            halt_limit = self.config.halt_max_steps if self.training or self.config.halt_max_steps_eval is None else self.config.halt_max_steps_eval\n",
        "            is_last_step = new_steps >= halt_limit\n",
        "            halted = is_last_step\n",
        "            \n",
        "            if self.training and self.config.halt_max_steps > 1:\n",
        "                if self.config.no_ACT_continue:\n",
        "                    halted = halted | (out['q_halt_logits'] > 0)\n",
        "                else:\n",
        "                    halted = halted | (out['q_halt_logits'] > out['q_continue_logits'])\n",
        "                \n",
        "                # Exploration (aligned with original)\n",
        "                min_halt_steps = (torch.rand_like(out['q_halt_logits']) < self.config.halt_exploration_prob) * torch.randint_like(new_steps, low=2, high=self.config.halt_max_steps + 1)\n",
        "                halted = halted & (new_steps >= min_halt_steps)\n",
        "        \n",
        "        return TRMCarry(new_inner, new_steps, halted, new_data), out\n",
        "\n",
        "print('TRM Model loaded (aligned with Trelis)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss Head loaded (aligned with Trelis)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 7: Loss Head (Aligned with Trelis)\n",
        "# ============================================================================\n",
        "def s(x, eps=1e-30): return torch.where(x<0, 1/(1-x+eps), x+1)\n",
        "def log_stablemax(x, dim=-1): sx = s(x); return torch.log(sx/sx.sum(dim=dim, keepdim=True))\n",
        "def stablemax_ce(logits, labels, ignore_index=-100, valid_mask=None):\n",
        "    lp = log_stablemax(logits.double(), -1)\n",
        "    if valid_mask is None: valid_mask = labels != ignore_index\n",
        "    tl = torch.where(valid_mask, labels, 0)\n",
        "    plp = torch.gather(lp, index=tl.long().unsqueeze(-1), dim=-1).squeeze(-1)\n",
        "    return -torch.where(valid_mask, plp, 0.0)\n",
        "\n",
        "class ACTLossHead(nn.Module):\n",
        "    def __init__(self, model, loss_type): super().__init__(); self.model = model; self.loss_fn = stablemax_ce\n",
        "    def initial_carry(self, *a, **k): return self.model.initial_carry(*a, **k)\n",
        "    def forward(self, return_keys, **kw):\n",
        "        carry, out = self.model(**kw)\n",
        "        labels = carry.data['labels']  # Updated field name matches TRMCarry\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            out['preds'] = out['logits'].argmax(-1)\n",
        "            mask = labels != IGNORE_LABEL_ID\n",
        "            cnt = mask.sum(-1); div = cnt.clamp_min(1).unsqueeze(-1)\n",
        "            correct = mask & (out['preds'] == labels)\n",
        "            seq_correct = correct.sum(-1) == cnt\n",
        "            \n",
        "            # Metrics only for halted sequences (aligned with original)\n",
        "            valid = carry.halted & (cnt > 0)\n",
        "            metrics = {\n",
        "                'count': valid.sum(),\n",
        "                'accuracy': torch.where(valid, (correct.float()/div).sum(-1), 0.0).sum(),\n",
        "                'exact_accuracy': (valid & seq_correct).sum(),\n",
        "                'q_halt_accuracy': (valid & ((out['q_halt_logits'] >= 0) == seq_correct)).sum(),  # Added (aligned)\n",
        "                'steps': torch.where(valid, carry.steps, 0).sum()\n",
        "            }\n",
        "        \n",
        "        # Losses\n",
        "        lm_loss = (self.loss_fn(out['logits'], labels, valid_mask=mask)/div).sum()\n",
        "        q_halt_loss = F.binary_cross_entropy_with_logits(out['q_halt_logits'], seq_correct.float(), reduction='sum')\n",
        "        \n",
        "        # Q continue loss (only if target exists, aligned with original)\n",
        "        q_continue_loss = 0\n",
        "        if 'target_q_continue' in out:\n",
        "            q_continue_loss = F.binary_cross_entropy_with_logits(out['q_continue_logits'], out['target_q_continue'], reduction='sum')\n",
        "            metrics['q_continue_loss'] = q_continue_loss.detach()\n",
        "        \n",
        "        metrics.update({'lm_loss': lm_loss.detach(), 'q_halt_loss': q_halt_loss.detach()})\n",
        "        \n",
        "        total_loss = lm_loss + 0.5 * (q_halt_loss + q_continue_loss)\n",
        "        return carry, total_loss, metrics, {k: out[k].detach() for k in return_keys if k in out}, carry.halted.all()\n",
        "\n",
        "print('Loss Head loaded (aligned with Trelis)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 8: Dataset Classes\n",
        "# ============================================================================\n",
        "class PuzzleDatasetMetadata(pydantic.BaseModel):\n",
        "    pad_id: int; ignore_label_id: Optional[int]; blank_identifier_id: int\n",
        "    vocab_size: int; seq_len: int; num_puzzle_identifiers: int\n",
        "    total_groups: int; mean_puzzle_examples: float; total_puzzles: int; sets: List[str]\n",
        "\n",
        "class PuzzleDatasetConfig(pydantic.BaseModel):\n",
        "    seed: int; dataset_paths: List[str]; global_batch_size: int\n",
        "    test_set_mode: bool; epochs_per_iter: int; rank: int = 0; num_replicas: int = 1\n",
        "\n",
        "class PuzzleDataset(IterableDataset):\n",
        "    \"\"\"Aligned with Trelis puzzle_dataset.py\"\"\"\n",
        "    def __init__(self, cfg, split='train'):\n",
        "        super().__init__()\n",
        "        self.cfg, self.split = cfg, split\n",
        "        with open(os.path.join(cfg.dataset_paths[0], split, 'dataset.json')) as f:\n",
        "            self.metadata = PuzzleDatasetMetadata(**json.load(f))\n",
        "        self.local_bs = cfg.global_batch_size // cfg.num_replicas\n",
        "        self._data = None; self._iters = 0\n",
        "    def _load(self):\n",
        "        if self._data: return\n",
        "        self._data = {}\n",
        "        for s in self.metadata.sets:\n",
        "            p = self.cfg.dataset_paths[0]\n",
        "            self._data[s] = {k: np.load(os.path.join(p, self.split, f'{s}__{k}.npy'), mmap_mode='r' if k in ['inputs','labels'] else None)\n",
        "                            for k in ['inputs','labels','puzzle_identifiers','puzzle_indices','group_indices']}\n",
        "            # Aligned: compute puzzle_group_ids for task_identifiers\n",
        "            gi = self._data[s]['group_indices']\n",
        "            puzzle_group_ids = np.empty(int(gi[-1]), dtype=np.int32)\n",
        "            for gid in range(gi.size - 1):\n",
        "                puzzle_group_ids[int(gi[gid]):int(gi[gid+1])] = gid\n",
        "            self._data[s]['puzzle_group_ids'] = puzzle_group_ids\n",
        "    def _collate(self, b):\n",
        "        \"\"\"Aligned: include task_identifiers with pad=-1\"\"\"\n",
        "        b = {k: v.astype(np.int32) for k,v in b.items()}\n",
        "        if self.metadata.ignore_label_id: b['labels'][b['labels']==self.metadata.ignore_label_id] = IGNORE_LABEL_ID\n",
        "        if b['puzzle_identifiers'].size < self.local_bs:\n",
        "            pad = self.local_bs - b['puzzle_identifiers'].size\n",
        "            pv = {'inputs': self.metadata.pad_id, 'labels': IGNORE_LABEL_ID, 'puzzle_identifiers': self.metadata.blank_identifier_id, 'task_identifiers': -1}\n",
        "            b = {k: np.pad(v, ((0,pad),)+((0,0),)*(v.ndim-1), constant_values=pv[k]) for k,v in b.items()}\n",
        "        return {k: torch.from_numpy(v) for k,v in b.items()}\n",
        "    def __iter__(self):\n",
        "        self._load()\n",
        "        if self.cfg.test_set_mode: yield from self._test()\n",
        "        else: yield from self._train()\n",
        "    def _test(self):\n",
        "        for sn, d in self._data.items():\n",
        "            for i in range(0, len(d['inputs']), self.cfg.global_batch_size):\n",
        "                j = min(len(d['inputs']), i+self.local_bs)\n",
        "                pid = d['puzzle_identifiers'][i:j]\n",
        "                pid = pid if pid.ndim==1 else pid[:,0]\n",
        "                # Aligned: include task_identifiers\n",
        "                yield sn, self._collate({'inputs': d['inputs'][i:j], 'labels': d['labels'][i:j],\n",
        "                      'puzzle_identifiers': pid, 'task_identifiers': d['puzzle_group_ids'][pid]}), j-i\n",
        "    def _train(self):\n",
        "        for sn, d in self._data.items():\n",
        "            self._iters += 1\n",
        "            rng = np.random.default_rng(self.cfg.seed + self._iters)\n",
        "            gi, pi = d['group_indices'], d['puzzle_indices']\n",
        "            for _ in range(self.cfg.epochs_per_iter):\n",
        "                order = rng.permutation(gi.size-1); idx = 0\n",
        "                while idx < len(order):\n",
        "                    bi, bp, bg, sz = [], [], [], 0  # Aligned: bg for group ids\n",
        "                    while idx < len(order) and sz < self.cfg.global_batch_size:\n",
        "                        gid = order[idx]; pid = rng.integers(gi[gid], gi[gid+1]); idx += 1\n",
        "                        ps, pe = pi[pid], pi[pid+1]; psz = int(pe-ps)\n",
        "                        add = min(psz, self.cfg.global_batch_size-sz)\n",
        "                        bi.append(ps + rng.choice(psz, add, replace=False))\n",
        "                        bp.append(np.full(add, pid, np.int32))\n",
        "                        bg.append(np.full(add, gid, np.int32))  # Aligned: task_identifiers\n",
        "                        sz += add\n",
        "                    if bi:\n",
        "                        ii = np.concatenate(bi)\n",
        "                        # Aligned: include task_identifiers\n",
        "                        yield sn, self._collate({'inputs': d['inputs'][ii], 'labels': d['labels'][ii], \n",
        "                              'puzzle_identifiers': np.concatenate(bp), 'task_identifiers': np.concatenate(bg)}), len(ii)\n",
        "\n",
        "print('Dataset loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training framework loaded (aligned with Trelis)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 9: Training Framework (Single GPU + AdamW, Aligned with Trelis)\n",
        "# ============================================================================\n",
        "class LossConfig(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra='allow')\n",
        "    name: str\n",
        "\n",
        "class ArchConfig(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra='allow')\n",
        "    name: str; loss: LossConfig\n",
        "\n",
        "class PretrainConfig(pydantic.BaseModel):\n",
        "    \"\"\"Aligned with Trelis PretrainConfig (pretrain.py)\"\"\"\n",
        "    arch: ArchConfig; data_paths: List[str]; data_paths_test: List[str] = []\n",
        "    global_batch_size: int; epochs: int; lr: float; lr_min_ratio: float; lr_warmup_steps: int\n",
        "    weight_decay: float; beta1: float; beta2: float\n",
        "    puzzle_emb_ndim: int = 0; puzzle_emb_lr: float = 0.0; puzzle_emb_weight_decay: float = 0.0\n",
        "    freeze_weights: bool = False; checkpoint_path: Optional[str] = None\n",
        "    checkpoint_every_eval: bool = False; load_checkpoint: Optional[str] = None\n",
        "    project_name: Optional[str] = None; run_name: Optional[str] = None\n",
        "    eval_interval: Optional[int] = None; min_eval_interval: int = 0  # Aligned: when to start eval\n",
        "    ema: bool = False; ema_rate: float = 0.999; seed: int = 0\n",
        "\n",
        "@dataclass\n",
        "class TrainState:\n",
        "    model: nn.Module\n",
        "    optimizers: List[Any]\n",
        "    optimizer_lrs: List[float]\n",
        "    carry: Any  # Carry persists across batches (aligned with original)\n",
        "    step: int\n",
        "    total_steps: int\n",
        "\n",
        "class EMAHelper:\n",
        "    def __init__(self, mu=0.999): self.mu = mu; self.shadow = {}\n",
        "    def register(self, m):\n",
        "        for n, p in m.named_parameters():\n",
        "            if p.requires_grad: self.shadow[n] = p.data.clone()\n",
        "    def update(self, m):\n",
        "        for n, p in m.named_parameters():\n",
        "            if p.requires_grad and n in self.shadow: self.shadow[n] = self.mu*self.shadow[n] + (1-self.mu)*p.data\n",
        "    def ema_copy(self, m):\n",
        "        mc = copy.deepcopy(m)\n",
        "        for n, p in mc.named_parameters():\n",
        "            if n in self.shadow: p.data.copy_(self.shadow[n])\n",
        "        return mc\n",
        "\n",
        "def cosine_lr(step, base_lr, warmup, total, min_ratio=0.0):\n",
        "    if step < warmup: return base_lr * step / max(1, warmup)\n",
        "    prog = (step - warmup) / max(1, total - warmup)\n",
        "    return base_lr * (min_ratio + (1-min_ratio) * 0.5 * (1 + math.cos(math.pi * prog)))\n",
        "\n",
        "def create_dataloader(cfg, split, **kw):\n",
        "    ds = PuzzleDataset(PuzzleDatasetConfig(seed=cfg.seed, dataset_paths=cfg.data_paths_test if split=='test' and cfg.data_paths_test else cfg.data_paths, **kw), split)\n",
        "    return DataLoader(ds, batch_size=None, num_workers=1, prefetch_factor=8, pin_memory=True, persistent_workers=True), ds.metadata\n",
        "\n",
        "def create_model(cfg, meta):\n",
        "    model_cfg = {**cfg.arch.__pydantic_extra__, 'batch_size': cfg.global_batch_size, 'vocab_size': meta.vocab_size,\n",
        "                'seq_len': meta.seq_len, 'num_puzzle_identifiers': meta.num_puzzle_identifiers, 'causal': False}\n",
        "    lora_rank = cfg.arch.__pydantic_extra__.get('lora_rank', 0)\n",
        "    lora_alpha = cfg.arch.__pydantic_extra__.get('lora_alpha', None)\n",
        "    lora_train_base = cfg.arch.__pydantic_extra__.get('lora_train_base', False)\n",
        "    with torch.device('cuda'):\n",
        "        model = TinyRecursiveReasoningModel_ACTV1(model_cfg)\n",
        "        model = ACTLossHead(model, cfg.arch.loss.__pydantic_extra__.get('loss_type', 'stablemax_cross_entropy'))\n",
        "        if lora_rank > 0:\n",
        "            cnt = enable_lora_for_model(model, lora_rank, lora_alpha, 0.0, lora_train_base, False)\n",
        "            print(f'LoRA enabled: {cnt} layers, rank={lora_rank}, alpha={lora_alpha}, train_base={lora_train_base}')\n",
        "        stats = count_parameters(model)\n",
        "        print(f'Params: {stats[\"total\"]:,} total, {stats[\"trainable\"]:,} trainable ({stats[\"ratio\"]:.2%}), {stats[\"lora\"]:,} LoRA')\n",
        "        if 'DISABLE_COMPILE' not in os.environ: model = torch.compile(model)\n",
        "    # AdamW optimizer (only trainable params)\n",
        "    opt = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=cfg.lr, weight_decay=cfg.weight_decay, betas=(cfg.beta1, cfg.beta2))\n",
        "    return model, [opt], [cfg.lr]\n",
        "\n",
        "def init_train_state(cfg, meta):\n",
        "    total = int(cfg.epochs * meta.total_groups * meta.mean_puzzle_examples / cfg.global_batch_size)\n",
        "    model, opts, lrs = create_model(cfg, meta)\n",
        "    return TrainState(model, opts, lrs, None, 0, total)\n",
        "\n",
        "def train_batch(cfg, ts, batch, gbs):\n",
        "    \"\"\"\n",
        "    Train one batch - aligned with original Trelis implementation.\n",
        "    \n",
        "    Key: Only ONE forward pass per batch. ACT loop happens across batches via carry.\n",
        "    \"\"\"\n",
        "    ts.step += 1\n",
        "    if ts.step > ts.total_steps: return None\n",
        "    batch = {k: v.cuda() for k,v in batch.items()}\n",
        "    \n",
        "    # Init carry if None (first batch)\n",
        "    if ts.carry is None:\n",
        "        with torch.device('cuda'):\n",
        "            ts.carry = ts.model.initial_carry(batch)\n",
        "    \n",
        "    # Single forward pass (carry handles ACT state across batches)\n",
        "    ts.carry, loss, metrics, _, _ = ts.model(carry=ts.carry, batch=batch, return_keys=[])\n",
        "    \n",
        "    # Backward\n",
        "    (loss / gbs).backward()\n",
        "    \n",
        "    # Update LR and step optimizers\n",
        "    lr_now = None\n",
        "    for opt, base_lr in zip(ts.optimizers, ts.optimizer_lrs):\n",
        "        lr_now = cosine_lr(ts.step, base_lr, cfg.lr_warmup_steps, ts.total_steps, cfg.lr_min_ratio)\n",
        "        for pg in opt.param_groups: pg['lr'] = lr_now\n",
        "        opt.step(); opt.zero_grad()\n",
        "    \n",
        "    if metrics:\n",
        "        keys = sorted(metrics.keys())\n",
        "        vals = torch.stack([metrics[k] for k in keys]).cpu().numpy()\n",
        "        rm = {k: vals[i] for i,k in enumerate(keys)}\n",
        "        cnt = max(rm.get('count', 1), 1)\n",
        "        return {f'train/{k}': v/(gbs if k.endswith('loss') else cnt) for k,v in rm.items()} | {'train/lr': lr_now}\n",
        "    return None\n",
        "\n",
        "def evaluate(cfg, ts, loader, meta, max_batches=100):\n",
        "    \"\"\"Evaluate with optional batch limit for faster iteration.\"\"\"\n",
        "    metrics = None\n",
        "    with torch.inference_mode():\n",
        "        for i, (sn, batch, gbs) in enumerate(loader):\n",
        "            if max_batches and i >= max_batches: break  # Limit for faster eval\n",
        "            batch = {k: v.cuda() for k,v in batch.items()}\n",
        "            with torch.device('cuda'): carry = ts.model.initial_carry(batch)\n",
        "            # Full ACT loop for evaluation (aligned with Trelis)\n",
        "            while True:\n",
        "                carry, _, m, _, done = ts.model(carry=carry, batch=batch, return_keys=[])\n",
        "                if done: break\n",
        "            if metrics is None: metrics = {k: 0.0 for k in m}\n",
        "            for k in m: metrics[k] += m[k].item() if torch.is_tensor(m[k]) else m[k]\n",
        "    if metrics:\n",
        "        cnt = max(metrics.get('count', 1), 1)\n",
        "        return {f'eval/{k}': v/cnt for k,v in metrics.items() if k != 'count'}\n",
        "    return {}\n",
        "\n",
        "print('Training framework loaded (aligned with Trelis)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA configs: ['baseline', 'lora-r1', 'lora-r4', 'lora-r16', 'lora-r4-full']\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 10: LoRA Configurations\n",
        "# ============================================================================\n",
        "LORA_CONFIGS = {\n",
        "    'baseline': {'lora_rank': 0, 'lora_alpha': None, 'train_base': True, 'desc': 'Full training'},\n",
        "    'lora-r1': {'lora_rank': 1, 'lora_alpha': 16.0, 'train_base': False, 'desc': 'LoRA r=1'},\n",
        "    'lora-r4': {'lora_rank': 4, 'lora_alpha': 16.0, 'train_base': False, 'desc': 'LoRA r=4'},\n",
        "    'lora-r16': {'lora_rank': 16, 'lora_alpha': 32.0, 'train_base': False, 'desc': 'LoRA r=16'},\n",
        "    'lora-r4-full': {'lora_rank': 4, 'lora_alpha': 16.0, 'train_base': True, 'desc': 'LoRA r=4 + base'},\n",
        "}\n",
        "print('LoRA configs:', list(LORA_CONFIGS.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Successfully authenticated with Hugging Face Hub\n",
            "Building Sudoku dataset...\n",
            "Available columns: ['source', 'question', 'answer', 'rating']\n",
            "Using: puzzle_key=question, solution_key=answer\n",
            "train: 1001000 examples\n",
            "test: 422786 examples\n",
            "Dataset built!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 11: Build Sudoku Dataset\n",
        "# ============================================================================\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import hf_hub_download, login\n",
        "import warnings\n",
        "\n",
        "# Set Hugging Face Token for authentication\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "# Login to Hugging Face Hub\n",
        "try:\n",
        "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "    print(\"‚úÖ Successfully authenticated with Hugging Face Hub\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Warning: Could not login to Hugging Face Hub: {e}\")\n",
        "    print(\"   Continuing with token in environment variable...\")\n",
        "\n",
        "DATASET_DIR = './data/sudoku_lora'\n",
        "\n",
        "def build_sudoku_dataset():\n",
        "    if os.path.exists(os.path.join(DATASET_DIR, 'train', 'dataset.json')) and not FORCE_REBUILD:\n",
        "        print('Dataset exists, skipping build')\n",
        "        return\n",
        "    print('Building Sudoku dataset...')\n",
        "    # Explicitly pass token to avoid Colab secrets timeout\n",
        "    ds = load_dataset('sapientinc/sudoku-extreme', token=HF_TOKEN)\n",
        "    \n",
        "    # Debug: print available columns\n",
        "    print(f'Available columns: {ds[\"train\"].column_names}')\n",
        "    \n",
        "    # For sapientinc/sudoku-extreme: columns are ['source', 'question', 'answer', 'rating']\n",
        "    # 'question' = puzzle (81 chars with '.' for empty), 'answer' = solution (81 digits)\n",
        "    sample = ds['train'][0]\n",
        "    \n",
        "    # Try common column name patterns\n",
        "    puzzle_key = next((k for k in sample.keys() if k in ['question', 'puzzle', 'quiz', 'input']), None)\n",
        "    solution_key = next((k for k in sample.keys() if k in ['answer', 'solution', 'output', 'target']), None)\n",
        "    \n",
        "    if puzzle_key is None or solution_key is None:\n",
        "        print(f'ERROR: Could not find puzzle/solution keys. Available: {list(sample.keys())}')\n",
        "        print(f'Sample data: {sample}')\n",
        "        raise ValueError('Cannot determine puzzle/solution columns')\n",
        "    \n",
        "    print(f'Using: puzzle_key={puzzle_key}, solution_key={solution_key}')\n",
        "    \n",
        "    def shuffle_sudoku(inp, out):\n",
        "        perm = np.random.permutation(9) + 1\n",
        "        mapping = np.zeros(11, dtype=inp.dtype); mapping[1:10] = perm\n",
        "        return mapping[inp], mapping[out]\n",
        "    \n",
        "    def convert(split):\n",
        "        data = ds[split]\n",
        "        if split == 'train': data = data.select(range(min(TRAIN_SUBSAMPLE, len(data))))\n",
        "        # Use detected keys, handle '.' as 0 for empty cells\n",
        "        inputs = [np.array([0 if c == '.' else int(c) for c in str(r[puzzle_key])], dtype=np.int8) for r in data]\n",
        "        labels = [np.array([int(c) for c in str(r[solution_key])], dtype=np.int8) for r in data]\n",
        "        aug = 0 if split == 'test' else NUM_AUGMENT\n",
        "        res = {k: [] for k in ['inputs', 'labels', 'puzzle_identifiers', 'puzzle_indices', 'group_indices']}\n",
        "        res['puzzle_indices'].append(0); res['group_indices'].append(0)\n",
        "        pid = eid = 0\n",
        "        for inp, lab in zip(inputs, labels):\n",
        "            for ai in range(1 + aug):\n",
        "                i, l = (inp, lab) if ai == 0 else shuffle_sudoku(inp, lab)\n",
        "                res['inputs'].append(i); res['labels'].append(l); eid += 1; pid += 1\n",
        "                res['puzzle_indices'].append(eid); res['puzzle_identifiers'].append(0)\n",
        "            res['group_indices'].append(pid)\n",
        "        def to_np(s): arr = np.stack(s); return (arr + 1).astype(np.int32)\n",
        "        res = {'inputs': to_np(res['inputs']), 'labels': to_np(res['labels']),\n",
        "              'group_indices': np.array(res['group_indices'], np.int32),\n",
        "              'puzzle_indices': np.array(res['puzzle_indices'], np.int32),\n",
        "              'puzzle_identifiers': np.array(res['puzzle_identifiers'], np.int32)}\n",
        "        meta = PuzzleDatasetMetadata(seq_len=81, vocab_size=11, pad_id=0, ignore_label_id=0, blank_identifier_id=0,\n",
        "                                    num_puzzle_identifiers=1, total_groups=len(res['group_indices'])-1,\n",
        "                                    mean_puzzle_examples=1, total_puzzles=len(res['group_indices'])-1, sets=['all'])\n",
        "        path = os.path.join(DATASET_DIR, split)\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        with open(os.path.join(path, 'dataset.json'), 'w') as f: json.dump(meta.model_dump(), f)\n",
        "        for k, v in res.items(): np.save(os.path.join(path, f'all__{k}.npy'), v)\n",
        "        print(f'{split}: {res[\"inputs\"].shape[0]} examples')\n",
        "    convert('train'); convert('test')\n",
        "    print('Dataset built!')\n",
        "\n",
        "build_sudoku_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Cell 12: Launch Training\n",
        "# ============================================================================\n",
        "import time\n",
        "\n",
        "def launch_experiment(config_name):\n",
        "    # Clear GPU memory before starting experiment\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        print(f'GPU memory cleared. Free: {torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0):.2f} GB')\n",
        "    \n",
        "    lora_cfg = LORA_CONFIGS[config_name]\n",
        "    print('='*70)\n",
        "    print(f'Experiment: {config_name} - {lora_cfg[\"desc\"]}')\n",
        "    print('='*70)\n",
        "    \n",
        "    # Track efficiency metrics for report\n",
        "    exp_start_time = time.time()\n",
        "    \n",
        "    # ======== ALIGNED WITH TRELIS (trm.yaml + cfg_pretrain_lora.yaml) ========\n",
        "    cfg_dict = {\n",
        "        'arch': {'name': 'TRM', 'loss': {'name': 'ACTLossHead', 'loss_type': 'stablemax_cross_entropy'},\n",
        "                'halt_exploration_prob': 0.1, 'halt_max_steps': 16, 'H_cycles': 3, 'L_cycles': 4,  # Aligned: Trelis uses 4\n",
        "                'H_layers': 0, 'L_layers': 2, 'hidden_size': 512, 'num_heads': 8, 'expansion': 4,\n",
        "                'puzzle_emb_ndim': 0, 'pos_encodings': 'rope', 'forward_dtype': 'bfloat16',\n",
        "                'mlp_t': False, 'puzzle_emb_len': 1, 'no_ACT_continue': True,  # Aligned: Trelis uses 1\n",
        "                'lora_rank': lora_cfg['lora_rank'], 'lora_alpha': lora_cfg['lora_alpha'], 'lora_train_base': lora_cfg['train_base']},\n",
        "        'data_paths': [DATASET_DIR], 'data_paths_test': [],\n",
        "        'global_batch_size': BATCH_SIZE, 'epochs': TRAIN_EPOCHS, 'eval_interval': EVAL_INTERVAL,\n",
        "        'lr': 1e-4, 'lr_min_ratio': 1.0, 'lr_warmup_steps': 1000,  # Aligned: Trelis cfg_pretrain_lora.yaml\n",
        "        'beta1': 0.9, 'beta2': 0.95, 'weight_decay': 0.1, 'ema': True, 'ema_rate': 0.999, 'seed': 0,\n",
        "        'min_eval_interval': 0,  # Aligned: Trelis - when to start eval\n",
        "        'project_name': 'TRM-A100-LoRA', 'run_name': f'sudoku-{config_name}',\n",
        "        'checkpoint_path': f'./checkpoints/{config_name}', 'checkpoint_every_eval': True,\n",
        "    }\n",
        "    cfg = PretrainConfig(**cfg_dict)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    \n",
        "    train_per_iter = cfg.eval_interval or cfg.epochs\n",
        "    total_iters = cfg.epochs // train_per_iter\n",
        "    train_loader, train_meta = create_dataloader(cfg, 'train', test_set_mode=False, epochs_per_iter=train_per_iter, global_batch_size=cfg.global_batch_size)\n",
        "    try: eval_loader, _ = create_dataloader(cfg, 'test', test_set_mode=True, epochs_per_iter=1, global_batch_size=cfg.global_batch_size)\n",
        "    except: eval_loader = None\n",
        "    \n",
        "    ts = init_train_state(cfg, train_meta)\n",
        "    pbar = tqdm(total=ts.total_steps)\n",
        "    \n",
        "    # Compute efficiency metrics for report\n",
        "    param_stats = count_parameters(ts.model)\n",
        "    memory_allocated = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
        "    \n",
        "    wandb.init(project=cfg.project_name, name=cfg.run_name, config=cfg_dict)\n",
        "    wandb.log({\n",
        "        'params/total': param_stats['total'],\n",
        "        'params/trainable': param_stats['trainable'],\n",
        "        'params/lora': param_stats['lora'],\n",
        "        'params/trainable_ratio': param_stats['ratio'],\n",
        "        'efficiency/memory_gb': memory_allocated,\n",
        "    }, step=0)\n",
        "    \n",
        "    print(f\"üìä Efficiency Metrics:\")\n",
        "    print(f\"   Total Params: {param_stats['total']:,}\")\n",
        "    print(f\"   Trainable: {param_stats['trainable']:,} ({param_stats['ratio']:.2%})\")\n",
        "    print(f\"   LoRA Params: {param_stats['lora']:,}\")\n",
        "    print(f\"   Memory: {memory_allocated:.2f} GB\")\n",
        "    \n",
        "    ema = EMAHelper(cfg.ema_rate) if cfg.ema else None\n",
        "    if ema: ema.register(ts.model)\n",
        "    \n",
        "    for it in range(total_iters):\n",
        "        print(f'Epoch {it * train_per_iter}')\n",
        "        ts.model.train()\n",
        "        for sn, batch, gbs in train_loader:\n",
        "            m = train_batch(cfg, ts, batch, gbs)\n",
        "            if m: wandb.log(m, step=ts.step); pbar.update(ts.step - pbar.n)\n",
        "            if ema: ema.update(ts.model)\n",
        "        \n",
        "        # Aligned with Trelis: check min_eval_interval before evaluating\n",
        "        if it >= cfg.min_eval_interval and eval_loader is not None:\n",
        "            print('Evaluating...')\n",
        "            # Aligned with Trelis: deep copy train_state for EMA evaluation\n",
        "            if ema:\n",
        "                ts_eval = copy.deepcopy(ts)\n",
        "                ts_eval.model = ema.ema_copy(ts.model)\n",
        "            else:\n",
        "                ts_eval = ts\n",
        "            ts_eval.model.eval()\n",
        "            em = evaluate(cfg, ts_eval, eval_loader, train_meta)\n",
        "            if em: wandb.log(em, step=ts.step); print(em)\n",
        "            if ema: del ts_eval  # Aligned: clean up\n",
        "    \n",
        "    pbar.close()\n",
        "    \n",
        "    # Final efficiency summary for report\n",
        "    total_time = time.time() - exp_start_time\n",
        "    peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
        "    time_per_step = total_time / max(ts.step, 1)\n",
        "    \n",
        "    summary = {\n",
        "        'config': config_name,\n",
        "        'total_params': param_stats['total'],\n",
        "        'trainable_params': param_stats['trainable'],\n",
        "        'lora_params': param_stats['lora'],\n",
        "        'trainable_ratio': param_stats['ratio'],\n",
        "        'peak_memory_gb': peak_memory,\n",
        "        'total_time_s': total_time,\n",
        "        'time_per_step_ms': time_per_step * 1000,\n",
        "        'total_steps': ts.step,\n",
        "    }\n",
        "    \n",
        "    wandb.log({\n",
        "        'summary/peak_memory_gb': peak_memory,\n",
        "        'summary/total_time_s': total_time,\n",
        "        'summary/time_per_step_ms': time_per_step * 1000,\n",
        "    }, step=ts.step)\n",
        "    wandb.finish()\n",
        "    \n",
        "    print(f'\\n{\"=\"*70}')\n",
        "    print(f'üìà EXPERIMENT SUMMARY: {config_name}')\n",
        "    print(f'{\"=\"*70}')\n",
        "    print(f'   Trainable Params: {param_stats[\"trainable\"]:,} / {param_stats[\"total\"]:,} ({param_stats[\"ratio\"]:.2%})')\n",
        "    print(f'   Peak Memory: {peak_memory:.2f} GB')\n",
        "    print(f'   Total Time: {total_time:.1f}s ({total_time/60:.1f} min)')\n",
        "    print(f'   Time/Step: {time_per_step*1000:.1f} ms')\n",
        "    print(f'   Total Steps: {ts.step}')\n",
        "    print(f'{\"=\"*70}\\n')\n",
        "    \n",
        "    return ts, summary\n",
        "\n",
        "# Run the selected experiment\n",
        "print(f'Running: {SELECTED_CONFIG}')\n",
        "train_state, experiment_summary = launch_experiment(SELECTED_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory cleared. Free: 42255340544.00 GB\n",
            "======================================================================\n",
            "Experiment: baseline - Full training\n",
            "======================================================================\n",
            "Params: 6,828,034 total, 6,828,034 trainable (100.00%), 0 LoRA\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>efficiency/memory_gb</td><td>‚ñÅ</td></tr><tr><td>params/lora</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>params/trainable_ratio</td><td>‚ñÅ</td></tr><tr><td>train/accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/count</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/exact_accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/lm_loss</td><td>‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÅ</td></tr><tr><td>train/lr</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>efficiency/memory_gb</td><td>0.02743</td></tr><tr><td>params/lora</td><td>18445</td></tr><tr><td>params/total</td><td>6846479</td></tr><tr><td>params/trainable</td><td>24077</td></tr><tr><td>params/trainable_ratio</td><td>0.00352</td></tr><tr><td>train/accuracy</td><td>0</td></tr><tr><td>train/count</td><td>0</td></tr><tr><td>train/exact_accuracy</td><td>0</td></tr><tr><td>train/lm_loss</td><td>2.53825</td></tr><tr><td>train/lr</td><td>0.0</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sudoku-lora-r1</strong> at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA/runs/ja9b3fkj' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA/runs/ja9b3fkj</a><br> View project at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251209_212606-ja9b3fkj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251209_212856-du9g8dxz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA/runs/du9g8dxz' target=\"_blank\">sudoku-baseline</a></strong> to <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA/runs/du9g8dxz' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA/runs/du9g8dxz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Efficiency Metrics:\n",
            "   Total Params: 6,828,034\n",
            "   Trainable: 6,828,034 (100.00%)\n",
            "   LoRA Params: 0\n",
            "   Memory: 0.25 GB\n",
            "Epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "{'eval/accuracy': 0.4120477709174156, 'eval/exact_accuracy': 0.0, 'eval/q_halt_accuracy': 1.0, 'eval/steps': 16.0, 'eval/lm_loss': 1.6305569865019307, 'eval/q_halt_loss': 0.0016139806667342781}\n",
            "Epoch 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "{'eval/accuracy': 0.6021992009878159, 'eval/exact_accuracy': 9.765625e-06, 'eval/q_halt_accuracy': 0.999990234375, 'eval/steps': 16.0, 'eval/lm_loss': 0.8943529775610007, 'eval/q_halt_loss': 0.00011378311552107334}\n",
            "Epoch 3000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "{'eval/accuracy': 0.6380148100852966, 'eval/exact_accuracy': 0.01474609375, 'eval/q_halt_accuracy': 0.98525390625, 'eval/steps': 16.0, 'eval/lm_loss': 0.8032578144405588, 'eval/q_halt_loss': 0.06528268065303564}\n",
            "Epoch 4000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "{'eval/accuracy': 0.6436900877952576, 'eval/exact_accuracy': 0.0227734375, 'eval/q_halt_accuracy': 0.992333984375, 'eval/steps': 16.0, 'eval/lm_loss': 0.7942803491411385, 'eval/q_halt_loss': 0.015955868144519627}\n",
            "Epoch 5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "{'eval/accuracy': 0.6355900460481644, 'eval/exact_accuracy': 0.029365234375, 'eval/q_halt_accuracy': 0.99392578125, 'eval/steps': 16.0, 'eval/lm_loss': 0.8906812705269657, 'eval/q_halt_loss': 0.02274650321342051}\n",
            "Epoch 6000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "{'eval/accuracy': 0.6267912179231644, 'eval/exact_accuracy': 0.04251953125, 'eval/q_halt_accuracy': 0.94431640625, 'eval/steps': 16.0, 'eval/lm_loss': 1.0942984497024826, 'eval/q_halt_loss': 0.14879593133926391}\n",
            "Epoch 7000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "{'eval/accuracy': 0.6251004391908646, 'eval/exact_accuracy': 0.046142578125, 'eval/q_halt_accuracy': 0.99541015625, 'eval/steps': 16.0, 'eval/lm_loss': 1.0419413143575298, 'eval/q_halt_loss': 0.02488753373734653}\n",
            "Epoch 8000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "{'eval/accuracy': 0.6287447053194046, 'eval/exact_accuracy': 0.0571484375, 'eval/q_halt_accuracy': 0.99806640625, 'eval/steps': 16.0, 'eval/lm_loss': 0.9874203723998375, 'eval/q_halt_loss': 0.012353496546857058}\n",
            "Epoch 9000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9765/9765 [1:10:38<00:00,  2.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval/accuracy': 0.6319086450338364, 'eval/exact_accuracy': 0.07302734375, 'eval/q_halt_accuracy': 0.998203125, 'eval/steps': 16.0, 'eval/lm_loss': 1.0002173564546688, 'eval/q_halt_loss': 0.011818661601282657}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>efficiency/memory_gb</td><td>‚ñÅ</td></tr><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>eval/exact_accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>eval/lm_loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>eval/q_halt_accuracy</td><td>‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñÅ‚ñá‚ñà‚ñà</td></tr><tr><td>eval/q_halt_loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>eval/steps</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>params/lora</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>+12</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>efficiency/memory_gb</td><td>0.24753</td></tr><tr><td>eval/accuracy</td><td>0.63191</td></tr><tr><td>eval/exact_accuracy</td><td>0.07303</td></tr><tr><td>eval/lm_loss</td><td>1.00022</td></tr><tr><td>eval/q_halt_accuracy</td><td>0.9982</td></tr><tr><td>eval/q_halt_loss</td><td>0.01182</td></tr><tr><td>eval/steps</td><td>16</td></tr><tr><td>params/lora</td><td>0</td></tr><tr><td>params/total</td><td>6828034</td></tr><tr><td>params/trainable</td><td>6828034</td></tr><tr><td>+12</td><td>...</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sudoku-baseline</strong> at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA/runs/du9g8dxz' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA/runs/du9g8dxz</a><br> View project at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251209_212856-du9g8dxz/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üìà EXPERIMENT SUMMARY: baseline\n",
            "======================================================================\n",
            "   Trainable Params: 6,828,034 / 6,828,034 (100.00%)\n",
            "   Peak Memory: 17.41 GB\n",
            "   Total Time: 4238.2s (70.6 min)\n",
            "   Time/Step: 423.8 ms\n",
            "   Total Steps: 10000\n",
            "======================================================================\n",
            "\n",
            "GPU memory cleared. Free: 41997966848.00 GB\n",
            "======================================================================\n",
            "Experiment: lora-r1 - LoRA r=1\n",
            "======================================================================\n",
            "LoRA enabled: 10 layers, rank=1, alpha=16.0, train_base=False\n",
            "Params: 6,846,479 total, 24,077 trainable (0.35%), 18,445 LoRA\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251209_223936-cuxfg9ly</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA/runs/cuxfg9ly' target=\"_blank\">sudoku-lora-r1</a></strong> to <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA/runs/cuxfg9ly' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-LoRA/runs/cuxfg9ly</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Efficiency Metrics:\n",
            "   Total Params: 6,846,479\n",
            "   Trainable: 24,077 (0.35%)\n",
            "   LoRA Params: 18,445\n",
            "   Memory: 0.50 GB\n",
            "Epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 13: Run All Experiments & Generate Comparison Table\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "\n",
        "# Collect all experiment summaries\n",
        "all_summaries = []\n",
        "for name in ['baseline', 'lora-r1', 'lora-r4', 'lora-r16', 'lora-r4-full']:\n",
        "    _, summary = launch_experiment(name)\n",
        "    all_summaries.append(summary)\n",
        "    torch.cuda.empty_cache()  # Clear memory between experiments\n",
        "\n",
        "# Create comparison DataFrame for report\n",
        "df = pd.DataFrame(all_summaries)\n",
        "df['param_reduction'] = df['total_params'] / df['trainable_params']\n",
        "df = df.round({'trainable_ratio': 4, 'peak_memory_gb': 2, 'time_per_step_ms': 1, 'param_reduction': 1})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä LORA EFFICIENCY COMPARISON TABLE (For Report)\")\n",
        "print(\"=\"*80)\n",
        "print(df[['config', 'trainable_params', 'trainable_ratio', 'param_reduction', 'peak_memory_gb', 'time_per_step_ms']].to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save to CSV for report\n",
        "df.to_csv('lora_experiment_results.csv', index=False)\n",
        "print(\"‚úÖ Results saved to lora_experiment_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Cell 14: Generate Report Figures\n",
        "# ============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load results if not in memory\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    df = pd.read_csv('lora_experiment_results.csv')\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Plot 1: Trainable Parameters\n",
        "ax1 = axes[0]\n",
        "colors = ['#FF6B6B' if 'baseline' in c else '#4ECDC4' for c in df['config']]\n",
        "ax1.bar(df['config'], df['trainable_params'], color=colors)\n",
        "ax1.set_ylabel('Trainable Parameters')\n",
        "ax1.set_title('Parameter Efficiency')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(df['trainable_params']):\n",
        "    ax1.text(i, v + 50000, f'{v/1e6:.2f}M', ha='center', fontsize=9)\n",
        "\n",
        "# Plot 2: Memory Usage\n",
        "ax2 = axes[1]\n",
        "ax2.bar(df['config'], df['peak_memory_gb'], color=colors)\n",
        "ax2.set_ylabel('Peak Memory (GB)')\n",
        "ax2.set_title('Memory Efficiency')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 3: Time per Step\n",
        "ax3 = axes[2]\n",
        "ax3.bar(df['config'], df['time_per_step_ms'], color=colors)\n",
        "ax3.set_ylabel('Time per Step (ms)')\n",
        "ax3.set_title('Training Speed')\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('lora_efficiency_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"‚úÖ Figure saved to lora_efficiency_comparison.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
