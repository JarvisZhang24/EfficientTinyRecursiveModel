{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ§  TinyRecursiveModel: Original vs Optimized Comparison\n",
        "\n",
        "This notebook provides a comprehensive comparison between the **original TinyRecursiveModels** implementation and the **optimized Trelis version**. We'll train both versions on the Sudoku dataset and compare their performance using Weights & Biases.\n",
        "\n",
        "## ğŸ“‹ Structure\n",
        "\n",
        "| Part | Description |\n",
        "|------|-------------|\n",
        "| **Part 1** | Environment Setup & Utility Functions |\n",
        "| **Part 2** | Original TinyRecursiveModels Implementation |\n",
        "| **Part 3** | Optimized Implementation (LoRA, Dropout, EMA) |\n",
        "| **Part 4** | Training & Evaluation Framework |\n",
        "| **Part 5** | Experiment Configurations |\n",
        "| **Part 6** | Experiments & Comparison |\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸ“¦ Part 1: Environment Setup & Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Environment Installation\n",
        "\n",
        "Install all required dependencies. This cell should be run once at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Triton already installed\n",
            "\n",
            "======================================================================\n",
            "âœ… All dependencies installed successfully!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 1: Environment Installation\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Core dependencies\n",
        "!pip install -q torch adam-atan2 einops tqdm numpy\n",
        "\n",
        "# Configuration & logging\n",
        "!pip install -q pydantic argdantic wandb omegaconf hydra-core\n",
        "\n",
        "# Hugging Face & utilities\n",
        "!pip install -q huggingface_hub packaging coolname\n",
        "\n",
        "# Build tools (for adam-atan2)\n",
        "!pip install -q ninja wheel setuptools setuptools-scm\n",
        "\n",
        "# Optional: Triton for GPU acceleration (if available)\n",
        "try:\n",
        "    import triton\n",
        "    print(\"âœ… Triton already installed\")\n",
        "except ImportError:\n",
        "    !pip install -q triton\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… All dependencies installed successfully!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Import Libraries\n",
        "\n",
        "Import all necessary libraries and setup device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ“¦ Library Versions\n",
            "======================================================================\n",
            "PyTorch: 2.9.0+cu126\n",
            "NumPy: 2.0.2\n",
            "CUDA Available: True\n",
            "CUDA Version: 12.6\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 2: Import Libraries\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "from typing import Any, Tuple, Dict, Sequence, Optional, List\n",
        "from dataclasses import dataclass, field\n",
        "from copy import deepcopy\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, IterableDataset, get_worker_info\n",
        "\n",
        "# Scientific computing\n",
        "import numpy as np\n",
        "\n",
        "# Tensor operations\n",
        "import einops\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "# Progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Configuration\n",
        "import pydantic\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Hugging Face (for downloading Sudoku dataset)\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Logging\n",
        "import wandb\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Print versions\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ“¦ Library Versions\")\n",
        "print(\"=\"*70)\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Utility Functions\n",
        "\n",
        "Core utility functions used throughout the notebook:\n",
        "- `set_seed()`: Set random seeds for reproducibility\n",
        "- `trunc_normal_init_()`: Truncated normal initialization (JAX-style)\n",
        "- Device setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ² Random seed set to 42\n",
            "ğŸš€ Using CUDA: NVIDIA A100-SXM4-40GB\n",
            "\n",
            "======================================================================\n",
            "âœ… Utility functions loaded!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 3: Utility Functions\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Random Seed Setting\n",
        "# -----------------------------------------------------------------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Set random seeds for reproducibility across all libraries.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # For deterministic behavior (may impact performance)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    print(f\"ğŸ² Random seed set to {seed}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Truncated Normal Initialization (JAX-style)\n",
        "# -----------------------------------------------------------------------------\n",
        "def trunc_normal_init_(tensor: torch.Tensor, std: float = 1.0, lower: float = -2.0, upper: float = 2.0):\n",
        "    \"\"\"\n",
        "    Truncated normal initialization.\n",
        "    \n",
        "    NOTE: PyTorch nn.init.trunc_normal_ is not mathematically correct - \n",
        "    the std dev is not actually the std dev of initialized tensor.\n",
        "    This function is a PyTorch version of JAX truncated normal init (default init method in Flax).\n",
        "    \n",
        "    References:\n",
        "    - https://github.com/jax-ml/jax/blob/main/jax/_src/random.py#L807-L848\n",
        "    - https://github.com/jax-ml/jax/blob/main/jax/_src/nn/initializers.py#L162-L199\n",
        "    \n",
        "    Args:\n",
        "        tensor: Tensor to initialize in-place\n",
        "        std: Standard deviation of the truncated normal distribution\n",
        "        lower: Lower truncation bound (in units of std)\n",
        "        upper: Upper truncation bound (in units of std)\n",
        "    \n",
        "    Returns:\n",
        "        The initialized tensor\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        if std == 0:\n",
        "            tensor.zero_()\n",
        "        else:\n",
        "            sqrt2 = math.sqrt(2)\n",
        "            a = math.erf(lower / sqrt2)\n",
        "            b = math.erf(upper / sqrt2)\n",
        "            z = (b - a) / 2\n",
        "\n",
        "            c = (2 * math.pi) ** -0.5\n",
        "            pdf_u = c * math.exp(-0.5 * lower ** 2)\n",
        "            pdf_l = c * math.exp(-0.5 * upper ** 2)\n",
        "            comp_std = std / math.sqrt(1 - (upper * pdf_u - lower * pdf_l) / z - ((pdf_u - pdf_l) / z) ** 2)\n",
        "\n",
        "            tensor.uniform_(a, b)\n",
        "            tensor.erfinv_()\n",
        "            tensor.mul_(sqrt2 * comp_std)\n",
        "            tensor.clip_(lower * comp_std, upper * comp_std)\n",
        "\n",
        "    return tensor\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Device Setup\n",
        "# -----------------------------------------------------------------------------\n",
        "def get_device():\n",
        "    \"\"\"Get the best available device (CUDA > MPS > CPU).\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"ğŸš€ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"ğŸ Using Apple MPS\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"ğŸ’» Using CPU\")\n",
        "    return device\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Helper function to find nearest multiple\n",
        "# -----------------------------------------------------------------------------\n",
        "def find_multiple(a: int, b: int) -> int:\n",
        "    \"\"\"Find the smallest multiple of b that is >= a.\"\"\"\n",
        "    return (-(a // -b)) * b\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Initialize\n",
        "# -----------------------------------------------------------------------------\n",
        "set_seed(42)\n",
        "DEVICE = get_device()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Utility functions loaded!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Configuration Management\n",
        "\n",
        "Define all configuration classes and constants used in the experiments:\n",
        "- `IGNORE_LABEL_ID`: Special token for ignored labels in loss computation\n",
        "- `PuzzleDatasetMetadata`: Metadata structure for puzzle datasets\n",
        "- `TrainingConfig`: Training hyperparameters\n",
        "- `ModelConfig`: Model architecture configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Configuration classes defined!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ Available configurations:\n",
            "  - TRMConfig: Model architecture\n",
            "  - TrainingConfig: Training hyperparameters\n",
            "  - SudokuConfig: Sudoku dataset settings\n",
            "  - PuzzleDatasetMetadata: Dataset metadata structure\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 4: Configuration Management\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Constants\n",
        "# -----------------------------------------------------------------------------\n",
        "IGNORE_LABEL_ID = -100  # PyTorch standard ignore index for cross-entropy loss\n",
        "\n",
        "# Dihedral transforms for data augmentation (rotation/flip symmetries)\n",
        "DIHEDRAL_INVERSE = [0, 3, 2, 1, 4, 5, 6, 7]\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Dataset Metadata\n",
        "# -----------------------------------------------------------------------------\n",
        "class PuzzleDatasetMetadata(pydantic.BaseModel):\n",
        "    \"\"\"Metadata for puzzle datasets (Sudoku, ARC, Maze, etc.).\"\"\"\n",
        "    pad_id: int                      # Token ID used for padding\n",
        "    ignore_label_id: Optional[int]   # Token ID to ignore in loss computation\n",
        "    blank_identifier_id: int         # ID for blank/unknown puzzle identifier\n",
        "    vocab_size: int                  # Total vocabulary size\n",
        "    seq_len: int                     # Sequence length (e.g., 81 for 9x9 Sudoku)\n",
        "    num_puzzle_identifiers: int      # Number of unique puzzle identifiers\n",
        "    total_groups: int                # Total number of puzzle groups\n",
        "    mean_puzzle_examples: float      # Average examples per puzzle\n",
        "    total_puzzles: int               # Total number of puzzles\n",
        "    sets: List[str]                  # Dataset split names (e.g., ['all'])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Model Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "@dataclass\n",
        "class TRMConfig:\n",
        "    \"\"\"\n",
        "    Configuration for TinyRecursiveReasoningModel.\n",
        "    \n",
        "    Architecture:\n",
        "    - H_cycles: Number of high-level (outer) recursion cycles\n",
        "    - L_cycles: Number of low-level (inner) recursion cycles  \n",
        "    - H_layers: Number of transformer layers in H-block (0 = no H-block)\n",
        "    - L_layers: Number of transformer layers in L-block\n",
        "    \n",
        "    The model uses Adaptive Computation Time (ACT) to learn when to halt.\n",
        "    \"\"\"\n",
        "    # Vocabulary & Embedding\n",
        "    vocab_size: int = 11            # 0-9 digits + padding\n",
        "    seq_len: int = 81               # 9x9 = 81 for Sudoku\n",
        "    hidden_size: int = 512          # Model dimension\n",
        "    \n",
        "    # Attention\n",
        "    num_heads: int = 8              # Number of attention heads\n",
        "    head_dim: int = 64              # Dimension per head (hidden_size // num_heads)\n",
        "    \n",
        "    # FFN\n",
        "    expansion: int = 4              # FFN expansion factor\n",
        "    \n",
        "    # Recursion structure\n",
        "    H_cycles: int = 3               # High-level recursion cycles\n",
        "    L_cycles: int = 6               # Low-level recursion cycles  \n",
        "    H_layers: int = 0               # Transformer layers in H-block\n",
        "    L_layers: int = 2               # Transformer layers in L-block\n",
        "    \n",
        "    # ACT (Adaptive Computation Time)\n",
        "    halt_exploration_prob: float = 0.1   # Probability of random halting during training\n",
        "    halt_max_steps: int = 16             # Maximum ACT steps during training\n",
        "    halt_max_steps_eval: Optional[int] = None  # Max steps during eval (None = use halt_max_steps)\n",
        "    \n",
        "    # Puzzle embeddings\n",
        "    puzzle_emb_len: int = 16        # Length of puzzle-specific embedding\n",
        "    puzzle_emb_ndim: int = 512      # Dimension of puzzle embedding\n",
        "    num_puzzle_identifiers: int = 1  # Number of unique puzzle types\n",
        "    \n",
        "    # Positional encoding\n",
        "    pos_encodings: str = \"rope\"     # Type: \"rope\" or \"none\"\n",
        "    \n",
        "    # Data types\n",
        "    forward_dtype: str = \"bfloat16\" # Computation dtype\n",
        "    \n",
        "    # Additional flags\n",
        "    mlp_t: bool = False             # Use MLP instead of transformer on L\n",
        "    no_ACT_continue: bool = True    # Only use halt sigmoid, no continue loss\n",
        "    \n",
        "    # Optimizations (Trelis version)\n",
        "    dropout: float = 0.0            # Dropout probability\n",
        "    use_lora: bool = False          # Enable LoRA adaptation\n",
        "    lora_rank: int = 8              # LoRA rank\n",
        "    lora_alpha: float = 16.0        # LoRA scaling factor\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        # Auto-compute head_dim if not specified\n",
        "        if self.head_dim is None:\n",
        "            self.head_dim = self.hidden_size // self.num_heads\n",
        "        # Set eval max steps\n",
        "        if self.halt_max_steps_eval is None:\n",
        "            self.halt_max_steps_eval = self.halt_max_steps\n",
        "        # Match puzzle embedding dim to hidden size\n",
        "        if self.puzzle_emb_ndim is None:\n",
        "            self.puzzle_emb_ndim = self.hidden_size\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Training Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Training hyperparameters.\"\"\"\n",
        "    # Batch size\n",
        "    global_batch_size: int = 768\n",
        "    \n",
        "    # Training schedule\n",
        "    epochs: int = 100000\n",
        "    eval_interval: int = 10000\n",
        "    checkpoint_every_eval: bool = True\n",
        "    \n",
        "    # Learning rate\n",
        "    lr: float = 1e-4\n",
        "    lr_min_ratio: float = 1.0       # Minimum LR as ratio of initial LR\n",
        "    lr_warmup_steps: int = 2000\n",
        "    \n",
        "    # Optimizer (AdamW-like)\n",
        "    beta1: float = 0.9\n",
        "    beta2: float = 0.95\n",
        "    weight_decay: float = 0.1\n",
        "    puzzle_emb_weight_decay: float = 0.1\n",
        "    \n",
        "    # Puzzle embedding LR\n",
        "    puzzle_emb_lr: float = 1e-2\n",
        "    \n",
        "    # Misc\n",
        "    seed: int = 42\n",
        "    min_eval_interval: int = 0\n",
        "    \n",
        "    # EMA (Exponential Moving Average)\n",
        "    ema: bool = False\n",
        "    ema_rate: float = 0.999\n",
        "    \n",
        "    # Freeze weights (learn only embeddings)\n",
        "    freeze_weights: bool = False\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Sudoku-specific Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "@dataclass  \n",
        "class SudokuConfig:\n",
        "    \"\"\"Configuration for Sudoku dataset.\"\"\"\n",
        "    source_repo: str = \"sapientinc/sudoku-extreme\"\n",
        "    output_dir: str = \"data/sudoku-extreme\"\n",
        "    \n",
        "    # Data sampling\n",
        "    subsample_size: Optional[int] = None  # None = use all data\n",
        "    min_difficulty: Optional[int] = None  # Filter by minimum difficulty rating\n",
        "    num_aug: int = 0                      # Number of augmentations per puzzle\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Configuration classes defined!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“‹ Available configurations:\")\n",
        "print(\"  - TRMConfig: Model architecture\")\n",
        "print(\"  - TrainingConfig: Training hyperparameters\")\n",
        "print(\"  - SudokuConfig: Sudoku dataset settings\")\n",
        "print(\"  - PuzzleDatasetMetadata: Dataset metadata structure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Weights & Biases Setup\n",
        "\n",
        "Configure Weights & Biases for experiment tracking and comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ“Š Configuring Weights & Biases\n",
            "======================================================================\n",
            "\n",
            "ğŸ“ W&B Project: TRM-Original-vs-Optimized\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 5: Weights & Biases Setup\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ“Š Configuring Weights & Biases\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Option 1: Login interactively (recommended for first time)\n",
        "# wandb.login()\n",
        "\n",
        "# Option 2: Login with API key (for Colab or automated runs)\n",
        "# Uncomment and replace with your API key:\n",
        "WANDB_API_KEY = ''\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "# Option 3: Use environment variable\n",
        "# os.environ[\"WANDB_API_KEY\"] = \"your-api-key-here\"\n",
        "\n",
        "# For now, let's try to login (will prompt if not already logged in)\n",
        "\n",
        "# Project configuration\n",
        "WANDB_PROJECT = \"TRM-Original-vs-Optimized\"\n",
        "WANDB_ENTITY = None  # Set to your W&B username/team if needed\n",
        "\n",
        "print(f\"\\nğŸ“ W&B Project: {WANDB_PROJECT}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Build Sudoku Dataset\n",
        "\n",
        "Download and preprocess the Sudoku dataset from Hugging Face Hub.\n",
        "\n",
        "The dataset contains:\n",
        "- **Input**: 9Ã—9 Sudoku puzzle with empty cells marked as 0\n",
        "- **Label**: Complete solution\n",
        "\n",
        "Data augmentation includes:\n",
        "- Digit permutation (relabeling 1-9)\n",
        "- Row/column band shuffling\n",
        "- Transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Sudoku dataset builder ready!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 6: Sudoku Dataset Builder\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# ============ é…ç½® ============\n",
        "SOURCE_REPO = \"sapientinc/sudoku-extreme\"\n",
        "TRAIN_SUBSAMPLE_SIZE = 10000  # è®­ç»ƒé›†é‡‡æ ·æ•°é‡ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨\n",
        "TEST_SUBSAMPLE_SIZE = 1000    # æµ‹è¯•é›†é‡‡æ ·æ•°é‡ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨\n",
        "NUM_AUG = 0                   # æ•°æ®å¢å¼ºæ¬¡æ•°\n",
        "\n",
        "# è¾“å‡ºç›®å½•ååŒ…å«é…ç½®ä¿¡æ¯\n",
        "train_str = f\"{TRAIN_SUBSAMPLE_SIZE}\" if TRAIN_SUBSAMPLE_SIZE else \"full\"\n",
        "test_str = f\"{TEST_SUBSAMPLE_SIZE}\" if TEST_SUBSAMPLE_SIZE else \"full\"\n",
        "OUTPUT_DIR = f\"data/sudoku-extreme-train{train_str}-test{test_str}-aug{NUM_AUG}\"\n",
        "\n",
        "# ============ æ•°æ®å¢å¼ºå‡½æ•° ============\n",
        "def shuffle_sudoku(board: np.ndarray, solution: np.ndarray):\n",
        "    \"\"\"å¯¹æ•°ç‹¬è¿›è¡Œç­‰ä»·å˜æ¢ï¼ˆä¿æŒæœ‰æ•ˆæ€§ï¼‰\"\"\"\n",
        "    # æ•°å­—æ˜ å°„ï¼šéšæœºç½®æ¢ 1-9\n",
        "    digit_map = np.pad(np.random.permutation(np.arange(1, 10)), (1, 0))\n",
        "    \n",
        "    # éšæœºè½¬ç½®\n",
        "    transpose_flag = np.random.rand() < 0.5\n",
        "\n",
        "    # è¡Œç½®æ¢ï¼šå…ˆæ‰“ä¹± 3 ä¸ª bandï¼Œå†æ‰“ä¹±æ¯ä¸ª band å†…çš„ 3 è¡Œ\n",
        "    bands = np.random.permutation(3)\n",
        "    row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
        "\n",
        "    # åˆ—ç½®æ¢ï¼šåŒç†\n",
        "    stacks = np.random.permutation(3)\n",
        "    col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
        "\n",
        "    # æ„å»º 81->81 çš„ä½ç½®æ˜ å°„\n",
        "    mapping = np.array([row_perm[i // 9] * 9 + col_perm[i % 9] for i in range(81)])\n",
        "\n",
        "    def apply_transformation(x: np.ndarray) -> np.ndarray:\n",
        "        if transpose_flag:\n",
        "            x = x.T\n",
        "        new_board = x.flatten()[mapping].reshape(9, 9).copy()\n",
        "        return digit_map[new_board]\n",
        "\n",
        "    return apply_transformation(board), apply_transformation(solution)\n",
        "\n",
        "# ============ å¤„ç†å•ä¸ªå­é›† ============\n",
        "def convert_subset(set_name: str):\n",
        "    \"\"\"å¤„ç†è®­ç»ƒé›†æˆ–æµ‹è¯•é›†\"\"\"\n",
        "    print(f\"\\nğŸ“¥ Processing {set_name} set...\")\n",
        "\n",
        "    # ä» HuggingFace ä¸‹è½½ CSV\n",
        "    csv_path = hf_hub_download(SOURCE_REPO, f\"{set_name}.csv\", repo_type=\"dataset\")\n",
        "\n",
        "    # è¯»å– CSV\n",
        "    inputs, labels = [], []\n",
        "    with open(csv_path, newline=\"\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)  # è·³è¿‡ header\n",
        "        for source, q, a, rating in tqdm(reader, desc=\"Reading CSV\"):\n",
        "            assert len(q) == 81 and len(a) == 81\n",
        "            inputs.append(np.frombuffer(q.replace('.', '0').encode(), dtype=np.uint8).reshape(9, 9) - ord('0'))\n",
        "            labels.append(np.frombuffer(a.encode(), dtype=np.uint8).reshape(9, 9) - ord('0'))\n",
        "\n",
        "    print(f\"  Loaded {len(inputs)} puzzles\")\n",
        "\n",
        "    # æ•°æ®é›†é‡‡æ ·\n",
        "    subsample_size = TRAIN_SUBSAMPLE_SIZE if set_name == \"train\" else TEST_SUBSAMPLE_SIZE\n",
        "    if subsample_size is not None and subsample_size < len(inputs):\n",
        "        indices = np.random.choice(len(inputs), size=subsample_size, replace=False)\n",
        "        inputs = [inputs[i] for i in indices]\n",
        "        labels = [labels[i] for i in indices]\n",
        "        print(f\"  Subsampled to {len(inputs)} puzzles\")\n",
        "\n",
        "    # æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒé›†ï¼‰\n",
        "    num_augments = NUM_AUG if set_name == \"train\" else 0\n",
        "\n",
        "    # æ„å»ºç»“æœ\n",
        "    results = {k: [] for k in [\"inputs\", \"labels\", \"puzzle_identifiers\", \"puzzle_indices\", \"group_indices\"]}\n",
        "    puzzle_id = 0\n",
        "    example_id = 0\n",
        "\n",
        "    results[\"puzzle_indices\"].append(0)\n",
        "    results[\"group_indices\"].append(0)\n",
        "\n",
        "    for orig_inp, orig_out in tqdm(zip(inputs, labels), total=len(inputs), desc=\"Augmenting\"):\n",
        "        for aug_idx in range(1 + num_augments):\n",
        "            if aug_idx == 0:\n",
        "                inp, out = orig_inp, orig_out\n",
        "            else:\n",
        "                inp, out = shuffle_sudoku(orig_inp, orig_out)\n",
        "\n",
        "            results[\"inputs\"].append(inp)\n",
        "            results[\"labels\"].append(out)\n",
        "            example_id += 1\n",
        "            puzzle_id += 1\n",
        "\n",
        "            results[\"puzzle_indices\"].append(example_id)\n",
        "            results[\"puzzle_identifiers\"].append(0)\n",
        "\n",
        "        results[\"group_indices\"].append(puzzle_id)\n",
        "\n",
        "    # è½¬æ¢ä¸º NumPy æ•°ç»„\n",
        "    def seq_to_numpy(seq):\n",
        "        arr = np.concatenate(seq).reshape(len(seq), -1)\n",
        "        assert np.all((arr >= 0) & (arr <= 9))\n",
        "        return arr + 1  # åç§» +1ï¼Œ0 ç•™ç»™ PAD\n",
        "\n",
        "    results = {\n",
        "        \"inputs\": seq_to_numpy(results[\"inputs\"]),\n",
        "        \"labels\": seq_to_numpy(results[\"labels\"]),\n",
        "        \"group_indices\": np.array(results[\"group_indices\"], dtype=np.int32),\n",
        "        \"puzzle_indices\": np.array(results[\"puzzle_indices\"], dtype=np.int32),\n",
        "        \"puzzle_identifiers\": np.array(results[\"puzzle_identifiers\"], dtype=np.int32),\n",
        "    }\n",
        "\n",
        "    # å…ƒæ•°æ®\n",
        "    metadata = PuzzleDatasetMetadata(\n",
        "        seq_len=81,\n",
        "        vocab_size=11,  # PAD (0) + digits 1-10 (representing 0-9)\n",
        "        pad_id=0,\n",
        "        ignore_label_id=0,\n",
        "        blank_identifier_id=0,\n",
        "        num_puzzle_identifiers=1,\n",
        "        total_groups=len(results[\"group_indices\"]) - 1,\n",
        "        mean_puzzle_examples=1,\n",
        "        total_puzzles=len(results[\"group_indices\"]) - 1,\n",
        "        sets=[\"all\"]\n",
        "    )\n",
        "\n",
        "    # ä¿å­˜\n",
        "    save_dir = os.path.join(OUTPUT_DIR, set_name)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    with open(os.path.join(save_dir, \"dataset.json\"), \"w\") as f:\n",
        "        json.dump(metadata.model_dump(), f, indent=2)\n",
        "\n",
        "    for k, v in results.items():\n",
        "        np.save(os.path.join(save_dir, f\"all__{k}.npy\"), v)\n",
        "\n",
        "    print(f\"  âœ… Saved to {save_dir}\")\n",
        "    print(f\"  ğŸ“Š Total examples: {results['inputs'].shape[0]}\")\n",
        "    \n",
        "    return metadata\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Sudoku dataset builder ready!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Download and Build Sudoku Dataset\n",
        "\n",
        "Actually download and preprocess the Sudoku dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ“¦ Building Sudoku Dataset\n",
            "======================================================================\n",
            "Source: sapientinc/sudoku-extreme\n",
            "Output: data/sudoku-extreme-train10000-test1000-aug0\n",
            "Train subsample: 10000\n",
            "Test subsample: 1000\n",
            "Augmentation: 0\n",
            "\n",
            "âœ… Dataset already exists! Loading metadata...\n",
            "\n",
            "======================================================================\n",
            "ğŸ“Š Dataset Summary\n",
            "======================================================================\n",
            "Train puzzles: 10,000\n",
            "Test puzzles: 1,000\n",
            "Sequence length: 81\n",
            "Vocabulary size: 11\n",
            "Dataset path: data/sudoku-extreme-train10000-test1000-aug0\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 7: Download and Build Sudoku Dataset\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ“¦ Building Sudoku Dataset\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Source: {SOURCE_REPO}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(f\"Train subsample: {TRAIN_SUBSAMPLE_SIZE}\")\n",
        "print(f\"Test subsample: {TEST_SUBSAMPLE_SIZE}\")\n",
        "print(f\"Augmentation: {NUM_AUG}\")\n",
        "\n",
        "# Check if already exists\n",
        "train_metadata_path = os.path.join(OUTPUT_DIR, \"train\", \"dataset.json\")\n",
        "test_metadata_path = os.path.join(OUTPUT_DIR, \"test\", \"dataset.json\")\n",
        "\n",
        "if os.path.exists(train_metadata_path) and os.path.exists(test_metadata_path):\n",
        "    print(\"\\nâœ… Dataset already exists! Loading metadata...\")\n",
        "    with open(train_metadata_path, \"r\") as f:\n",
        "        train_metadata = PuzzleDatasetMetadata(**json.load(f))\n",
        "    with open(test_metadata_path, \"r\") as f:\n",
        "        test_metadata = PuzzleDatasetMetadata(**json.load(f))\n",
        "else:\n",
        "    # Build train set\n",
        "    train_metadata = convert_subset(\"train\")\n",
        "    \n",
        "    # Build test set (with subsampling if TEST_SUBSAMPLE_SIZE is set)\n",
        "    test_metadata = convert_subset(\"test\")\n",
        "    \n",
        "    # Save identifiers.json (only once)\n",
        "    with open(os.path.join(OUTPUT_DIR, \"identifiers.json\"), \"w\") as f:\n",
        "        json.dump([\"<blank>\"], f)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“Š Dataset Summary\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Train puzzles: {train_metadata.total_puzzles:,}\")\n",
        "print(f\"Test puzzles: {test_metadata.total_puzzles:,}\")\n",
        "print(f\"Sequence length: {train_metadata.seq_len}\")\n",
        "print(f\"Vocabulary size: {train_metadata.vocab_size}\")\n",
        "print(f\"Dataset path: {OUTPUT_DIR}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Dataset and DataLoader\n",
        "\n",
        "Create PyTorch Dataset and DataLoader classes for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Dataset and DataLoader classes defined!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 8: Dataset and DataLoader Classes\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def _sample_batch(\n",
        "    rng: np.random.Generator,\n",
        "    group_order: np.ndarray,\n",
        "    puzzle_indices: np.ndarray,\n",
        "    group_indices: np.ndarray,\n",
        "    start_index: int,\n",
        "    global_batch_size: int\n",
        "):\n",
        "    \"\"\"Sample a batch of puzzle indices.\"\"\"\n",
        "    batch = []\n",
        "    batch_puzzle_indices = []\n",
        "    current_size = 0\n",
        "\n",
        "    while (start_index < group_order.size) and (current_size < global_batch_size):\n",
        "        group_id = group_order[start_index]\n",
        "        puzzle_id = rng.integers(group_indices[group_id], group_indices[group_id + 1])\n",
        "        start_index += 1\n",
        "\n",
        "        puzzle_start = puzzle_indices[puzzle_id]\n",
        "        puzzle_size = int(puzzle_indices[puzzle_id + 1] - puzzle_start)\n",
        "\n",
        "        append_size = min(puzzle_size, global_batch_size - current_size)\n",
        "\n",
        "        batch_puzzle_indices.append(np.full(append_size, puzzle_id, dtype=np.int32))\n",
        "        batch.append(puzzle_start + np.random.choice(puzzle_size, append_size, replace=False))\n",
        "\n",
        "        current_size += append_size\n",
        "\n",
        "    return start_index, np.concatenate(batch), np.concatenate(batch_puzzle_indices)\n",
        "\n",
        "\n",
        "class PuzzleDataset(IterableDataset):\n",
        "    \"\"\"\n",
        "    Iterable dataset for puzzle training.\n",
        "    \n",
        "    Features:\n",
        "    - Memory-efficient: uses memory-mapped arrays\n",
        "    - Supports distributed training\n",
        "    - Handles multiple epochs per iteration\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_paths: List[str],\n",
        "        global_batch_size: int,\n",
        "        seed: int = 42,\n",
        "        test_set_mode: bool = False,\n",
        "        epochs_per_iter: int = 1,\n",
        "        rank: int = 0,\n",
        "        num_replicas: int = 1,\n",
        "        split: str = \"train\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dataset_paths = dataset_paths\n",
        "        self.global_batch_size = global_batch_size\n",
        "        self.seed = seed\n",
        "        self.test_set_mode = test_set_mode\n",
        "        self.epochs_per_iter = epochs_per_iter\n",
        "        self.rank = rank\n",
        "        self.num_replicas = num_replicas\n",
        "        self.split = split\n",
        "\n",
        "        # Load metadata\n",
        "        self.metadata = self._load_merged_metadata()\n",
        "\n",
        "        # Compute local batch size\n",
        "        assert global_batch_size % num_replicas == 0\n",
        "        self.local_batch_size = global_batch_size // num_replicas\n",
        "\n",
        "        # State\n",
        "        self._data = None\n",
        "        self._iters = 0\n",
        "\n",
        "    def _load_merged_metadata(self) -> PuzzleDatasetMetadata:\n",
        "        \"\"\"Load and merge metadata from all dataset paths.\"\"\"\n",
        "        first_metadata = None\n",
        "        total_groups = 0\n",
        "        total_puzzles = 0\n",
        "        mean_puzzle_examples_sum = 0\n",
        "        \n",
        "        for path in self.dataset_paths:\n",
        "            with open(os.path.join(path, self.split, \"dataset.json\"), \"r\") as f:\n",
        "                meta = PuzzleDatasetMetadata(**json.load(f))\n",
        "            \n",
        "            if first_metadata is None:\n",
        "                first_metadata = meta\n",
        "            else:\n",
        "                # Validate consistency\n",
        "                assert first_metadata.seq_len == meta.seq_len\n",
        "                assert first_metadata.vocab_size == meta.vocab_size\n",
        "            \n",
        "            total_groups += meta.total_groups\n",
        "            total_puzzles += meta.total_puzzles\n",
        "            mean_puzzle_examples_sum += meta.mean_puzzle_examples * meta.total_puzzles\n",
        "        \n",
        "        return PuzzleDatasetMetadata(\n",
        "            seq_len=first_metadata.seq_len,\n",
        "            vocab_size=first_metadata.vocab_size,\n",
        "            pad_id=first_metadata.pad_id,\n",
        "            ignore_label_id=first_metadata.ignore_label_id,\n",
        "            blank_identifier_id=first_metadata.blank_identifier_id,\n",
        "            num_puzzle_identifiers=first_metadata.num_puzzle_identifiers,\n",
        "            total_groups=total_groups,\n",
        "            mean_puzzle_examples=mean_puzzle_examples_sum / total_puzzles if total_puzzles > 0 else 0,\n",
        "            total_puzzles=total_puzzles,\n",
        "            sets=first_metadata.sets\n",
        "        )\n",
        "\n",
        "    def _lazy_load_dataset(self):\n",
        "        \"\"\"Lazily load dataset arrays.\"\"\"\n",
        "        if self._data is not None:\n",
        "            return\n",
        "\n",
        "        field_mmap_modes = {\n",
        "            \"inputs\": \"r\",\n",
        "            \"labels\": \"r\",\n",
        "            \"puzzle_identifiers\": None,\n",
        "            \"puzzle_indices\": None,\n",
        "            \"group_indices\": None\n",
        "        }\n",
        "\n",
        "        self._data = {}\n",
        "        for set_name in self.metadata.sets:\n",
        "            for i, dataset_path in enumerate(self.dataset_paths):\n",
        "                key = set_name if i == 0 else f\"{set_name}{i}\"\n",
        "                self._data[key] = {\n",
        "                    field: np.load(\n",
        "                        os.path.join(dataset_path, self.split, f\"{set_name}__{field}.npy\"),\n",
        "                        mmap_mode=mmap\n",
        "                    )\n",
        "                    for field, mmap in field_mmap_modes.items()\n",
        "                }\n",
        "\n",
        "    def _collate_batch(self, batch):\n",
        "        \"\"\"Collate and convert batch to tensors.\"\"\"\n",
        "        batch = {k: v.astype(np.int32) for k, v in batch.items()}\n",
        "\n",
        "        # Convert ignore label IDs\n",
        "        if self.metadata.ignore_label_id is not None:\n",
        "            batch[\"labels\"][batch[\"labels\"] == self.metadata.ignore_label_id] = IGNORE_LABEL_ID\n",
        "\n",
        "        # Pad if needed\n",
        "        if batch[\"puzzle_identifiers\"].size < self.local_batch_size:\n",
        "            pad_size = self.local_batch_size - batch[\"puzzle_identifiers\"].size\n",
        "            pad_values = {\n",
        "                \"inputs\": self.metadata.pad_id,\n",
        "                \"labels\": IGNORE_LABEL_ID,\n",
        "                \"puzzle_identifiers\": self.metadata.blank_identifier_id\n",
        "            }\n",
        "            batch = {\n",
        "                k: np.pad(v, ((0, pad_size),) + ((0, 0),) * (v.ndim - 1), constant_values=pad_values[k])\n",
        "                for k, v in batch.items()\n",
        "            }\n",
        "\n",
        "        return {k: torch.from_numpy(v) for k, v in batch.items()}\n",
        "\n",
        "    def _iter_train(self):\n",
        "        \"\"\"Training iteration.\"\"\"\n",
        "        for set_name, dataset in self._data.items():\n",
        "            self._iters += 1\n",
        "\n",
        "            rng = np.random.Generator(np.random.Philox(seed=self.seed + self._iters))\n",
        "            group_order = np.concatenate([\n",
        "                rng.permutation(dataset[\"group_indices\"].size - 1)\n",
        "                for _ in range(self.epochs_per_iter)\n",
        "            ])\n",
        "            start_index = 0\n",
        "            \n",
        "            while start_index < group_order.size:\n",
        "                start_index, batch_indices, batch_puzzle_indices = _sample_batch(\n",
        "                    rng,\n",
        "                    group_order=group_order,\n",
        "                    puzzle_indices=dataset[\"puzzle_indices\"],\n",
        "                    group_indices=dataset[\"group_indices\"],\n",
        "                    start_index=start_index,\n",
        "                    global_batch_size=self.global_batch_size,\n",
        "                )\n",
        "\n",
        "                global_effective_batch_size = batch_puzzle_indices.size\n",
        "\n",
        "                # Drop last incomplete batch\n",
        "                if global_effective_batch_size < self.global_batch_size:\n",
        "                    break\n",
        "\n",
        "                # Select local batch\n",
        "                local_start = self.rank * self.local_batch_size\n",
        "                local_end = (self.rank + 1) * self.local_batch_size\n",
        "                \n",
        "                batch_indices = batch_indices[local_start:local_end]\n",
        "                batch_puzzle_indices = batch_puzzle_indices[local_start:local_end]\n",
        "                \n",
        "                batch = self._collate_batch({\n",
        "                    \"inputs\": dataset[\"inputs\"][batch_indices],\n",
        "                    \"labels\": dataset[\"labels\"][batch_indices],\n",
        "                    \"puzzle_identifiers\": dataset[\"puzzle_identifiers\"][batch_puzzle_indices]\n",
        "                })\n",
        "\n",
        "                yield set_name, batch, global_effective_batch_size\n",
        "\n",
        "    def _iter_test(self):\n",
        "        \"\"\"Test iteration (sequential).\"\"\"\n",
        "        for set_name, dataset in self._data.items():\n",
        "            total_examples = len(dataset[\"inputs\"])\n",
        "            start_index = 0\n",
        "            \n",
        "            while start_index < total_examples:\n",
        "                end_index = min(total_examples, start_index + self.global_batch_size)\n",
        "                \n",
        "                local_start = start_index + self.rank * self.local_batch_size\n",
        "                local_end = min(start_index + (self.rank + 1) * self.local_batch_size, end_index)\n",
        "                \n",
        "                # Get puzzle indices for this batch\n",
        "                puzzle_indices_list = []\n",
        "                puzzle_index = np.searchsorted(dataset[\"puzzle_indices\"], local_start, side=\"right\") - 1\n",
        "                for i in range(local_start, local_end):\n",
        "                    while puzzle_index + 1 < len(dataset[\"puzzle_indices\"]) and i >= dataset[\"puzzle_indices\"][puzzle_index + 1]:\n",
        "                        puzzle_index += 1\n",
        "                    puzzle_indices_list.append(puzzle_index)\n",
        "                \n",
        "                batch = self._collate_batch({\n",
        "                    \"inputs\": dataset[\"inputs\"][local_start:local_end],\n",
        "                    \"labels\": dataset[\"labels\"][local_start:local_end],\n",
        "                    \"puzzle_identifiers\": dataset[\"puzzle_identifiers\"][puzzle_indices_list]\n",
        "                })\n",
        "\n",
        "                yield set_name, batch, end_index - start_index\n",
        "                start_index += self.global_batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        worker_info = get_worker_info()\n",
        "        assert worker_info is None or worker_info.num_workers == 1\n",
        "        \n",
        "        self._lazy_load_dataset()\n",
        "        \n",
        "        if self.test_set_mode:\n",
        "            yield from self._iter_test()\n",
        "        else:\n",
        "            yield from self._iter_train()\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Dataset and DataLoader classes defined!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Visualization Utilities\n",
        "\n",
        "Helper functions for visualizing Sudoku puzzles and training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Visualization utilities ready!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 9: Visualization Utilities\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def visualize_sudoku(puzzle: np.ndarray, solution: np.ndarray = None, prediction: np.ndarray = None, title: str = \"Sudoku\"):\n",
        "    \"\"\"\n",
        "    Visualize a Sudoku puzzle with optional solution and prediction.\n",
        "    \n",
        "    Args:\n",
        "        puzzle: 9x9 input puzzle (1-10, where 1 = empty after preprocessing)\n",
        "        solution: 9x9 ground truth solution (optional)\n",
        "        prediction: 9x9 model prediction (optional)\n",
        "        title: Plot title\n",
        "    \"\"\"\n",
        "    # Convert back to 0-9 range\n",
        "    puzzle = puzzle - 1 if puzzle.max() > 9 else puzzle\n",
        "    if solution is not None:\n",
        "        solution = solution - 1 if solution.max() > 9 else solution\n",
        "    if prediction is not None:\n",
        "        prediction = prediction - 1 if prediction.max() > 9 else prediction\n",
        "    \n",
        "    n_plots = 1 + (solution is not None) + (prediction is not None)\n",
        "    fig, axes = plt.subplots(1, n_plots, figsize=(4 * n_plots, 4))\n",
        "    if n_plots == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    def draw_sudoku(ax, data, subtitle, highlight_errors=False, original=None):\n",
        "        ax.set_xlim(0, 9)\n",
        "        ax.set_ylim(0, 9)\n",
        "        ax.set_aspect('equal')\n",
        "        ax.axis('off')\n",
        "        ax.set_title(subtitle, fontsize=12, fontweight='bold')\n",
        "        \n",
        "        # Draw cells\n",
        "        for i in range(9):\n",
        "            for j in range(9):\n",
        "                val = data[i, j]\n",
        "                \n",
        "                # Determine cell color\n",
        "                if highlight_errors and original is not None and val != original[i, j]:\n",
        "                    color = '#ffcccc'  # Light red for errors\n",
        "                elif puzzle[i, j] == 0:  # Originally empty cell\n",
        "                    color = '#e6f3ff'  # Light blue for filled cells\n",
        "                else:\n",
        "                    color = 'white'\n",
        "                \n",
        "                rect = plt.Rectangle((j, 8-i), 1, 1, fill=True, facecolor=color, edgecolor='gray', linewidth=0.5)\n",
        "                ax.add_patch(rect)\n",
        "                \n",
        "                if val != 0:\n",
        "                    ax.text(j + 0.5, 8 - i + 0.5, str(val), ha='center', va='center',\n",
        "                           fontsize=14, fontweight='bold' if puzzle[i, j] != 0 else 'normal')\n",
        "        \n",
        "        # Draw 3x3 box borders\n",
        "        for i in range(4):\n",
        "            lw = 2 if i % 3 == 0 else 0.5\n",
        "            ax.axhline(y=i*3, color='black', linewidth=lw)\n",
        "            ax.axvline(x=i*3, color='black', linewidth=lw)\n",
        "    \n",
        "    # Draw puzzle\n",
        "    draw_sudoku(axes[0], puzzle, \"Input\")\n",
        "    \n",
        "    # Draw solution if provided\n",
        "    plot_idx = 1\n",
        "    if solution is not None:\n",
        "        draw_sudoku(axes[plot_idx], solution, \"Solution\")\n",
        "        plot_idx += 1\n",
        "    \n",
        "    # Draw prediction if provided\n",
        "    if prediction is not None:\n",
        "        draw_sudoku(axes[plot_idx], prediction, \"Prediction\", \n",
        "                   highlight_errors=solution is not None, original=solution)\n",
        "    \n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_training_curves(history: Dict[str, List[float]], title: str = \"Training Progress\"):\n",
        "    \"\"\"\n",
        "    Plot training and validation metrics.\n",
        "    \n",
        "    Args:\n",
        "        history: Dictionary with metric names as keys and lists of values\n",
        "        title: Plot title\n",
        "    \"\"\"\n",
        "    metrics = list(history.keys())\n",
        "    n_metrics = len(metrics)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, n_metrics, figsize=(5 * n_metrics, 4))\n",
        "    if n_metrics == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for ax, metric in zip(axes, metrics):\n",
        "        values = history[metric]\n",
        "        ax.plot(values, linewidth=2)\n",
        "        ax.set_xlabel('Step')\n",
        "        ax.set_ylabel(metric)\n",
        "        ax.set_title(metric)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Visualization utilities ready!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Test Dataset Loading\n",
        "\n",
        "Verify that the dataset loads correctly and visualize some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ§ª Testing Dataset Loading\n",
            "======================================================================\n",
            "\n",
            "ğŸ“Š Dataset metadata:\n",
            "  Sequence length: 81\n",
            "  Vocabulary size: 11\n",
            "  Total puzzles: 1000\n",
            "\n",
            "ğŸ“¦ Sample batch:\n",
            "  Set name: all\n",
            "  Batch size: 32\n",
            "  Input shape: torch.Size([32, 81])\n",
            "  Labels shape: torch.Size([32, 81])\n",
            "  Input range: [1, 10]\n",
            "  Labels range: [2, 10]\n",
            "\n",
            "ğŸ¨ Visualizing first example...\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAGNCAYAAAC7a38TAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAlvJJREFUeJzs3XdUFGfbx/HvItKkhWJBBQQ1xl6wIxYUjd3Y0Ggs0RhrrLHEJ6JRYyzEggUsEHtUVCxRiQXs3diNihUsidgLzd33Dx82bsCWJ8wMr9fnHM5hZ4adH7vLXBez99yrMxgMBoQQQgghhBCKM1M7gBBCCCGEEO8racaFEEIIIYRQiTTjQgghhBBCqESacSGEEEIIIVQizbgQQgghhBAqkWZcCCGEEEIIlUgzLoQQQgghhEqkGRdCCCGEEEIl0owLIYQQQgihEmnGhRDvlc6dO6PT6bhy5YraUf51MTEx6HQ6goKC/qf70el01KpV61/J9P9VUFAQOp2OmJgYtaMIIbI5acaFEBk8efKE8ePHU758eWxtbbG0tKRAgQLUqFGD4cOHExcXp3ZEzTIYDCxevJg6derg7OyMhYUFefLkoVy5cvTq1YvY2Fi1I2pSrVq10Ol0xi8zMzM++OADatSoQUREBAaDQe2IQgiRJczVDiCE0JZHjx7h6+vLiRMnKFy4MB06dMDZ2Zk7d+5w8OBBJkyYgLe3N97e3mpH1aSuXbsSERHBBx98QOPGjcmfPz/Pnj3j+PHjzJ8/n4cPH1KzZk21Y2rWoEGDsLW15fnz51y6dInVq1eze/dujhw5wowZM9SOJ4QQ/zppxoUQJqZOncqJEyfo1q0bYWFh6HQ6k/WXL18mOTlZpXTatmvXLiIiIihbtiyxsbHY29ubrL9//z5nzpxRKV32MHjwYPLmzWu8ffLkSSpXrszMmTMZOHAghQoVUjGdEEL8+2SYihDCxL59+wDo3bt3hkYcoFChQhQrVsxk2Y4dO+jatSsffvghtra22Nra4uPjQ1hYWKb7SB+TnJCQQPv27XFxccHOzo5GjRpx6dIlAM6ePUvz5s1xcnLCzs6OVq1acfv2bZP7uXLlCjqdjs6dO3P69GkaNWqEo6Mjtra2BAQEcOTIkXf63Xfu3EmTJk1wcXHB0tKSIkWKMHLkSJ4+ffpWP5/+2HXq1ClDIw7g6OhItWrVTJalD8/IzKvGtz979oxhw4ZRsGBBrKysKFmyJHPnzn1ttj179tCoUSOcnJywsrKiWLFijBo16q1/N4PBwIABA9DpdHz66aekpqa+dtx0REQEOp2OiIiIt7r/VylVqhQ1a9bEYDBw+PBh4N0fs5eHv2T2lZ4x/Wdf9fW24+hPnDhBYGAg+fLlw8LCAg8PD/r27UtiYuL/8lAIIf6fkjPjQggTzs7OAJw/f56yZcu+1c/88MMPXLx4kSpVqtCiRQvu37/P5s2b6dGjB7///jtTpkzJ8DP37t3D19eXvHnz0qlTJ86fP8+GDRs4d+4cUVFR1KhRgwoVKtC1a1eOHDlCZGQkd+/eZfv27Rnu69KlS1SvXp3y5cvTs2dPrl69ysqVK/Hz82P79u1Urlz5jb/D7Nmz6d27N46OjjRp0oTcuXNz+PBhxo0bx44dO9ixYwcWFhZv/dhlFb1eT9OmTdm6dSulSpWiffv2JCYmMmDAAGrXrp3pz6xcuZJ27dphaWlJ27ZtyZ07N9HR0YwZM4YtW7YQExODlZXVK/eZmppK586dWbp0Kf379yc4OPiVzXBW+qf7HDVqVKbLZ8+ezR9//IGNjQ0AzZs3x9PTM8N2+/btIzo62rjd66xbt442bdpgZmZGs2bNKFiwIGfOnCEkJIQtW7Zw4MABPvjgg3/0ewgh/p8yCCHES6KiogyAwc7OzjBo0CDDli1bDHfu3Hntz1y6dCnDstTUVEO9evUMOXLkMFy9etVkHWAADAMGDDBZ3rNnTwNgcHR0NEydOtW4XK/XGxo2bGgADEeOHDEuv3z5svG+hg0bZnJfmzdvNgCGUqVKmSzv1KmTATBcvnzZuOz06dMGc3NzQ5kyZTL8rt9//70BMEyePPm1j4HBYDBcv37dYG9vb9DpdIb27dsbVq5cabhy5cprf6ZmzZqGVx2KM8saHh5uAAwNGjQwpKWlGZefOHHCYGFhYQAMo0aNMi5/8OCBwcHBwWBpaWk4fvy4cfnz588Nbdu2NQCGMWPGmOwXMNSsWdNgMBgMjx49MgQEBBgAw/fff2+y3ahRowyAYceOHRmyp+cMDw9/7e//98fh5s2bJstPnTplsLa2Nuh0OuPj8K6PWWYmTJhgAAzNmjUzPH/+/JXbnTt3zuDo6GhwcnIynD9/3rg8s9/9zp07Bnt7e0P+/PkzPO/Lli0zAIY+ffq8NpcQ4v0jzbgQIoMpU6YYbG1tjY0uYPD29jb07t3bpCF5k8jISANgiIiIMFkOGGxtbQ1PnjwxWb5z507jvvR6vcm6hQsXGgDDggULjMvSm3FHR0fDo0ePMuzf39/fABgOHz5sXJZZs9avXz8DYNi5c2eG+3j+/LnB1dXVUKFChbf6nX/99VeDu7u7yWPn6upqaNOmjWHbtm0Ztn/XxrJ27doZ/ilJ9/nnn2doxtMft549e2bY/urVqwZzc3ODl5eXyfL0ZvzPP/80VKxY0ZAjRw6Txz1dVjTjgwYNMowaNcowcuRIw6effmqwtrY2AIZ+/fpl2DYzb9OMR0ZGGnQ6naF8+fKGx48fv3K7P//80+Dt7W2wsLAwxMbGmqzL7HcPDg42AIaFCxdmen/ly5c3uLi4vHJ/Qoj3kwxTEUJkMHDgQLp3787mzZvZu3cvhw8f5sCBA8ycOZP58+fz888/07RpU+P2jx49YvLkyaxdu5a4uDiePHlicn83btzIsI8iRYpkeNs/X758AJQuXTrDkIT0dZndV7ly5bC1tc2wvEaNGmzbto1jx45RoUKFV/6++/fvB2DLli1s27Ytw/qcOXNy7ty5V/78y+rWrUtcXBwxMTHs3LmTI0eOsHv3blasWMGKFSsYPnw448ePf6v7yszx48fJlSsX5cuXz7CuRo0azJ8/32TZsWPHADId7+zu7o6Xlxfnz5/n0aNH2NnZGdfdvn2b6tWrc/36ddasWUOTJk3+ceZ3kT6kSafTYW9vj4+PD59//jmfffbZv3L/hw8fpmPHjri5ubF+/Xpy5cqV6XbJycm0aNGCuLg4IiIi8PPze+N9p7+ODhw4kOn0n0lJSdy5c4c7d+7g4uLyv/0iQoj/N6QZF0Jkys7OjtatW9O6dWsAHjx4wIgRI5g1axaff/45CQkJWFhYkJKSQq1atTh69CjlypWjY8eOODs7Y25uzpUrV/jpp58ynX0lswsczc3N37guNTU1w7o8efJk+jukL3/w4MFrf9e7d+8CMG7cuNdu97bMzc2pW7cudevWBSAtLY2IiAh69uzJ999/T6tWrTJtpt/GgwcPKFiwYKbrMnscHj58+Mp18OKfnPPnz/Pw4UOTZvzmzZs8fPiQwoULv9WY+3/LzZs3TWZT+Tddv36dJk2aoNPpWL9+PW5ubq/c9vPPP2f37t2MGDGCTp06vdX9p7+OZs6c+drtnjx5Is24EMJIZlMRQrwVBwcHQkJC8PDw4M6dO5w8eRKAqKgojh49yueff87Ro0eZPXs2Y8eOJSgoiAYNGiiS7e+zrPx9uYODw2t/Pr35f/jwIYYXw/cy/fqnzM3N6datG+3btwdezD6TzszsxWE4LS0tw89l9k+Eg4MDf/75Z6b7yexxSP/dXvUY3bp1y2S7dGXLlmX+/PnExcVRu3btTH/+XbP/W/7Jfh89ekTjxo35448/WLp0KeXKlXvl/Y8ePZolS5bQunVrxo4d+9a50h/DkydPvvZ15OHh8db3KYT4/0+acSHEW9PpdBne1k9/O75Zs2YZtt+1a5ciuY4dO8bjx49fuf/XNV6A8cxv+jCDrJLZUJr0mTUSEhJMluv1eo4fP55h+zJlyvDkyROOHj2aYV1mj3f6757Z9IPXr18nLi4OLy8vk7Pi6bp06UJ4eDjnzp3LtCF/VXb4a3hMVnjXx+z58+cEBgZy4sQJJk2aZDLE6u+WLVtGUFAQlSpV4qeffnqnGVzSX0fpU1wKIcTbkGZcCGEiNDSUQ4cOZbpu7dq1nD17FkdHR0qWLAlgPMu3e/duk21jY2PfOPf1v+X+/fsZhpikj/8uWbLka8eLA/Tq1Qtzc3P69u3LtWvXMr3/t2kuN2/eTFRUVKZnbC9evMjKlSsB8PX1NS6vWLEiQIb5uIODg7l8+XKG++nYsSMA33zzDc+fPzcuP3nyJIsWLcqwfbNmzXBwcCA8PJzTp08blxsMBoYOHUpaWhqdO3d+5e/02WefERERwe+//06tWrWMZ9Jfzr5w4UL0er1x+b59+1iyZMkr7/N/9a6PWf/+/fnll1/44osvGDhw4Cvvd+/evXTp0gV3d3fWrVuHtbX1O+Xq0qULdnZ2fPPNNyaPdbqnT59m+T98QojsR8aMCyFMbNq0iS+//JLChQtTvXp13NzcePLkCceOHWPXrl2YmZkxa9YsLC0tAWjSpAmenp5MnDiRU6dOUbJkSX7//Xc2bNhAixYtWLVqVZZnrlGjBrNnz+bAgQNUqVKFK1eusHLlSqytrZk3b94bf75kyZLMmjWLnj178uGHH9KwYUO8vb159OgRly5dIjY2ls6dOzNnzpzX3s+5c+cYMGAALi4u+Pn54e3tjcFg4OLFi/zyyy+kpKTQs2dPkzHYXbp0YeLEiQQFBfHbb7/h7e3N4cOHOXXqFDVr1iQ2NtZkH506dWLp0qVs3ryZcuXK8fHHH3P37l2WLVtGQEAAGzZsMNne3t6euXPn0q5dOypXrkzbtm1xdXVl69atHDlyhEqVKjFkyJDX/l4dO3bEzMyMTp06UatWLXbs2EG+fPmoUqUK1atXZ/v27VStWhU/Pz+uXr1KVFQUTZo0Yc2aNW987P+Jd3nMDh48SEhICNbW1ri6uhIUFJTh/po3b07ZsmXp1q0bycnJVKpUidmzZ2fYztPT87X/uLi6urJs2TJat25NmTJlaNCgAcWKFSM5OZkrV64QGxtLtWrV2Lx587/xMAgh/r9QfgIXIYSWnTt3zjBx4kRDvXr1DIUKFTJYWVkZrKysDN7e3oZOnTqZTBOY7tKlS4aWLVsaXF1dDTY2NoaKFSsali9fbtixY0eGqfYMBtN5rF+WPlVhp06dMqzL7L5e3v7UqVOGhg0bGuzt7Q25cuUy1K1bN9Osr5v67uDBg4bAwECDm5ubIWfOnAYXFxdD+fLlDcOGDTOcPXv2TQ+d4Y8//jDMnTvX0KpVK8OHH35osLOzM+TMmdOQL18+Q+PGjQ2rVq3K9Od+++03g7+/v8HGxsZgb29vaNasmeHChQuvzPrkyRPD119/bcifP7/B0tLSULx4cUNYWNgrH2+D4cW0kR9//LHB0dHRYGFhYShatKjhP//5T6ZT+73q+Vm6dKkhR44chg8//NCQkJBgMBhezK392WefGZycnAzW1taGKlWqGLZs2fKvzTP+Km/7mKU/Jq/7Ss/o4eHx2u1efkxeN63juXPnDJ9//rnBw8PDYGFhYfjggw8MpUqVMvTr189w8ODBt/r9hBDvD53B8D9clSSEECq6cuUKhQoVolOnTv/zx64LIYQQapAx40IIIYQQQqhEmnEhhBBCCCFUIs24EEIIIYQQKpEx40IIIYQQQqhEzowLIYQQQgihEmnGhRBCCCGEUIk040IIIYQQQqhEmnEhhBBCCCFUIs24EEIIIYQQKpFmXAghhBBCCJVIMy6EEEIIIYRKpBkXQgghhBBCJdKMCyGEEEIzrly5gk6nQ6fTUatWrSzbT61atYz7uXLlSpbtR4g3kWZcZImgoCDjQa5z585qx8lUREQEQUFBBAUFcf/+fbXjCCFEthMfH0/37t3x9PTEwsICBwcHChcuTJMmTRgzZoyq2X777TfjMT4mJkbVLEK8jrnaAYRQS0REBLGxsQB07twZR0dHdQMJIUQ2cuvWLSpVqsTNmzeNy1JTU3n48CFxcXFs2rSJb7/9VrV8v/32G6NHjzbe/vtZ9hkzZvDgwQMA8uXLp2Q0IUxIMy6EEEKIdzZjxgxjI+7v70/v3r2xtbXlypUrHDx4kLVr16ob8A1KlSqldgQhABmmIhT08tCV8PBwpk6dSuHChbG0tKRMmTJs377dZPuXx/OdPHmS3r174+rqSq5cuWjcuDFxcXEm26dv6+np+cr7uXLlCjExMeh0OuNZcYBChQrJ2EEhhHgHR48eNX7/448/0qJFC+rVq0f37t2ZO3cuV69eNdn+1q1b9OvXD29vbywtLXF0dKRWrVqsXLnyrfbXuXNn43H65WEnERERxuVBQUEAeHp60qVLF+M2o0ePzrDN68aMr1q1itq1a+Po6IilpSVeXl706dPH5F2Av2eKjo7m22+/pUCBAlhZWVG9enWOHz/+Vr+beL/JmXGhirFjx3Lp0iXj7RMnTtC8eXOuXr3KBx98kGH71q1b8/vvvxtvb9y4kd9++43jx4/j7OysSGYhhBB/sbOzM34/cuRIhgwZQqVKlbCwsADAxsbGuP7y5ctUq1aNW7duGZelpKQQGxtLbGwsQ4cOZcKECcqFf42hQ4cyceJEk2WXL19m5syZREZGsnfvXgoVKpTh53r27GlS1/bu3Uvz5s25cOEC5ubSbolXkzPjQhWXLl1i6NChrFu3jjJlygDw6NEjli5dmun2iYmJhIeHs3LlSry8vABISEhg/Pjx77zvcuXKsWvXLsqWLWtctnLlSnbt2sWuXbtk7KAQQryFunXrGr9ft24dNWrUwM7ODl9fX6ZMmcKTJ0+M63v16mVsxGvVqsW6desIDg7GysoKgB9++IEDBw78a9lWrVrFiBEjjLe7dOliPMZ37dr1lT934MABYyNuZWXF5MmTWbduHbVr1wZenN3v1atXpj97/fp1fvjhB1avXk3BggWBFzPDbNmy5d/6tcT/U/KvmlBFs2bNjGdBnj59SmBgIAAXL17MdPvvv//eOCuLo6Mj9erVA2Dt2rVMmTLlnfbt4OCAr68vDg4OxmU+Pj4ZhrcIIYR4tc8//5ydO3eyZMkS47KUlBT27NnDnj17mD17NocOHcJgMBgbUktLS1atWmV8RzMhIcF4DF+2bBmVK1f+V7L5+Phw6tQp4213d3d8fX3f+HMvnxDq3bs3gwYNAqBq1aoUKFCA5ORktmzZwt27d3FycjL52V69evH1118DcP78eYYNGwa8uq4JkU7OjAtV1KxZ0/j9y8NMXjXF4MsH6EqVKhm/v3LlCgaD4d8PKIQQ4rVy5MjB4sWL2b9/P4MGDaJcuXKYmf3VVsTFxTFp0iQuXLhgPE57e3ubHPNfPp6fP39eufCv8HKGl+uOi4uL8V1Zg8GQaYP9rnVNiHTSjAtVvDwu/OWxdG/TWOt0uteuf/78ucntO3fuvGM6IYQQb6ty5cpMnjyZo0ePcuPGDT755BPjupcv8szMm47nr9r25eO8Usf4N2X9X+qaeL9JMy6yhYMHDxq/f3lcoaenp/EAmT7sJDExkdTUVODFmfNz585lep8vn8HR6/X/emYhhPj/bOfOnTx+/NhkWZ48eejUqZPx9vPnzylcuLDxOB0XF0diYqJx/cvH86JFi752fy8PLXz5QtDNmzdnuv0/Oca/nOHlupOYmGicwUun01G4cOG3uj8h3oaMGRfZwvDhwzE3NydXrlwMHz7cuLxZs2bG7wsXLsyRI0d49uwZ7du3x8/Pj1mzZmU4U57u5bMYc+fOpWHDhlhbW+Pj45N1v4gQQvw/ERYWxsaNG2ndujU1a9bEzc2N27dvm1xYX7FiRZydnalfvz6bN28mOTmZNm3aMGDAAOLi4pg1a5Zx23bt2r12fy83wCNHjuT+/fvs3buXbdu2Zbr9y8f4zZs34+fnh5WVFaVKlTJp7F/Wrl07pk+fDkBISAhubm4UKVKEqVOnkpycDED9+vUzjBcX4n8hzbjIFvLly2e8gPPlZS835l988QU9evQAXlxJv2rVKmxtbSlQoADx8fEZ7rN27dqsXr0agAkTJjBhwgQ8PDxknnEhhHhL9+/fZ+7cucydOzfDurx589KvXz8AZs6cSfXq1bl16xbbt2/P8LkSQ4cOfePFm+3atWP48OE8fvyYK1eu0KdPHwA++ugjzp49m2H7qlWrYmlpSXJyMocOHTJe+L9jx44Mn8aZrkqVKnz99ddMnDiRpKQkBg4cmOF3evkfCCH+DTJMRWQLy5Yto1+/fri6umJtbc3HH3/Mzp07cXV1NW7TrVs3hg8fTu7cubG2tqZOnTrs2rULb2/vTO+zR48eDB06FHd3d5O3M4UQQrzZqFGjmDhxIgEBAXh7e5MrVy4sLCzw9vamZ8+eHD58mLx58wLg5eXF0aNH6dOnD4UKFSJnzpzY29vj5+fHzz///FZzjDs7O7N27VpKly5t3M/MmTONM5j8nYuLC2vXrqVcuXJYW1u/9e/1ww8/sGLFCmrWrIm9vT05c+bE09OT3r17c/To0UznGBfif6EzyJUFQqNq1apl/JTMy5cvy9SDQgghhPh/R04HCiGEEEIIoRJpxoUQQgghhFCJNONCCCGEEEKoRMaMCyGEEEIIoRI5My6EEEIIIYRKpBkXQgghhBBCJdKMCyGEEEIIoZK3/gROnU6XlTmEECJbkstuXpAaIYQQpt62Prx1Mw5gZmZGnrz5/lGgf9uzp0+wtsmldgwTWst0588/yJ07t9oxjJ480dbjAy+es1y5tJPp9u0/cNXQc6a117TW8ty+dVPtCJoiNeLVtJZHa/UBtFcjpD68mdZe11rK8y714Z2a8Tx583Hk9/h3DpQVVi8K5ZOOPdSOYUJrmRpUL8Px48fVjmE0a04oLTpo5/EBWLcklB49tJOpdOkybNmrnedMa69preWp8GEBtSNoitSIV9NaHq3VB9BejZD68GZae11rKc+71AcZMy6EEEIIIYRKpBkXQgghhBBCJdKMCyGEEEIIoRJpxoUQQgghhFCJNONCCCGEEEKoRJpxIYQQQgghVCLNuBBCCCGEECqRZlyYMBgMLFmyhLp16+Ls7IyFhQVubm60bt2affv2qR1PdUlJSYwYMYKAgAA8PT2xs7MjZ86cuLi4UK1aNcaNG8eDBw/UjilEtrNp3RraNq1HCXdnCrlYUblkIXp2aUdC/HW1oxES/ANudjrc7HQcObhf0X0bDAZ+iVpNq4a1KVs4H165bfAt9yFf9+vB1cuXFM2SnkdLNeLmjQTmzpxKYLMAfD5yx8PJgjLeeen2aUuOHjqgeB6t1ohKJTyNr+G/f7X8uJbieSKXL+brfj1o4OeDp7MlbnY6fl4coXiOdD8vjnjl45P+1aaxf5bt/50+9Ef8/5aWlkabNm1Ys2aNyfKbN2+yatUqIiMjmTZtGn379lUpofoeP37M999/n2F5YmIi+/btY9++fYSHh3Pw4EGcnJxUSChE9mIwGBj61ZcsDg/D08ubpq0CsbW14/atG+zbHUv8tavkL1BQtXznzpxiyvhR2OTKxdMnTxTf/5gRgwkNCSZP3nw0aNwcWzt7zpw6zpKIuaxdtYx1W/dSrHhJRbJosUYsmDODmT/+gKeXNzXrBODs4srluAts3rCWzRvWMnPBUpq1bKtYHi3XCHsHB7r16p9heUF3T0VzAPzw3Ujir13FydmF3HnzEX/tquIZXlaidFkGDh+V6bqNa1fx+9nT1PSvn2X7l2ZcGAUHB5scZBs3bkzFihXZuXMn27Ztw2Aw0L9/fypVqkTlypVVTKqu/PnzU61aNTw8PHBycuLOnTtERkZy9eqLg0lcXBxhYWEMGzZM5aRCaN/82dNZHB5Gp+69GDtpOjly5DBZn5aWplIySE1N5asenShRqiyFChchcvliRff/x+1bzJ01lQLuHmzdexx7BwfjurCQHwkaPpDQGcH8OHuBInm0WCPKVahE5KYYqvrWNFl+YM8u2jTxZ/iAnjRo3BxLS0tF8oB2a4S9gyODRwQpus9XmRwyDy/vIhRw92DGlAl8HzRc1TwlS5elZOmyGZanpKQQHhqCubk5bT7tlGX7l2ZcGIWHhxu/9/X1Zf369QDo9XpKlSrFmTNn0Ov1jB8/nqioKLViqsrFxYX4+Iwf9z1w4EAKFPjro2/TD7pCiFd79uwZwRNG41HIi+8mTsvQiAOYm6tXpqZNGsf5s6fZsvsos36cqPj+r1+9gl6vp2KV6iaNOEDdBo0JGj6QxDt/KpZHizWiYbNPMl1euXoNqvnVJnZbNOdOn6RMeR9F8kiNeDt+teuqHeGtbN6wlnt3E2nQuDmuufNk2X6kGRdGly79Nf6wTJkyxu/NzMwoWbIkZ86cASA6Opq0tDRVi6RWPH/+nFu3bjF37lyT5SVKlFApkRDZR+y2aO7fu0fbT7vw/PlzojeuI+7ieRwcHalRqy6FvAurlu3Eb0eZPmkcQ74ZQ9FixVXJUMi7CBYWFhzav4dHDx9iZ29vXLd18wYAatTKunGsf5fdakRO85wA5FAxh5ZqREpyMj8vjuD2zRvY2ttTtnxFyld8f9/lfhtLf5oHQPtO3bJ0P9JNCSMHBwf+/PPFWZaTJ08alxsMBk6fPm28nZSUxMWLFylWrJjiGbVi69at1KtXL9N1fn5+dOuWtX+4Qvx/cOK3IwCY5ciBf5XSXLp43rjOzMyM7r0HMGr8ZMVzJScn89UXn1GidFl6Dfha8f2nc3J2ZsToCYweMQi/CsWo36iZccz4ntjtdOreiy49+iiWJzvViPjr19gVs5U8efPxUYlSiu9fizXij9u3GNCzi8myshUqMmvBMjy9vBXPo3Xx166yO2Yb+fIXoHa9Blm6L5lNRRg1adLE+P3OnTtp1qwZ3333HfXr1zc50ALcu3dP6XjZQvv27dm4cSNWVlZqRxFC8xL//AOAsJBg7B0c+CXmIBduPmLN5p14FS5K6Iwp/DRvtuK5Jo39lstxF/hxdnimQ2eU9EWfAcyOWM6TJ49ZOH8Os6ZOJGbrFsr5VKZF6/aKnn3OLjUiNTWVft07kpyczDdjflD9OXyZWjWibYcurNiwjROXbnPx9hOi9xyjVbuO/HbkEG2a+PP40SNF82QHyxeHo9fraftp5yx/DUkzLozGjx+Pl5eX8fa6dev49ttv+fXXXzNsa2FhoWQ0zSlatCiTJk1i7NixdO/eHWdnZwCWLl1KxYoV3/vxgEK8Db1eD0BOCwsWLFtL2QoVyWVrS+XqNQhbtBIzMzNCZ0xRNNPhA/uYM30yX309UrFZSl4neMIY+nbrQN9BIzh87vqLf1a27CI5OYlWDWuxZeM6xbJkhxqh1+vp/2Vn9u/Zyaedu9OqXUdVcmitRgwaPgrfmnVwcc2NjY0NJUuXZXrYQlq160j8tassiZj75jt5j+j1en5eHI5OpyOwY9cs358048IoT548HD58mCFDhlCkyIuxinny5KFRo0YMHTrUZFs3NzeVUmqDu7s7gwcP5ptvviEsLIwzZ86QL18+AM6dO0f//v3VDShENmBn/+KixDLlfMibz/SYUqx4STwKeXHlUhwP7t9XJE9aWhr9e3Tio5Kl6TNQ/dmQdu7YyuRxo+jyRR/6DhqGW/4CL/5ZqebLTyvWY54zJ2O+GaRYHq3XCL1ez8CeXVmzYiktAzvww7Q5imdIl11qRIcuPQA4tH+Pykm0ZeeOrSRcv0b1mnVw9yyU5fuTZlyY+OCDD5g4cSLnz58nOTmZW7dusWHDBh4/fmzcxsPDw3hQES/kzp2bKlWqGG/HxMSoF0aIbMK7yIfAiynXMpO+PCnpmSJ5njx+zKW4C5w+8RseThYmH/ixYulPADTxr4qbnY5N69dmeZ4d0ZsAqOZXO8O63HnyUrhoMS7HXeTJS8fnrKbVGqHX6xnwZRdWLP2J5q3bMXVOBGZm2mlxtFojnJxdAHj6VPk59LVsmUIXbqaTCziFkV6v5+HDhzg6Opos37VrF2FhYcbbXbp04X21Y8cOfHx8sLOzM1l+584dDhz469PedDqd0tGEyHaq/7fJvHj+bIZ1qampXLl0EZtcuXB2cVUkj4WlJe0++zzTdQf27ORS3AUCGjbF2cWVgh6eWZ4nJTUF4JXTFybe+RMzMzPMc+bM8iyg3RqR3oivXLaQpi3bMmPuItXGiWe3GnHs8ItManzwj1bdTUxky8YoPvjAiY+btFBkn9KMC6OnT5+SN29eAgICKF68OJaWlpw8eZKoqCjj2E4vLy8GDBigclL1TJs2jV9//RV/f39Kly6NjY0NCQkJREZGcvv2beN2jRs3VjGlENmDp5c3Nf0DiN0WzZKIeXza+a+zUCHBE3hw/z4tAzsodpGitbU1U2bOy3Rd/x6duRR3gb6DhlOhUpVMt/m3VaxSnfDQEMJCgmnUrKXJXOML58/hZkI8FatUV+wDbbRYI9KHpqxctpAmLVoTMm+xqhdsarFGXPj9HPkLumNjY5Nh+bhvXwwvatG6vWJ5tC5y+SJSUlLo+HkHxf62pBkXJpKTk1m/fr3xwxxe5u3tzS+//IL9S3Pdvo+ePn36yscIoGzZskyZouxFZ0JkV98Hz6Jp3WoM6dudLRvW4l20GKdPHGN37HYKuHvwn7GT1I6omiYtWrNw3mz279mJb7miBDRsir2DI6eOH2V37HasrK0J+j5Y0UxaqxHBE8awYulP5LK1xatwUaZOHJthmwaNm2f66YpZRWs1IipyOWEhwVSp5kd+dw9sbHJx6eJ5tkf/QmpqKn0HDaeKr59ieQCWRMzj0L7dAJw982KazKU/zWPfrhgAKlb1NfnnXEnLFs4HlBuiAtKMi5dYWVkxdOhQYmNjuXz5Mnfv3sXW1pbixYvzySef0LNnT6ytrdWOqarevXuTN29eDhw4wI0bN7h79y7m5ubkyZOH0qVL06JFCzp06EBOhd42FiK78/TyZtPOw0wa+y0xWzcTuz0a1zx56fxFbwYO+xYX19xqR1RNjhw5WBYVTVjIj6xfs4I1K5eSmpKCa+48tAzsQL9BIyhS7CPF8mixRsRfvQK8GO8/bdK4TLcp6O6pWDOuxRpR3a82F34/y+njxziwbxfPnj7FydmFOgEN6dS9F7X8AxTLku7Qvt3G6zCMy/bvMbmQVI1m/Njhg5w7c4pyPpUUnZ9emnFhZG5uzoQJE9SOoWn16tV75Qc5CCH+mfwFCjJ1TvibN1TR1NAIpoZGKL5fS0tL+g4aRt9B6s/uosUaodbz8iparBFVfWtS1bem2jFMaO15S1fOpxI3HhkU3692LjUWQgghhBDiPSPNuBBCCCGEECqRZlwIIYQQQgiVSDMuhBBCCCGESqQZF0IIIYQQQiXSjAshhBBCCKESacaFEEIIIYRQiTTjQgghhBBCqESacSGEEEIIIVQizbgQQgghhBAqkWZcCCGEEEIIlUgzLoQQQgghhEp0BoPB8FYb6nQ4OjoydvyErM70VpKTnmFpZa12DBNayzR75gz69u2rdgyjJ0+19fjAi+fMylo7mWaFyHP2Olr7Gxs5Yhj37t1TO4YmSI14Pa3l0Vp9AG0eb6Q+vJ4WnzOt5HmX+vBOzXg+t/wc+T3+fwr3b1m9KJRPOvZQO4YJrWVqUL0Mx48fVzuG0aw5obTooJ3HB+Q5exOtPWdae74qfFiAGwnaOCaqTWrE62ktj9aONSDHmzeR5+zNtPScvUt9kGEqQgghhBBCqESacSGEEEIIIVQizbgQQgghhBAqkWZcCCGEEEIIlUgzLoQQQgghhEqkGRdCCCGEEEIl0owLIYQQQgihEmnGhYlatWqh0+ne6ktoV7NmzUyeq1q1aqkdSYhMVSrhiZudLtOvlh/XUjTLzRsJzJ05lcBmAfh85I6HkwVlvPPS7dOWHD10QNEs6fR6PQtCQwjwLY9XbhuKutnTor4fWzauUzyL1uvDpnVraNu0HiXcnSnkYkXlkoXo2aUdCfHXFc0RuXwxX/frQQM/HzydLXGz0/Hz4ghFM7yJ2jXCYDDwS9RqWjWsTdnC+fDKbYNvuQ/5ul8Prl6+pGiWpKQkgoYNpEV9P8oVcaOQixVlvPPStG51li8KJzU1NcszmGf5HoQQilq0aBHr1ilfqIX4p+wdHOjWq3+G5QXdPRXNsWDODGb++AOeXt7UrBOAs4srl+MusHnDWjZvWMvMBUtp1rKtYnkMBgM9PmvDxqhIPL28affZ56QkJ7NlYxRdApsxdvIMuvboo1gerTIYDAz96ksWh4fh6eVN01aB2NracfvWDfbtjiX+2lXyFyioWJ4fvhtJ/LWrODm7kDtvPuKvXVVs329DCzVizIjBhIYEkydvPho0bo6tnT1nTh1nScRc1q5axrqteylWvKQiWZ48fszC+bMpW6ES/vUb4eziyoP799j+6yYG9upKVORylqzehJlZ1p2/lmZcmOjZsyeNGzfOsDwtLY3//Oc/pKWlAfDxxx8rHU28hRs3bvDVV1+pHUOId2Lv4MjgEUFqx6BchUpEboqhqm9Nk+UH9uyiTRN/hg/oSYPGzbG0tFQkz8aoSDZGRVKxSnWWr/sV6/9+NPuwUeP5uKYP330zmHoNGlPQw1ORPFqtD/NnT2dxeBiduvdi7KTp5MiRI0M+JU0OmYeXdxEKuHswY8oEvg8aruj+X0cLNeKP27eYO2sqBdw92Lr3OPYODsZ1YSE/EjR8IKEzgvlx9gJF8nzg5MS5hAdYWFiYLE9LSyOwaT1it0WzPXoTdRs0yrIM0owLE23bZn7WZ+nSpSYHtK+//lqpSOId9OjRg3v37uHu7o6zszPHjh1TO5IQ2UbDZp9kurxy9RpU86tN7LZozp0+SZnyPork2bIxCoB+g0cYG3EAZxcXvug9gG+H9mf54nCGfDNakTxarA/Pnj0jeMJoPAp58d3EaRkacQBzc2VbHb/adRXd37vQQo24fvUKer2eilWqmzTiAHUbNCZo+EAS7/ypWB4zM7MMjTi8eN00aNKCvbtiuHLpYtZmyNJ7F/9vTJ482fi9j4+PjEHWoIiICDZs2IBOp2PBggXY29urHUmIt5KSnMzPiyOYPmk8C0JDVBuf/To5zXMCkEPBxu6P27cAcPcolGFdwf8u2xO7XbE8r6JmfYjdFs39e/do0Kg5z58/55eo1cyYMoGF8+dwOS5rG6jsRis1opB3ESwsLDi0fw+PHj40Wbd18wYAatTyVyOaCb1eT8zWzQB8mMVDZuTMuHijrVu3mvz3LGfFtSchIYEBAwYA8OWXX+Lv7893332ncioh3s4ft28xoGcXk2VlK1Rk1oJleHp5q5TqL/HXr7ErZit58ubjoxKlFNuvk7MLANeuXqZIsY9M1l2/ehmASxfPK5YnM2rXhxO/HQHALEcO/KuUNnk8zMzM6N57AKPGT37Vj783tFQjnJydGTF6AqNHDMKvQjHqN2pmHDO+J3Y7nbr3oosK10KkpKQwffJ4DAYD9+4msjtmGxfPn6Nthy5Z/s+BNOPijSZNmmT83svLi08+yfytXKGe7t27c//+fQoVKsTEiRPVjiPEW2vboQuVq9WgWPGS2OSy5dLF84SFBLNq2SLaNPFn+/6T2NrZqZYvNTWVft07kpyczDdjfsh0GERWqVPvY6JWLSckeALVa9bBysoKgLuJicydNRWAhw/uK5YnM2rXh8Q//wAgLCSYUmXL80vMQYp8+BGnjh9jSL8vCJ0xBU8vbzp166loLq3RWo34os8A8rrlZ3CfbiycP8e4vFJVX1q0bq/40CKA1JQUgr//a8iXTqfjy36DGTH6+yzftwxTEa914sQJoqOjjbcHDhyoaDESb7ZgwQI2bdqETqcjPDwcW1tbtSMJ8dYGDR+Fb806uLjmxsbGhpKlyzI9bCGt2nUk/tpVlkTMVS2bXq+n/5ed2b9nJ5927k6rdh0V3X+LNu2p7lebA3t34V+lFN8M7svQr76kdqUS2Nm9GGKQlTM8vIkW6oNerwcgp4UFC5atpWyFiuSytaVy9RqELVqJmZkZoTOmKJpJa7RYI4InjKFvtw70HTSCw+euc+HmI9Zs2UVychKtGtZSZerOXLa23HhkIP7Bcw6fu8744JksWziPlh/XyjCc5t8mzbh4rZfHArq4uNC1a1cV04i/S0pKYuDAgQD06dOHmjVrvuEnhMgeOnTpAcCh/XtU2b9er2dgz66sWbGUloEd+GHanDf/0L/M3Nycxas3MWhEEDqdGUvCw/hl3WrqN2pG2KJVADi75lY8Vzot1Ac7+xcXAJYp50PefG4m64oVL4lHIS+uXIrjwf37imfTAi3WiJ07tjJ53Ci6fNGHvoOG4Za/wIt/oKr58tOK9ZjnzMmYbwapls/MzAy3/AXo1K0nE6eHcWj/HqZNGpe1+8zSexfZWnx8PMuXLzfe7t27t8kV/UJ9SUlJPHjwAIAZM2aYfIhDbGyscbvY2Fj58B+RraSPl3769Ini+9br9Qz4sgsrlv5E89btmDonQrUz0JaWlgwaPordx37nSmIyJy//wcTpody6mQC8aELVoJX64F3kQ+DF9JiZSV+elPRMoUTaosUasSN6EwDV/GpnWJc7T14KFy3G5biLPHn8OMuzvEnNOgEA7Nsdk6X7kWZcvNLUqVONnzxlbW1Nnz7y4RJCCGUcO/xiRhWlP/gnvRFfuWwhTVu2ZcbcRZocmrf65yUANGsVqMr+tVIfqv+3obt4/myGdampqVy5dBGbXLlwdnFVOpp4hZTUFIBXTl+YeOdPzMzMMM+ZU8lYmbp98wYA5uZZm0Uu4BSZevjwIXPn/jVWs0uXLri4uKiYSGTGwsKCli1bZrouNjaWO3fuAC/eQq5ZsyYlSpRQMp4Qr3Xh93PkL+iOjY1NhuXjvh0KQIvW7RXLkz40ZeWyhTRp0ZqQeYtVb8QfPXyI3d+moNuwdhXLFy2gbIWKNGyq/AX1WqoPnl7e1PQPIHZbNEsi5vFp527GdSHBE3hw/z4tAzuockGgFmixRlSsUp3w0BDCQoJp1KylyVzjC+fP4WZCPBWrVFfsw7XOnztDAXfPDMehp0+fEjTixRAf/4CGWZrh/Xx1ijcKDQ3l4X8vWMiRIweDBqk3fku8mo2NDatWrcp0Xa1atYxvQ5YoUeKV2wmhlqjI5YSFBFOlmh/53T2wscnFpYvn2R79C6mpqfQdNJwqvn6K5QmeMIYVS38il60tXoWLMnXi2AzbNGjcnJKlyyqWqVHtyrgVKEiRDz/C0tKK344cZO+uGDwKeRG2cKUq/yxorT58HzyLpnWrMaRvd7ZsWIt30WKcPnGM3bHbKeDuwX/GTnrznfyLlkTM49C+3QCcPXMSgKU/zWPfrhgAKlb1NfmnIStpsUY0adGahfNms3/PTnzLFSWgYVPsHRw5dfwou2O3Y2VtTdD3wYpkAVi3egVhIcFUqupLQXdPbO3suXUzge3Rm7h3N5HK1WrQvc+ALM0gzbjIIDU1lWnTphlvf/LJJ3h5eamYSAjx/1F1v9pc+P0sp48f48C+XTx7+hQnZxfqBDSkU/de1PIPUDRP/NUrADx5/PiVF2wVdPdUtBlv2rItm9at5uih/aSlplLQoxD9vx5Jz6+GZDhjrgQt1gdPL2827TzMpLHfErN1M7Hbo3HNk5fOX/Rm4LBvcVH4ItdD+3azYulPpsv27zG5GFmpZlyLcuTIwbKoaMJCfmT9mhWsWbmU1JQUXHPnoWVgB/oNGpFhXv2sVK9BY27fvMHhA3s5cnAfTx4/xs7egeIlS9OsZSCBn3XN8ndWpBkXGeTMmZP4+Hi1Y4j/UUxMjNoRhHitqr41qeqr/uwO6aaGRjA1NELtGCYGjwhi8IggtWMYabU+5C9QkKlzwtWOAWjzdZQZNWuEpaUlfQcNo++gYaplSFemvA9lyqtzIXQ6uYBTCCGEEEIIlUgzLoQQQgghhEqkGRdCCCGEEEIl0owLIYQQQgihEmnGhRBCCCGEUIk040IIIYQQQqhEmnEhhBBCCCFUIs24EEIIIYQQKpFmXAghhBBCCJVIMy6EEEIIIYRKpBkXQgghhBBCJdKMCyGEEEIIoRKdwWAwvNWGOh2Ojo6MHT8hqzO9ldTkZ1hbW6sdw8STp8+wtNJOplkzZ9C7T1+1YxilJMlz9iZae86Snmnr8dHa3/2wYcO4d++e2jE0QWrE68mx5s20ViPkOXszqRGv9i714Z2a8Xxu+Tnye/z/FO7fsm5JKD169FA7holZc0Jp0UE7mQKqlSF673G1YxjJc/ZmWnvOVi8K5ZOO2nl8tPYaKlCgAPHx2jgmqk1qxOvJsebN5Dl7PS0+Z1IjXu1d6oMMUxFCCCGEEEIl0owLIYQQQgihEmnGhRBCCCGEUIk040IIIYQQQqhEmnEhhBBCCCFUIs24EEIIIYQQKpFmXAghhBBCCJVIMy6EEEIIIYRK3qtmPCkpiREjRhAQEICnpyd2dnbkzJkTFxcXqlWrxrhx43jw4IHaMcXfeHp6otPpXvu1e/dutWOKbODYsWN07doVb29vrK2tsbe3p3DhwgQGBhIdHa12vPdS5PLFfN2vBw38fPB0tsTNTsfPiyNUyaLFGvHz4gjc7HSv/WrT2F/RTAaDgV+iVtOqYW3KFs6HV24bfMt9yNf9enD18iVFs0D2qBEhwT8Yn68jB/crvn+9Xs+C0BACfMvjlduGom72tKjvx5aN6xTPcvNGAnNnTiWwWQA+H7nj4WRBGe+8dPu0JUcPHVA8z8vUqhHmWXbPGvT48WO+//77DMsTExPZt28f+/btIzw8nIMHD+Lk5KRCQiFEVhk9ejSjR4/m5Q8dTkpK4tGjR8TFxWFra0tAQICKCd9PP3w3kvhrV3FydiF33nzEX7uqWhYt1ogSpcsycPioTNdtXLuK38+epqZ/fUWypBszYjChIcHkyZuPBo2bY2tnz5lTx1kSMZe1q5axbuteihUvqWgmLTt35hRTxo/CJlcunj55ovj+DQYDPT5rw8aoSDy9vGn32eekJCezZWMUXQKbMXbyDLr26KNYngVzZjDzxx/w9PKmZp0AnF1cuRx3gc0b1rJ5w1pmLlhKs5ZtFcuTTs0a8V414wD58+enWrVqeHh44OTkxJ07d4iMjOTq1RcFIC4ujrCwMIYNG6ZyUpGZSZMmZbq8UKFCCicR2cns2bMJCgoy3q5atSrVqlXDycmJu3fvcvbsWVxcXNQL+B6bHDIPL+8iFHD3YMaUCXwfNFzVPFqrESVLl6Vk6bIZlqekpBAeGoK5uTltPu2kSBaAP27fYu6sqRRw92Dr3uPYOzgY14WF/EjQ8IGEzgjmx9kLFMv0Mq3ViNTUVL7q0YkSpcpSqHARIpcvVjzDxqhINkZFUrFKdZav+xVra2sAho0az8c1ffjum8HUa9CYgh6eiuQpV6ESkZtiqOpb02T5gT27aNPEn+EDetKgcXMsLS0VyQPq14j3qhl3cXEhPj4+w/KBAwdSoEAB4+30g67QnsGDB6sdQWQzDx8+NGmc5syZQ48ePVRMJF7mV7uu2hGMslON2LxhLffuJtKgcXNcc+dRbL/Xr15Br9dTsUp1k0YcoG6DxgQNH0jinT8Vy/N3WqsR0yaN4/zZ02zZfZRZP05UJcOWjVEA9Bs8wtiIAzi7uPBF7wF8O7Q/yxeHM+Sb0Yrkadjsk0yXV65eg2p+tYndFs250ycpU95HkTxaqBHv1Zjxv3v+/DkJCQnMnTvXZHmJEiVUSiTexNvbGwsLC+zt7alUqRITJkzg6dOnascSGhYZGcnDhw8BKFCgAAkJCZQqVQobGxtcXFxo3rw5Bw6oO05RaJOWa8TSn+YB0L5TN0X3W8i7CBYWFhzav4dH//27Srd18wYAatRSdgz7y7RUI078dpTpk8YxcNgoihYrrkoGePFuBoC7R8Z3Bwr+d9me2O2KZnqVnOY5Achhrty5Yi3UiPfqzHi6rVu3Uq9evUzX+fn50a2bsgc38fYuXXpxcVBqaiqHDh3i0KFDLFq0iJiYGFxdXVVOJ7Ro7969xu/j4+P57rvvjLefPXtGVFQUGzduZMmSJbRp00aNiEJjtF4j4q9dZXfMNvLlL0Dteg0U3beTszMjRk9g9IhB+FUoRv1GzYxjxvfEbqdT9150UXD88d9ppUYkJyfz1RefUaJ0WXoN+Fqx/WbGyfnF8IprVy9TpNhHJuuuX70MwKWL5xXP9Xfx16+xK2YrefLm46MSpRTbrxZqxHt9Zvzv2rdvz8aNG7GyslI7ivibwoUL07VrV0aPHs1XX32Fm5ubcd2ZM2fo1auXiumElt28edPktqWlJX369GHIkCE4/Pdt9rS0NLp160ZiYqIaEUU2oZUasXxxOHq9nrafdiZHjhyK7/+LPgOYHbGcJ08es3D+HGZNnUjM1i2U86lMi9btMVfwrGY6rdWISWO/5XLcBX6cHa7Kc/SyOvU+BiAkeAJJSUnG5XcTE5k7ayoADx/cVyHZX1JTU+nXvSPJycl8M+YHRR8zLdSI9/LMeNGiRZk0aRLJyclcvXqV1atXk5iYyNKlSzl69CibN2/Gw8ND7ZjivzZv3kyxYsVMln333XdUrFiR33//HYA1a9bw4MED4x+OEOlSUlJMbk+aNIm+ffsCUKNGDZo2bQrAo0ePWLduHV26dFE8o9AWLdcIvV7Pz4vD0el0BHbsqkqG4AljmDZxLIO/GUPLwA44ODhy6sRvBA0fQKuGtZi7OJL6jZoqlkdrNeLwgX3MmT6ZQSOCNDGrTIs27VmxJII9O3fgX6UUteo2IC01lc0b1hqvNzAzU+/crF6vp/+Xndm/Zyefdu5Oq3YdFd2/FmrEe3lm3N3dncGDB/PNN98QFhbGmTNnyJcvHwDnzp2jf//+6gYUJv5+kAWws7Mz+YN4/vw558+r/zab0B5HR0eT27Vq1cr0e3gxU4YQWq4RO3dsJeH6NarXrIO7p/IzhOzcsZXJ40bR5Ys+9B00DLf8Bchla0vlar78tGI95jlzMuabQYpm0lKNSEtLo3+PTnxUsjR9BmpjVjZzc3MWr97EoBFB6HRmLAkP45d1q6nfqBlhi1YB4OyaW5Vser2egT27smbFUloGduCHaXMUz6CFGvFeNuN/lzt3bqpUqWK8HRMTo14Y8Y/pdDq1IwgNKlny1WemXp5PFlB9+IHQJi3ViGUqXbiZbkf0JgCq+dXOsC53nrwULlqMy3EXefL4sdLR3kiJGvHk8WMuxV3g9Inf8HCyMPlwphVLfwKgiX9V3Ox0bFq/NsvzpLO0tGTQ8FHsPvY7VxKTOXn5DyZOD+XWzQQAypRTZuaSl+n1egZ82YUVS3+ieet2TJ0TocoZei3UiPdqmMqOHTvw8fHBzs7OZPmdO3dMrpSVpk47Vq9ezbNnz2jbtq3JOMRHjx4RHh5uvG1hYcGHH36oRkShcY0aNWLUqL8+NCU2NpZSpV5cHLRz506TbX18lC9IQju0XiPuJiayZWMUH3zgxMdNWqiSISX1xVv6r5q+MPHOn5iZmWGeM6ciebRWIywsLWn32eeZrjuwZyeX4i4Q0LApzi6uis3r/Tqrf14CQLNWgYruN70RX7lsIU1btmXG3EWqja3XQo14r5rxadOm8euvv+Lv70/p0qWxsbEhISGByMhIbt++bdyucePGKqYUL7t27RoDBgxgyJAhfPzxx3h5eXHnzh1WrlxJQkKCcbsOHTpkKKBCAFSoUIH69euzZcsWAIYMGcKFCxewsrIymbKuWLFir5xBQ7wftF4jIpcvIiUlhY6fd1D0A1FeVrFKdcJDQwgLCaZRs5Ymc40vnD+HmwnxVKxSXbF8WqsR1tbWTJk5L9N1/Xt05lLcBfoOGk6FSlUy3SarPHr4EDt7e5NlG9auYvmiBZStUJGGTTOf+zsrpA9NWblsIU1atCZk3mJVL3LVQo14r5pxgKdPn7J+/XrWr1+f6fqyZcsyZcoUhVOJN7l58yYLFmT+iW5+fn5MmzZN4UQiOwkPD8ff35+zZ8+SlJTE9OnTTdbnzZuXVatWqT7rwftoScQ8Du3bDcDZMyeBF3No79sVA0DFqr582lm5IRlarhHLFs4H1BuiAtCkRWsWzpvN/j078S1XlICGTbF3cOTU8aPsjt2OlbU1Qd8HK55LasTrNapdGbcCBSny4UdYWlrx25GD7N0Vg0chL8IWrlT02Bc8YQwrlv5ELltbvAoXZerEsRm2adC4eaafPJtV1K4R71Uz3rt3b/LmzcuBAwe4ceMGd+/exdzcnDx58lC6dGlatGhBhw4dyKnQ22vizTp37oyLiwsbNmzgxIkT3L59m4cPH/LBBx9QtmxZ2rdvT8eOHaWJEq+VL18+Dh48yNSpU1m1ahUXL17k+fPneHp60qRJEwYPHkzu3OpcwPS+O7Rvt3EsrXHZ/j0c2r/HeFupZlzLNeLY4YOcO3OKcj6VFJ2D+e9y5MjBsqhowkJ+ZP2aFaxZuZTUlBRcc+ehZWAH+g0akWEu66wkNeLtNG3Zlk3rVnP00H7SUlMp6FGI/l+PpOdXQzKcMc9q8VevAC/G10+bNC7TbQq6eyrajKtdI96rZrxevXryNnQ24+joSIcOHejQoYPaUUQ2Z2try8iRIxk5cqTaUcRLpoZGMDU0Qu0YgLZrRDmfStx4ZHjzhgqwtLSk76Bh9B2k/mwh2alGqPlaHzwiiMEjglTZ999p6W/+ZWrWCJlNRQghhBBCCJVIMy6EEEIIIYRKpBkXQgghhBBCJdKMCyGEEEIIoRJpxoUQQgghhFCJNONCCCGEEEKoRJpxIYQQQgghVCLNuBBCCCGEECqRZlwIIYQQQgiVSDMuhBBCCCGESqQZF0IIIYQQQiU6g8FgeKsNdTrMzc0p9lHxrM70VvT65+Qwy6F2DBN6/XNy5NBOpkuXL+PpWUjtGEbynL2ZFp8zMw09Z1p7DZ09e5bU1BS1Y2iC1IjXk2PNm8lz9npafc6kRmTuXeqD+bvcsWvuPETvPf6PQv3bVi8K5ZOOPdSOYWLdklB69NBOptKly7BFI88XyHP2NuQ5ez2t5anwYQG1I2iK1IhXk2PNm8lz9nrynL2ZlvK8S32QYSpCCCGEEEKoRJpxIYQQQgghVCLNuBBCCCGEECqRZlwIIYQQQgiVSDMuhBBCCCGESqQZF0IIIYQQQiXSjAshhBBCCKESacaFEEIIIYRQiTTjGnHs2DG6du2Kt7c31tbW2NvbU7hwYQIDA4mOjlY7nvivhIQEZs+eTWBgIKVKlcLV1ZWcOXPi6upK3bp1WbhwIW/5obZCvPd+XhyBm53utV9tGvsrmslgMPBL1GpaNaxN2cL58Mptg2+5D/m6Xw+uXr6kaJaXaalGVCrh+crnq+XHtRTN8iohwT8YMx05uF+x/WqxRkQuX8zX/XrQwM8HT2dL3Ox0/Lw4QtEML0tKSiJo2EBa1PejXBE3CrlYUcY7L03rVmf5onBSU1MVz6TX61kQGkKAb3m8cttQ1M2eFvX92LJxnSL7f6dP4BRZY/To0YwePdrkDzQpKYlHjx4RFxeHra0tAQEBKiYU6RYtWsTw4cMzLL9z5w7btm1j27ZtrFq1ijVr1mjqY5SF0KISpcsycPioTNdtXLuK38+epqZ/fUUzjRkxmNCQYPLkzUeDxs2xtbPnzKnjLImYy9pVy1i3dS/FipdUNJMWa4S9gwPdevXPsLygu6eiOTJz7swppowfhU2uXDx98kTRfWuxRvzw3Ujir13FydmF3HnzEX/tqiL7fZUnjx+zcP5sylaohH/9Rji7uPLg/j22/7qJgb26EhW5nCWrN2Fmpsz5YoPBQI/P2rAxKhJPL2/affY5KcnJbNkYRZfAZoydPIOuPfpkaQZpxlU2e/ZsgoKCjLerVq1KtWrVcHJy4u7du5w9exYXFxf1AopM5c2bl4YNG+Ll5cWVK1dYvHgxSUlJAKxfv57w8HC6deumckohtK1k6bKULF02w/KUlBTCQ0MwNzenzaedFMvzx+1bzJ01lQLuHmzdexx7BwfjurCQHwkaPpDQGcH8OHuBYpm0WiPsHRwZPCLojdspLTU1la96dKJEqbIUKlyEyOWLVcmhpRoxOWQeXt5FKODuwYwpE/g+KOM/C0r6wMmJcwkPsLCwMFmelpZGYNN6xG6LZnv0Juo2aKRIno1RkWyMiqRileosX/cr1tbWAAwbNZ6Pa/rw3TeDqdegMQU9PLMsgzTjKnr48CHDhg0z3p4zZw49evRQMZF4E3d3dxYtWkRgYCDm5n/9+bRv3546deoYb2/atEmacSH+oc0b1nLvbiINGjfHNXcexfZ7/eoV9Ho9FatUN2nEAeo2aEzQ8IEk3vlTsTxSI97dtEnjOH/2NFt2H2XWjxMV378Wa4Rf7bqK7OdtmZmZZWjEAczNzWnQpAV7d8Vw5dJFxfJs2RgFQL/BI4yNOICziwtf9B7At0P7s3xxOEO+GZ1lGWTMuIoiIyN5+PAhAAUKFCAhIYFSpUphY2ODi4sLzZs358CBAyqnFC9r3749HTp0MDnIAtSuXRtnZ2fj7ZSUFKWjCfH/xtKf5gHQvpOy/9AW8i6ChYUFh/bv4dF/j83ptm7eAECNWsqNYddyjUhJTubnxRFMnzSeBaEhHD2kfq068dtRpk8ax8BhoyharLgqGaRG/HN6vZ6YrZsB+FDBoWB/3L4FgLtHoQzrCv532Z7Y7VmaQc6Mq2jv3r3G7+Pj4/nuu++Mt589e0ZUVBQbN25kyZIltGnTRo2I4i3dunWLBw8eGG9XqlRJxTRCZF/x166yO2Yb+fIXoHa9Boru28nZmRGjJzB6xCD8KhSjfqNmxjHje2K306l7L7pk8djRl2m5Rvxx+xYDenYxWVa2QkVmLViGp5e3olkAkpOT+eqLzyhRuiy9Bnyt+P7fRGpERikpKUyfPB6DwcC9u4nsjtnGxfPnaNuhi6L/9Do5vxjmde3qZYoU+8hk3fWrlwG4dPF8lmaQZlxFN2/eNLltaWlJ9+7dsba2JiwsjAcPHpCWlka3bt3w9/c3+a9aaEdaWhpffPEFaWlpAOTOnZsvv/xS5VRCZE/LF4ej1+tp+2lnVS6C/qLPAPK65Wdwn24snD/HuLxSVV9atG6f4YxnVtJqjWjboQuVq9WgWPGS2OSy5dLF84SFBLNq2SLaNPFn+/6T2NrZKZIl3aSx33I57gKbdx3R3MXzUiMyl5qSQvD3fw390Ol0fNlvMCNGf69ojjr1PiZq1XJCgidQvWYdrKysALibmMjcWVMBePjgfpZmkGEqKvr721STJk1ixowZTJw4kUWLFhmXP3r0iHXrlJleR7ybR48e0bRpU9avXw+AnZ0d69atw9XVVeVkQmQ/er2enxeHo9PpCOzYVZUMwRPG0LdbB/oOGsHhc9e5cPMRa7bsIjk5iVYNayk21Rlot0YMGj4K35p1cHHNjY2NDSVLl2V62EJatetI/LWrLImYq1gWgMMH9jFn+mS++nqk4jPdvInUiFfLZWvLjUcG4h885/C564wPnsmyhfNo+XGtDMPEslKLNu2p7lebA3t34V+lFN8M7svQr76kdqUS2NnZA2T5zC7SjKvI0dHR5HatWrUy/R4gLi4u6wOJd3L9+nV8fX3ZtGkTAK6urmzbto3KlSurnEyI7Gnnjq0kXL9G9Zp1cPfMOH5Tif1PHjeKLl/0oe+gYbjlL0AuW1sqV/PlpxXrMc+ZkzHfDFIsT3arER26vLi49ND+PYrtMy0tjf49OvFRydL0GTjszT+gIKkRb8fMzAy3/AXo1K0nE6eHcWj/HqZNGqfY/s3NzVm8ehODRgSh05mxJDyMX9atpn6jZoQtWgWAs2vuLM0gzbiKSpZ89X/wf/9QgPS3TYQ2HD58mMqVK3PixAkAihYtyr59+6hYsaLKyYTIvpapdOFmuh3RL5qman61M6zLnScvhYsW43LcRZ48fqxInuxWI9LH3j59qtzc3k8eP+ZS3AVOn/gNDycLkw8gWrH0JwCa+FfFzU7HpvVrFcslNeKfqVnnxXz5+3bHKLpfS0tLBg0fxe5jv3MlMZmTl/9g4vRQbt1MAKBMOZ8s3b+MGVdRo0aNGDXqrw+8iI2NpVSpUgDs3LnTZFsfn6x9IYi3t2bNGjp06MDTp08BqFGjBmvXrsXJyUnlZEJkX3cTE9myMYoPPnDi4yYtVMmQkvpiWMirpi9MvPMnZmZmmOfMqUie7FYjjh1+MaOKkh/8Y2FpSbvPPs903YE9O7kUd4GAhk1xdnHN0nmiXyY14p+7ffMGAObmyvyNvcnqn5cA0KxVYJbuR5pxFVWoUIH69euzZcsWAIYMGcKFCxewsrJi7ty/xtwVK1aMevXqqRVTvGTlypUEBgai1+sBcHBwoH79+ixYYPohIA4ODnTv3l2NiEJkS5HLF5GSkkLHzztgaWmpSoaKVaoTHhpCWEgwjZq1NJlrfOH8OdxMiKdileqK5dNijbjw+znyF3THxsYmw/Jx3w4FoEXr9opkAbC2tmbKzHmZruvfozOX4i7Qd9BwKlSqokgeqRFvdv7cGQq4e2Z4DT19+pSgEQMB8A9oqGimRw8fYmdvb7Jsw9pVLF+0gLIVKtKw6SdZun9pxlUWHh6Ov78/Z8+eJSkpienTp5usz5s3L6tWrdLc1eHvq9OnTxsPsgAPHjxg5MiRGbbz8PB4bw+0QvwTyxbOB9QbogLQpEVrFs6bzf49O/EtV5SAhk2xd3Dk1PGj7I7djpW1NUHfByuaSWs1IipyOWEhwVSp5kd+dw9sbHJx6eJ5tkf/QmpqKn0HDaeKr58iWbRIizViScQ8Du3bDcDZMyeBF3P579sVA0DFqr582lm5v7t1q1cQFhJMpaq+FHT3xNbOnls3E9gevYl7dxOpXK0G3fsMUCwPQKPalXErUJAiH36EpaUVvx05yN5dMXgU8iJs4cos//uSZlxl+fLl4+DBg0ydOpVVq1Zx8eJFnj9/jqenJ02aNGHw4MHkzp21Fw4IIYSajh0+yLkzpyjnU4mPSpRSLUeOHDlYFhVNWMiPrF+zgjUrl5KakoJr7jy0DOxAv0EjMsxDnNW0ViOq+9Xmwu9nOX38GAf27eLZ06c4ObtQJ6Ahnbr3opZ/gGJZxNs5tG+3cfy8cdn+PSYX2irZjNdr0JjbN29w+MBejhzcx5PHj7Gzd6B4ydI0axlI4GddFZ1CFKBpy7ZsWreao4f2k5aaSkGPQvT/eiQ9vxqS4Yx5VpBmXANsbW0ZOXJkpv89C20JCgoiKChI7RhC/L9SzqcSNx4Z3ryhAiwtLek7aBh9B2lnZg4t1YiqvjWp6ltT7RhvZWpoBFNDIxTdpxZrhBqPw+uUKe9DmfLqX+PwssEjghg8Iki1/ctsKkIIIYQQQqhEmnEhhBBCCCFUIs24EEIIIYQQKpFmXAghhBBCCJVIMy6EEEIIIYRKpBkXQgghhBBCJdKMCyGEEEIIoRJpxoUQQgghhFCJNONCCCGEEEKoRJpxIYQQQgghVCLNuBBCCCGEECrRGQwGw1ttqNPh6OjI2PETsjrTW0lNfoa1tbXaMUw8efoMKw1lmhkyg169+6odw0ieszfT2nOWnPQMSyvtPD5aew0NGzaMe/fuqR1DE6RGvJ4ca95MnrPX0+JzJjXi1d6lPrxTM57PLT9Hfo//n8L9W9YtCaVHjx5qxzAxa04oLTpoJ1NAtTJE7z2udgwjec7eTGvP2epFoXzSUTuPj9ZeQwUKFCA+XhvHRLVJjXg9Oda8mTxnr6fF50xqxKu9S32QYSpCCCGEEEKoRJpxIYQQQgghVCLNuBBCCCGEECqRZlwIIYQQQgiVSDMuhBBCCCGESqQZF0IIIYQQQiXSjAshhBBCCKESacaFEEIIIYRQyXvXjHt6eqLT6V77tXv3brVjipcYDAaWLFlC3bp1cXZ2xsLCAjc3N1q3bs2+ffvUjieygZSUFGbPnk2dOnXInTs3OXPmxMrKCg8PDz755BPWr1+vdsT3UlJSEkHDBtKivh/lirhRyMWKMt55aVq3OssXhZOamqp4Jq3ViMjli/m6Xw8a+Png6WyJm52OnxdHKLb/19m0bg1tm9ajhLszhVysqFyyED27tCMh/rpiGbRYH35eHIGbne61X20a+yuW5+aNBObOnEpgswB8PnLHw8mCMt556fZpS44eOqBYjjcJCf7B+PgcObhf0X2rXSPMs/TehfgfpaWl0aZNG9asWWOy/ObNm6xatYrIyEimTZtG377a+ohgoR1paWnUr1+fmJiYDMuvXbvGtWvXWLNmDSNGjGDcuHHqhHxPPXn8mIXzZ1O2QiX86zfC2cWVB/fvsf3XTQzs1ZWoyOUsWb0JM7P37ryR0Q/fjST+2lWcnF3InTcf8deuqh0Jg8HA0K++ZHF4GJ5e3jRtFYitrR23b91g3+5Y4q9dJX+BglmeQ6v1oUTpsgwcPirTdRvXruL3s6ep6V9fsTwL5sxg5o8/4OnlTc06ATi7uHI57gKbN6xl84a1zFywlGYt2yqWJzPnzpxiyvhR2OTKxdMnTxTdtxZqxHvdjE+aNCnT5YUKFVI4iXiV4OBgkwNt48aNqVixIjt37mTbtm0YDAb69+9PpUqVqFy5sopJhVatWbPG5CBbvnx5mjdvzv3795k/fz4PHjwAYOLEiXz99dc4ODiolPT984GTE+cSHmBhYWGyPC0tjcCm9YjdFs326E3UbdBIlXxaqBGTQ+bh5V2EAu4ezJgyge+Dhiu271eZP3s6i8PD6NS9F2MnTSdHjhwm69PS0hTJodX6ULJ0WUqWLptheUpKCuGhIZibm9Pm006K5SlXoRKRm2Ko6lvTZPmBPbto08Sf4QN60qBxcywtLRXL9LLU1FS+6tGJEqXKUqhwESKXL1Z0/1qoEe91Mz548GC1I4g3CA8PN37v6+trfKtIr9dTqlQpzpw5g16vZ/z48URFRakVU2hYXFycye3o6GicnZ0BcHNzMx4H0tLSuH//vjTjCjIzM8vQiAOYm5vToEkL9u6K4cqli8oH+y8t1Ai/2nXVjmDi2bNnBE8YjUchL76bOC1DIw4vnj8lZLf6sHnDWu7dTaRB4+a45s6j2H4bNvsk0+WVq9egml9tYrdFc+70ScqU91Es08umTRrH+bOn2bL7KLN+nKj4/rVQI97f9/4Ab29vLCwssLe3p1KlSkyYMIGnT5+qHUu85NKlS8bvy5QpY/zezMyMkiVLGm9HR0crdjZGZC/Fixc3ub1ixQqePXvGzZs32bp1q3H5Rx99hLu7u9LxRCb0ej0xWzcD8GHxkm/YOutIjcgodls09+/do0Gj5jx//pxfolYzY8oEFs6fw+U4Zf9xym71YelP8wBo36mbykn+ktM8JwA5FPoH6u9O/HaU6ZPGMXDYKIoWK/7mH8gCWqgR7/WZ8fQ/5NTUVA4dOsShQ4dYtGgRMTExuLq6qpxOADg4OPDnn38CcPLkSeNyg8HA6dOnjbeTkpK4ePEixYoVUzyj0LYmTZrQvHlz1q5dC0CvXr3o1auXyTZ16tRh7ty56HQ6FRKKlJQUpk8ej8Fg4N7dRHbHbOPi+XO07dCFGrWUu9Dt76RGZHTityMAmOXIgX+V0ly6eN64zszMjO69BzBq/GRFsmSn+hB/7Sq7Y7aRL38BatdroFqOl8Vfv8aumK3kyZuPj0qUUnz/ycnJfPXFZ5QoXZZeA75WfP/ptFAj3ssz44ULF6Zr166MHj2ar776Cjc3N+O6M2fOZHgShHqaNGli/H7nzp00a9aM7777jvr165scbAHu3bundDyRDeh0OlavXs1//vOfTA+kHh4edOjQAS8vLxXSCYDUlBSCvx/NjxPGEBE2k7gLv/Nlv8FMmhGmSh6pEa+W+OcfAISFBGPv4MAvMQe5cPMRazbvxKtwUUJnTOGnebMVyZKd6sPyxeHo9Xrafto506E9SktNTaVf944kJyfzzZgfVMk0aey3XI67wI+zw1V9TLRQI967M+ObN2/O8N/xd999R8WKFfn999+BF4P5Hzx4IGNHNWD8+PHExMQYz1CtW7eOdevWZbptZmNPhUhNTeWzzz5j+fLlwIu3JFu1asXdu3dZsGABV69epWvXrhw7dozp06ernPb9lMvWlhuPDOj1em7dvMGvm9YzYfQIjhzcx+LIX7Czt1csi9SI19Pr9QDktLBgwbK15M334h+VytVrELZoJXWrliF0xhQ6deuZ5VmyS33Q6/X8vDgcnU5HYMeuquV4OU//Lzuzf89OPu3cnVbtOiqe4fCBfcyZPplBI4IopuJQNNBGjXjvzoxn9jaVnZ0dXbp0Md5+/vw558+fz7CdUF6ePHk4fPgwQ4YMoUiRIlhYWJAnTx4aNWrE0KFDTbZ9+eyVEOlCQ0ONB1lHR0f27t3L6NGjmTFjBrNn/3UGLyQkRP7uVWZmZoZb/gJ06taTidPDOLR/D9MmKTvdpNSI17Ozf/EPSJlyPsZGPF2x4iXxKOTFlUtxPLh/P8uzZJf6sHPHVhKuX6N6zTq4e6o7W5ter2dgz66sWbGUloEd+GHaHMUzpKWl0b9HJz4qWZo+A4cpvv+/00KNeO+a8bclY0e144MPPmDixImcP3+e5ORkbt26xYYNG3j8+LFxGw8PD/Lly6diSqFV27ZtM35ftGhRk7OZPj5/zR5gMBg4ceKEotnEq9WsEwDAvt0x6gZ5hfe1RngX+RAAewfHTNenL09KeqZInuxQH5Zp5MJNvV7PgC+7sGLpTzRv3Y6pcyJUmcP/yePHXIq7wOkTv+HhZGHyYUgrlv4EQBP/qrjZ6di0fm2W59FCjXivhqmsXr2aZ8+e0bZtW5Oplx49emQyRZKFhQUffvihGhHF3+j1eh4+fIijo6PJ8l27dhEW9td40pfPWgnxsufPnxu/P3/+vMnwgsOHD5tsa21trWg28Wq3b94AwPy/sz0oQWrEm1X3qw3AxfNnM6xLTU3lyqWL2OTKhbNL1l/gmh3qw93ERLZsjOKDD5z4uEkL1XKkN+Irly2kacu2zJi7SLVx2haWlrT77PNM1x3Ys5NLcRcIaNgUZxdXCnp4ZnkeLdSI96oZv3btGgMGDGDIkCF8/PHHeHl5cefOHVauXElCQoJxuw4dOmBnZ6diUpHu6dOn5M2bl4CAAIoXL46lpSUnT54kKirKOHbRy8uLAQMGqJxUaFWtWrWM8w/fv3+fatWq0apVK+7du8eCBQuM2+XKlYvq1aurFfO9dP7cGQq4e2JjY2Oy/OnTpwSNGAiAf0BDxfJIjXgzTy9vavoHELstmiUR8/i0819ne0OCJ/Dg/n1aBnZQZK7x7FAfIpcvIiUlhY6fd1DtQ3XSh6asXLaQJi1aEzJvsaoXTFpbWzNl5rxM1/Xv0ZlLcRfoO2g4FSpVUSSPFmrEe9WMp7t586bJA/wyPz8/pk2bpnAi8TrJycmsX7/e+MfyMm9vb3755RfsFbzAS2QvPXv2ZOXKlezfvx94MRvGmDFjTLYxMzNjxowZGc6wiay1bvUKwkKCqVTVl4Luntja2XPrZgLbozdx724ilavVoHsf5RspLdWIJRHzOLRvNwBnz7yYvm/pT/PYtysGgIpVfU0aYiV8HzyLpnWrMaRvd7ZsWIt30WKcPnGM3bHbKeDuwX/GZv7JpVlB6/Vh2cL5gLpDVIInjGHF0p/IZWuLV+GiTJ04NsM2DRo3z/RTQ98HWqgR71Uz3rlzZ1xcXNiwYQMnTpzg9u3bPHz4kA8++ICyZcvSvn17OnbsqIlph8QLVlZWDB06lNjYWC5fvszdu3extbWlePHifPLJJ/Ts2VOGFojXsra2JjY2lrlz57Jq1SpOnTrF/fv3MTc3x83NjerVq9O3b18qVqyodtT3Tr0Gjbl98waHD+zlyMF9PHn8GDt7B4qXLE2zloEEftZVsU9zBG3WiEP7dhvH0RqX7d/Dof17jLeVbsY9vbzZtPMwk8Z+S8zWzcRuj8Y1T146f9GbgcO+xcU1tyI5tF4fjh0+yLkzpyjnU0mVebzTxV+9ArwYq/2qC6ILunu+t824FmrEe9WMOzo60qFDBzp06KB2FPGWzM3NmTBhgtoxRDZnYWFB79696d27t9pRxEvKlPdR7SO4M6PFGjE1NIKpoRFqx8ggf4GCTJ0T/uYNs5DW60M5n0rceGRQO4ZmX0OZUSur2jVCZlMRQgghhBBCJdKMCyGEEEIIoRJpxoUQQgghhFCJNONCCCGEEEKoRJpxIYQQQgghVCLNuBBCCCGEECqRZlwIIYQQQgiVSDMuhBBCCCGESqQZF0IIIYQQQiXSjAshhBBCCKESacaFEEIIIYRQiTTjQgghhBBCqERnMBgMb7WhToejoyMTJkzI6kxv5cnTZ1hZW6sdw0RK0jOsNZRp+owZ9OrdV+0YRslJ2nvOkp49w9JKO5lmzdTWc5aarK3XtNb+7r8ZPox79+6pHUMTpEa8ntSHN9NajZD68GZSI17tXerDOzXj+fPnJz4+/n8K92+ZNSeUFh16qB3DxLolofTooZ1MpUuXYcve42rHMFq9KJRPOmrn8QHtZQqoVoZoDT1nWntNa+3vvsKHBbiRoI1jotqkRrye1v6WtFYfQHvHY63l0Vp9AO29rrX0d/8u9UGGqQghhBBCCKESacaFEEIIIYRQiTTjQgghhBBCqESacSGEEEIIIVQizbgQQgghhBAqkWZcCCGEEEIIlUgzLoQQQgghhEoUb8abNWuGTqczftWqVUvpCJqRkJDA7NmzCQwMpFSpUri6upIzZ05cXV2pW7cuCxcu5C2ngRdCM2rVqmXyN/66LyH+Tu0aodfrWRAaQoBvebxy21DUzZ4W9f3YsnGdojlAuzUicvlivu7XgwZ+Png6W+Jmp+PnxRGK5wBISkoiaNhAWtT3o1wRNwq5WFHGOy9N61Zn+aJwUlNTFc9kMBj4JWo1rRrWpmzhfHjltsG33Id83a8HVy9fUjzPyzatW0PbpvUo4e5MIRcrKpcsRM8u7UiIv67I/rVaHyqV8MTNTpfpV8uPa2X5/s2zfA8vWbRoEevWKX9A06pFixYxfPjwDMvv3LnDtm3b2LZtG6tWrWLNmjXkyJFDhYRCCKEctWuEwWCgx2dt2BgViaeXN+0++5yU5GS2bIyiS2Azxk6eQdcefRTLo9Ua8cN3I4m/dhUnZxdy581H/LWriu377548fszC+bMpW6ES/vUb4eziyoP799j+6yYG9upKVORylqzehJmZcucex4wYTGhIMHny5qNB4+bY2tlz5tRxlkTMZe2qZazbupdixUsqlgdevLaHfvUli8PD8PTypmmrQGxt7bh96wb7dscSf+0q+QsUVDST1tg7ONCtV/8Mywu6e2b5vhVrxm/cuMFXX32l1O6ylbx589KwYUO8vLy4cuUKixcvJikpCYD169cTHh5Ot27dVE4pxNvp2bMnjRs3zrA8LS2N//znP6SlpQHw8ccfKx1NaJgWasTGqEg2RkVSsUp1lq/71fgx38NGjefjmj58981g6jVoTEEPT0Vzaa1GTA6Zh5d3EQq4ezBjygS+D8r4D4NSPnBy4lzCAywsLEyWp6WlEdi0HrHbotkevYm6DRopkueP27eYO2sqBdw92Lr3OPYODsZ1YSE/EjR8IKEzgvlx9gJF8qSbP3s6i8PD6NS9F2MnTc/wz1v6cTmrabk+2Ds4MnhEkOL7BQWb8R49enDv3j3c3d1xdnbm2LFjSu1as9zd3Vm0aBGBgYGYm//1VLRv3546deoYb2/atEmacZFttG3bNtPlS5cuNTngf/3110pFEtmAFmrElo1RAPQbPMLYiAM4u7jwRe8BfDu0P8sXhzPkm9GK5NFqjfCrXVexfb2JmZlZhkYcwNzcnAZNWrB3VwxXLl1ULM/1q1fQ6/VUrFLdpBEHqNugMUHDB5J450/F8gA8e/aM4Amj8SjkxXcTp2X6LsrLr6+sJPUhc4o8+hEREWzYsAGdTseCBQv47rvvlNit5rVv3z7T5bVr18bZ2ZnExEQAUlJSlIwlRJaYPHmy8XsfH5/3+noRYUorNeKP27cAcPcolGFdwf8u2xO7XbFmXGrEP6fX64nZuhmADxUcElLIuwgWFhYc2r+HRw8fYmdvb1y3dfMGAGrU8lcsD0Dstmju37tH20+78Pz5c6I3riPu4nkcHB2pUasuhbwLK5onM1qoDynJyfy8OILbN29ga29P2fIVKV+xsiL7zvJmPCEhgQEDBgDw5Zdf4u/vL834G9y6dYsHDx4Yb1eqVEnFNEL877Zu3WpypvN9O+shXk1LNcLJ2QWAa1cvU6TYRybrrl+9DMCli+cVz/V3UiMySklJYfrk8RgMBu7dTWR3zDYunj9H2w5dFG1+nZydGTF6AqNHDMKvQjHqN2pmHDO+J3Y7nbr3oouC1x0AnPjtCABmOXLgX6W0yWvYzMyM7r0HMGr85Ff9eJbTSn344/YtBvTsYrKsbIWKzFqwDE8v7yzdd5Y34927d+f+/fsUKlSIiRMnZvXusr20tDS++OIL49s1uXPn5ssvv1Q5lRD/m0mTJhm/9/Ly4pNPPlExjdASLdWIOvU+JmrVckKCJ1C9Zh2srKwAuJuYyNxZUwF4+OC+egGRGvEqqSkpBH//1zsWOp2OL/sNZsTo7xXP8kWfAeR1y8/gPt1YOH+OcXmlqr60aN1esSEh6RL//AOAsJBgSpUtzy8xByny4UecOn6MIf2+IHTGFDy9vOnUraeiudJpoT607dCFytVqUKx4SWxy2XLp4nnCQoJZtWwRbZr4s33/SWzt7LJs/1l6efGCBQvYtGkTOp2O8PBwbG1ts3J32d6jR49o2rQp69evB8DOzo5169bh6uqqcjIh/rkTJ04QHR1tvD1w4ECZHUgA2qsRLdq0p7pfbQ7s3YV/lVJ8M7gvQ7/6ktqVSmBn92K4gZKzcvyd1IhXy2Vry41HBuIfPOfwueuMD57JsoXzaPlxLR49fKholuAJY+jbrQN9B43g8LnrXLj5iDVbdpGcnESrhrUUnyZTr9cDkNPCggXL1lK2QkVy2dpSuXoNwhatxMzMjNAZUxTNlE4r9WHQ8FH41qyDi2tubGxsKFm6LNPDFtKqXUfir11lScTcLN1/lh1VkpKSGDhwIAB9+vShZs2aWbWr/xeuX7+Or68vmzZtAsDV1ZVt27ZRubIy45WEyCovjwV0cXGha9euKqYRWqHFGmFubs7i1ZsYNCIInc6MJeFh/LJuNfUbNSNs0SoAnF1zq5JNasTbMTMzwy1/ATp168nE6WEc2r+HaZPGKbb/nTu2MnncKLp80Ye+g4bhlr/Ai8a3mi8/rViPec6cjPlmkGJ5AOzsX1xIWqacD3nzuZmsK1a8JB6FvLhyKY4H9+8rmgu0Xx86dOkBwKH9e7J0P1najKePaZsxY4bJRO6xsbHG7WJjY9/7D/85fPgwlStX5sSJEwAULVqUffv2UbFiRZWTCfG/iY+PZ/ny5cbbvXv3NpmlQry/tFojLC0tGTR8FLuP/c6VxGROXv6DidNDuXUzAXjR0ChNasQ/U7NOAAD7dscots8d0S/+WarmVzvDutx58lK4aDEux13kyePHimXyLvIh8GLqvsykL09KeqZQoheyQ31Iv47k6dMnWbof9d5vEwCsWbOGmjVrcvPmTQBq1KjBvn378PbO2osFhFDC1KlTjZ+AZ21tTZ8+yl64JMS/ZfXPSwBo1ipQ0f1Kjfjnbt+8AYC5eU7F9pmS+mJmm1dNX5h450/MzMwwz6lcpur//cfg4vmzGdalpqZy5dJFbHLlwtlF2eFO2aE+HDt8AMj6D/7JsqsILCwsaNmyZabrYmNjuXPnDvDibYmaNWtSokSJrIqiWStXriQwMNA4nsvBwYH69euzYIHphwE4ODjQvXt3NSIK8Y89fPiQuXP/GmfXpUsXXFxcVEwktESrNeLv09EBbFi7iuWLFlC2QkUaNlXu4jKpEW92/twZCrh7YmNjY7L86dOnBI14MQzKP6ChYnkqVqlOeGgIYSHBNGrW0mSu8YXz53AzIZ6KVapjaWmpWCZPL29q+gcQuy2aJRHz+LTzX3PShwRP4MH9+7QM7KDohaVaqg8Xfj9H/oLuGV5DF34/x7hvhwLQonXm04z+W7LskbexsWHVqlWZrqtVq5bxbcgSJUq8crv/706fPm08yAI8ePCAkSNHZtjOw8PjvT3QiuwrNDSUh/+9cCpHjhwMGqTsOEmhbVqtEY1qV8atQEGKfPgRlpZW/HbkIHt3xeBRyIuwhSsVvbhMqzViScQ8Du3bDcDZMycBWPrTPPbtigGgYlVfk4YvK61bvYKwkGAqVfWloLsntnb23LqZwPboTdy7m0jlajXo3meAIlkAmrRozcJ5s9m/Zye+5YoS0LAp9g6OnDp+lN2x27Gytibo+2DF8qT7PngWTetWY0jf7mzZsBbvosU4feIYu2O3U8Ddg/+MnfTmO/kXaak+REUuJywkmCrV/Mjv7oGNTS4uXTzP9uhfSE1Npe+g4VTx9cvSDMrOryOEeC+kpqYybdo04+1PPvkELy8vFRMJ8XaatmzLpnWrOXpoP2mpqRT0KET/r0fS86shGc6Yv68O7dvNiqU/mS7bv8fkIjelmvF6DRpz++YNDh/Yy5GD+3jy+DF29g4UL1maZi0DCfysq6JnfHPkyMGyqGjCQn5k/ZoVrFm5lNSUFFxz56FlYAf6DRqRYQ57JXh6ebNp52Emjf2WmK2bid0ejWuevHT+ojcDh32Li4IXJmutPlT3q82F389y+vgxDuzbxbOnT3FydqFOQEM6de9FLf+ALM+gSjMeExOjxm41JygoiKCgILVjCPGvy5kzJ/Hx8WrHENmUmjVi8IggBo8IUm3/L9NqjZgaGsHU0Ai1YwBQprwPZcorf1Ht61haWtJ30DD6DhqmdhQT+QsUZOqccLVjaK4+VPWtSVVfdWdzkgs4hRBCCCGEUIk040IIIYQQQqhEmnEhhBBCCCFUIs24EEIIIYQQKpFmXAghhBBCCJVIMy6EEEIIIYRKpBkXQgghhBBCJdKMCyGEEEIIoRJpxoUQQgghhFCJNONCCCGEEEKoRJpxIYQQQgghVCLNuBBCCCGEECrRGQwGw1ttqNNhbm7ORx8Vz+pMb+W5/jlmZjnUjmHCoH9OjhzayXTp8mU8PQupHcNIa48PQNpzbb2Orl65TKFC2nnO0p4/J4eGHh+t/d2fO3uW1NQUtWNogtSI19Pa8U9r9QG09xhJfXgzqRGv9i71wfxd7tg1dx627D3+j0L921YvCuWTjj3UjmFi3ZJQevTQTqbSpcto5vkC7T0+ALPmhNKig3YyNahehuPHtfOcae3x0drffYUPC6gdQVOkRrya1o5/WqsPoL3HSGvHP63VB9DeY6Slv/t3qQ8yTEUIIYQQQgiVSDMuhBBCCCGESqQZF0IIIYQQQiXSjAshhBBCCKESacaFEEIIIYRQiTTjQgghhBBCqESacSGEEEIIIVQizbgQQgghhBAqkWZcRUlJSYwYMYKAgAA8PT2xs7MjZ86cuLi4UK1aNcaNG8eDBw/Ujqm6lStX8uWXX+Lj44OlpSU6nc74JSAlJYXZs2dTp04dcufOTc6cObGyssLDw4NPPvmE9evXqx1RiFfS6/UsCA0hwLc8XrltKOpmT4v6fmzZuE7xLD8vjsDNTvfarzaN/RXLk11qREjwD8bH58jB/YruW8v1YdO6NbRtWo8S7s4UcrGicslC9OzSjoT464rm0GKNqFTC85V/Yy0/rqVolps3Epg7cyqBzQLw+cgdDycLynjnpdunLTl66IAiGd7pEzjFv+vx48d8//33GZYnJiayb98+9u3bR3h4OAcPHsTJyUmFhNowbtw4zX3qmFakpaVRv359YmJiMiy/du0a165dY82aNYwYMYJx48apE1KIVzAYDPT4rA0boyLx9PKm3Wefk5KczJaNUXQJbMbYyTPo2qOPYnlKlC7LwOGjMl23ce0qfj97mpr+9RXLkx1qxLkzp5gyfhQ2uXLx9MkTxfevxfpgMBgY+tWXLA4Pw9PLm6atArG1teP2rRvs2x1L/LWr5C9QUJEsWq4R9g4OdOvVP8Pygu6eiuZYMGcGM3/8AU8vb2rWCcDZxZXLcRfYvGEtmzesZeaCpTRr2TZLM0gzrrL8+fNTrVo1PDw8cHJy4s6dO0RGRnL16lUA4uLiCAsLY9iwYSonVY9Op8Pb2xsfHx9u3bpFbGys2pE0Y82aNSYH2fLly9O8eXPu37/P/PnzjWfNJk6cyNdff42Dg4NKSYXIaGNUJBujIqlYpTrL1/2KtbU1AMNGjefjmj58981g6jVoTEEPT0XylCxdlpKly2ZYnpKSQnhoCObm5rT5tJMiWdJpuUakpqbyVY9OlChVlkKFixC5fLHiGbRYH+bPns7i8DA6de/F2EnTyZEjh8n6tLQ0xbJouUbYOzgyeESQYvt7lXIVKhG5KYaqvjVNlh/Ys4s2TfwZPqAnDRo3x9LSMssySDOuIhcXF+Lj4zMsHzhwIAUKFDDeTj/ovq/27t1rLNJBQUGaONhqRVxcnMnt6OhonJ2dAXBzc2Pw4MHAi4P//fv3pRkXmrJlYxQA/QaPMP6NAzi7uPBF7wF8O7Q/yxeHM+Sb0WpFBGDzhrXcu5tIg8bNcc2dR7H9ar1GTJs0jvNnT7Nl91Fm/ThRlQxaqw/Pnj0jeMJoPAp58d3EaRkacQBzc+VaL6kRb9aw2SeZLq9cvQbV/GoTuy2ac6dPUqa8T5ZlkDHjGvL8+XMSEhKYO3euyfISJUqolEgbXi7SwlTx4sVNbq9YsYJnz55x8+ZNtm7dalz+0Ucf4e7urnQ8IV7rj9u3AHD3KJRhXcH/LtsTu13RTJlZ+tM8ANp36qZqDi3ViBO/HWX6pHEMHDaKosWKv/kHsojW6kPstmju37tHg0bNef78Ob9ErWbGlAksnD+Hy3EXFc+j5RqRkpzMz4sjmD5pPAtCQxQbn/0ucprnBCBHFv8DJWfGNWDr1q3Uq1cv03V+fn5066ZuARDa1aRJE5o3b87atWsB6NWrF7169TLZpk6dOsydO1cTFzQJ8TInZxcArl29TJFiH5msu371MgCXLp5XPNfL4q9dZXfMNvLlL0Dteg1UyaC1GpGcnMxXX3xGidJl6TXga0X3rXUnfjsCgFmOHPhXKW3y+jUzM6N77wGMGj9ZsTxarhF/3L7FgJ5dTJaVrVCRWQuW4enlrWiWzMRfv8aumK3kyZuPj0qUytJ9yZlxDWvfvj0bN27EyspK7ShCo3Q6HatXr+Y///lPpgdSDw8POnTogJeXlwrphHi9OvU+BiAkeAJJSUnG5XcTE5k7ayoADx/cVyHZX5YvDkev19P2086ZDjlQk1o1YtLYb7kcd4EfZ4dr7jFRW+KffwAQFhKMvYMDv8Qc5MLNR6zZvBOvwkUJnTGFn+bNViyPVmtE2w5dWLFhGycu3ebi7SdE7zlGq3Yd+e3IIdo08efxo0eK5vm71NRU+nXvSHJyMt+M+SHLX+dyZlwDihYtyqRJk0hOTubq1ausXr2axMREli5dytGjR9m8eTMeHh5qxxQalJqaymeffcby5cuBF29JtmrVirt377JgwQKuXr1K165dOXbsGNOnT1c5rRCmWrRpz4olEezZuQP/KqWoVbcBaampbN6w1jg228xMvXNGer2enxeHo9PpCOzYVbUcWqoRhw/sY870yQwaEUSx4iUV2Wd2otfrAchpYcGCZWvJm88NeDH+OGzRSupWLUPojCl06tZTkTxarRGD/jZrUcnSZZkethCAVcsWsSRiLj36DlQsz8v0ej39v+zM/j07+bRzd1q165jl+5Qz4xrg7u7O4MGD+eabbwgLC+PMmTPky5cPgHPnztG/f391AwrNCg0NNR5kHR0d2bt3L6NHj2bGjBnMnv3X2ZeQkBDOn1f37X4h/s7c3JzFqzcxaEQQOp0ZS8LD+GXdauo3akbYolUAOLvmVi3fzh1bSbh+jeo16+DumXFcu1K0UiPS0tLo36MTH5UsTZ+B7+8MX69jZ//iAsgy5XyMjXi6YsVL4lHIiyuX4nhw/74iebJbjejQpQcAh/bvUWX/er2egT27smbFUloGduCHaXMU2a804xqUO3duqlSpYrz99/lBhUi3bds24/dFixY1uRLex+evK78NBgMnTpxQNJsQb8PS0pJBw0ex+9jvXElM5uTlP5g4PZRbNxOAF02NWpZp5MLNv1OrRjx5/JhLcRc4feI3PJwsTD6oZcXSnwBo4l8VNzsdm9avVSST1ngX+RB4MW1fZtKXJyU9UyRPdqsR6deRPH2q/Jz1er2eAV92YcXSn2jeuh1T50Qo9s6cDFNR0Y4dO/Dx8cHOzs5k+Z07dzhw4K+riuXCO/Eqz58/N35//vx5Hjx4YDzYHj582GRbrc06IMTrrP55CQDNWgWqsv+7iYls2RjFBx848XGTFqpk0FqNsLC0pN1nn2e67sCenVyKu0BAw6Y4u7gqNje81lT3qw3AxfNnM6xLTU3lyqWL2OTKhbOLqyJ5sluNOHb4xeta6Q/+SW/EVy5bSNOWbZkxd5Gi10NIM66iadOm8euvv+Lv70/p0qWxsbEhISGByMhIbt++bdyucePGKqZU3+zZs41zpe7du9dkXfocqQA9e/bE21v9K7CVVKtWLeNHGd+/f59q1arRqlUr7t27x4IFC4zb5cqVi+rVq6sVU4hXevTwIXb29ibLNqxdxfJFCyhboSINm2Y+B3BWi1y+iJSUFDp+3iFLP+zjdbRWI6ytrZkyc16m6/r36MyluAv0HTScCpWqZLpNVtBaffD08qamfwCx26JZEjGPTzv/9a5KSPAEHty/T8vADorNNa7FGnHh93PkL+iOjY1NhuXjvh0KQIvW7RXJAn8NTVm5bCFNWrQmZN5ixS9MlmZcZU+fPmX9+vXGP5a/K1u2LFOmTFE4lbb8/PPPr/wgh5cfm8aNG793zXjPnj1ZuXIl+/fvB+DMmTOMGTPGZBszMzNmzJiBo6OjCgmFeL1GtSvjVqAgRT78CEtLK347cpC9u2LwKORF2MKVqs3WsWzhfED9ISpSI15Pi/Xh++BZNK1bjSF9u7Nlw1q8ixbj9Ilj7I7dTgF3D/4zdpIiOUCbNSIqcjlhIcFUqeZHfncPbGxyceniebZH/0Jqaip9Bw2niq+fIlkAgieMYcXSn8hla4tX4aJMnTg2wzYNGjfP9NN5/y3SjKuod+/e5M2blwMHDnDjxg3u3r2Lubk5efLkoXTp0rRo0YIOHTqQM2dOtaMKjbK2tiY2Npa5c+eyatUqTp06xf379zE3N8fNzY3q1avTt29fKlasqHZUITLVtGVbNq1bzdFD+0lLTaWgRyH6fz2Snl8NyXDGXCnHDh/k3JlTlPOplOXzC7+O1IjsydPLm007DzNp7LfEbN1M7PZoXPPkpfMXvRk47FtcFLwoWYs1orpfbS78fpbTx49xYN8unj19ipOzC3UCGtKpey9q+QcolgUg/uoV4MU1EdMmjct0m4LuntKM/39Vr169V36Qg/iLXMD6ehYWFvTu3ZvevXurHUWIdzZ4RBCDRwSpHcNEOZ9K3HhkUDtGtqoRU0MjmBoaofh+tVof8hcoyNQ54WrHALRXI6r61qSqb021Yxip9dp9mcymIoQQQgghhEqkGRdCCCGEEEIl0owLIYQQQgihEmnGhRBCCCGEUIk040IIIYQQQqhEmnEhhBBCCCFUIs24EEIIIYQQKpFmXAghhBBCCJVIMy6EEEIIIYRKpBkXQgghhBBCJdKMCyGEEEIIoRKdwWAwvNWGOh2Ojo6MHT8hqzO9ldTkZ1hbW6sdw8STp8+w0lCmmSEz6NW7r9oxjJKTtPX4AKQkaet1NH3GDHr30c5zprXH58nTZ1haaSfPyBHDuHfvntoxNEFqxOtJfXgzrdUIrR3/tFYfQHuPkZZqxLvUh3dqxvO55efI7/H/U7h/y7olofTo0UPtGCZmzQmlRQftZAqoVobovcfVjmG0elEon3TUzuMD2nsdlS5dhi0aes609vho7W+swocFuJGgjWOi2qRGvJ7WXrtaqw+gvRqhtdeQ1uoDaO8x0tLf2bvUBxmmIoQQQgghhEqkGRdCCCGEEEIl0owLIYQQQgihEmnGhRBCCCGEUIk040IIIYQQQqhEmnEhhBBCCCFUIs24EEIIIYQQKpFmXAghhBBCCJW8d834ypUr+fLLL/Hx8cHS0hKdTmf8EuJtGAwGlixZQt26dXF2dsbCwgI3Nzdat27Nvn371I6nKceOHaNr1654e3tjbW2Nvb09hQsXJjAwkOjoaLXjCZXdvJHA3JlTCWwWgM9H7ng4WVDGOy/dPm3J0UMHFM+j1fpQqYQnbna6TL9aflxL0SxJSUkEDRtIi/p+lCviRiEXK8p456Vp3eosXxROamqqonngxTH5l6jVtGpYm7KF8+GV2wbfch/ydb8eXL18SfEsWqsPPy+OeOXrJ/2rTWN/xXNpvT6EBP9gfHyOHNyfpfsyz9J716Bx48Zx/Li2PsFKZB9paWm0adOGNWvWmCy/efMmq1atIjIykmnTptG3r7Y+slgNo0ePZvTo0bz8Ib9JSUk8evSIuLg4bG1tCQgIUDGhUNuCOTOY+eMPeHp5U7NOAM4urlyOu8DmDWvZvGEtMxcspVnLtorl0XJ9sHdwoFuv/hmWF3T3VDTHk8ePWTh/NmUrVMK/fiOcXVx5cP8e23/dxMBeXYmKXM6S1ZswM1PuXN+YEYMJDQkmT958NGjcHFs7e86cOs6SiLmsXbWMdVv3Uqx4ySzPodX6UKJ0WQYOH5Xpuo1rV/H72dPU9K+vaCat14dzZ04xZfwobHLl4umTJ1m+v/euGdfpdHh7e+Pj48OtW7eIjY1VO5LIRoKDg00OtI0bN6ZixYrs3LmTbdu2YTAY6N+/P5UqVaJy5coqJlXX7NmzCQoKMt6uWrUq1apVw8nJibt373L27FlcXFzUCyg0oVyFSkRuiqGqb02T5Qf27KJNE3+GD+hJg8bNsbS0VCSPluuDvYMjg0cEqR2DD5ycOJfwAAsLC5PlaWlpBDatR+y2aLZHb6Jug0aK5Pnj9i3mzppKAXcPtu49jr2Dg3FdWMiPBA0fSOiMYH6cvSDLs2i1PpQsXZaSpctmWJ6SkkJ4aAjm5ua0+bSTYnm0Xh9SU1P5qkcnSpQqS6HCRYhcvjjL9/neNeN79+7F2toagKCgIE0dbIX2hYeHG7/39fVl/fr1AOj1ekqVKsWZM2fQ6/WMHz+eqKgotWKq6uHDhwwbNsx4e86cOfTo0UPFREKrGjb7JNPllavXoJpfbWK3RXPu9EnKlPdRJI/UhzczMzPL0IgDmJub06BJC/buiuHKpYuK5bl+9Qp6vZ6KVaqbNOIAdRs0Jmj4QBLv/KlIluxWHzZvWMu9u4k0aNwc19x5FNlndqgP0yaN4/zZ02zZfZRZP05UZJ/v3Zjx9AOtEP/EpUt/jT8sU6aM8XszMzNKlvzrbdDo6GjS0tIUzaYVkZGRPHz4EIACBQqQkJBAqVKlsLGxwcXFhebNm3PggPLjgUX2ktM8JwA5zJU7Z6Tl+pCSnMzPiyOYPmk8C0JDVBlT/zp6vZ6YrZsB+FCBISHpCnkXwcLCgkP79/Dov8eddFs3bwCgRi1lxkNnt/qw9Kd5ALTv1E2xfWq9Ppz47SjTJ41j4LBRFC1WXLH9vndnxoX4Xzg4OPDnny/Ospw8edK43GAwcPr0aePtpKQkLl68SLFixRTPqLa9e/cav4+Pj+e7774z3n727BlRUVFs3LiRJUuW0KZNGzUiCo2Lv36NXTFbyZM3Hx+VKKV2HE344/YtBvTsYrKsbIWKzFqwDE8vb8XzpKSkMH3yeAwGA/fuJrI7ZhsXz5+jbYcuijW/AE7OzowYPYHRIwbhV6EY9Rs1M44Z3xO7nU7de9GlRx9FsmSn+hB/7Sq7Y7aRL38BatdroNh+tVwfkpOT+eqLzyhRuiy9Bnyt6L6lGRfiHTRp0oQFC16MPdy5cyfNmjXDx8eHXbt2mRxsAe7du6dGRNXdvHnT5LalpSXdu3fH2tqasLAwHjx4QFpaGt26dcPf3x9nZ2eVkgotSk1NpV/3jiQnJ/PNmB/IkSOH2pFU17ZDFypXq0Gx4iWxyWXLpYvnCQsJZtWyRbRp4s/2/SextbNTNFNqSgrB34823tbpdHzZbzAjRn+vaA6AL/oMIK9bfgb36cbC+XOMyytV9aVF6/aYK/TuSnaqD8sXh6PX62n7aWdF/8a0XB8mjf32xQXku44oftx574apCPG/GD9+PF5eXsbb69at49tvv+XXX3/NsG1m4yrfBykpKSa3J02axIwZM5g4cSKLFi0yLn/06BHr1q1TOp7QML1eT/8vO7N/z04+7dydVu06qh1JEwYNH4VvzTq4uObGxsaGkqXLMj1sIa3adST+2lWWRMxVPFMuW1tuPDIQ/+A5h89dZ3zwTJYtnEfLj2tlGC6S1YInjKFvtw70HTSCw+euc+HmI9Zs2UVychKtGtZiy0ZljjPZpT7o9Xp+XhyOTqcjsGNXRfet1fpw+MA+5kyfzFdfj1Rk5p2/k2ZciHeQJ08eDh8+zJAhQyhS5MVYxTx58tCoUSOGDh1qsq2bm5tKKdXl6OhocrtWrVqZfg8QFxeX9YFEtqDX6xnYsytrViylZWAHfpg2580/9J7r0OXFhW+H9u9RLYOZmRlu+QvQqVtPJk4P49D+PUybNE6x/e/csZXJ40bR5Ys+9B00DLf8Bchla0vlar78tGI95jlzMuabQYpkyS71YeeOrSRcv0b1mnVw9yyk6L61WB/S0tLo36MTH5UsTZ+Bw978A1lAmnEh3tEHH3zAxIkTOX/+PMnJydy6dYsNGzbw+PFj4zYeHh7ky5dPxZTqeflCpb97eU5ZACsrq6yOI7IBvV7PgC+7sGLpTzRv3Y6pcyIUnac6u3JyfjH929OnWT8P8tuoWefFvND7dscots8d0ZsAqOZXO8O63HnyUrhoMS7HXeTJS8fnrJQd6sMyFS7cTKfF+vDk8WMuxV3g9Inf8HCyMPkwpBVLfwKgiX9V3Ox0bFq/NksyyJhxId6BXq/n4cOHGf6737VrF2FhYcbbXbp04X3VqFEjRo366wMmYmNjKVXqxUV4O3fuNNnWx0eZKeuEdqU34iuXLaRpy7bMmLtIxom/pWOHX8w6ofQH/7zK7Zs3ADD/70w4SkhJfTHs4VXTFybe+RMzMzPMc2Z9puxQH+4mJrJlYxQffODEx01aKL5/LdYHC0tL2n32eabrDuzZyaW4CwQ0bIqziysFPTyzJMN714zPnj3b+NbHy1f1AgwePNj4fc+ePfH2Vv4KdaFtT58+JW/evAQEBFC8eHEsLS05efIkUVFR6PV6ALy8vBgwYIDKSdVToUIF6tevz5YtWwAYMmQIFy5cwMrKirlz/xrbWqxYMerVq6dWTKEB6UNTVi5bSJMWrQmZt1jVRlyL9eHC7+fIX9AdGxubDMvHffti6EOL1u0VyQJw/twZCrh7Zsjz9OlTgkYMBMA/oKFieSpWqU54aAhhIcE0atbSZK7xhfPncDMhnopVqivywVHZoT5ELl9ESkoKHT/voNiHab1Mi/XB2tqaKTPnZbquf4/OXIq7QN9Bw6lQqUqWZXjvmvGff/75lR/kMGXKFOP3jRs3lmZcZCo5OZn169cbP9DhZd7e3vzyyy/Y29urkEw7wsPD8ff35+zZsyQlJTF9+nST9Xnz5mXVqlVyBvQ9FzxhDCuW/kQuW1u8Chdl6sSxGbZp0Lh5pp8emBW0WB+iIpcTFhJMlWp+5Hf3wMYmF5cunmd79C+kpqbSd9Bwqvj6KZIFYN3qFYSFBFOpqi8F3T2xtbPn1s0Etkdv4t7dRCpXq0H3Pso1m01atGbhvNns37MT33JFCWjYFHsHR04dP8ru2O1YWVsT9H2wYnm0Xh+WLZwPqDNEJZ3Uh4zeu2ZciP+FlZUVQ4cOJTY2lsuXL3P37l1sbW0pXrw4n3zyCT179tT0B4coJV++fBw8eJCpU6eyatUqLl68yPPnz/H09KRJkyYMHjyY3Llzqx1TqCz+6hXgxZjNV130V9DdU7FmXIuq+9Xmwu9nOX38GAf27eLZ06c4ObtQJ6Ahnbr3opZ/gKJ56jVozO2bNzh8YC9HDu7jyePH2Nk7ULxkaZq1DCTws66KTSUIkCNHDpZFRRMW8iPr16xgzcqlpKak4Jo7Dy0DO9Bv0AiKFPtIkSxarw/HDh/k3JlTlPOppOr8/VIfMnrvmvGYmBi1I4hszNzcnAkTJqgdI1uwtbVl5MiRjBw5Uu0oQqOmhkYwNTRC7RhGWqwPVX1rUtW3ptoxjMqU96FMeW1d62FpaUnfQcPoO0idmTDSab0+lPOpxI1Hhv9r797je6z/P44/NrNhxtgoc9hsmkNyalRWiJzKYeIb1RxLjCSir/Qrh+i7iLBpRjY51BgzIofw3cQSOvmVRJSF0Te2OY0dPvv9sa9PPszp9vO5rsvN8367ud32ua6L6+lzfT7X67XP53q/rxtvaIA7pT4YdY7ScHUREREREZOoGRcRERERMYmacRERERERk6gZFxERERExiZpxERERERGTqBkXERERETGJmnEREREREZOoGRcRERERMYmacRERERERk6gZFxERERExiZpxERERERGTqBkXERERETGJS2FhYeFNbejigre3N5GRkc7OdFPOnc+hVOnSZsdwcCEnB49S1sn04ewohgwdZnYMu4sXrPX8AORdzKG0hV5Hs6Ksd8ys9D7LvWCt4zVmzBgyMzPNjmEJqhHXp/pwY1arEaoPN6YacW23Uh9uqRmvWrUqR44c+X+Fu10+nBNLt/BBZsdwkLQolqd7WydTu+YN2Zj2g9kx7Kz2/ACsXhLLoEHWydSgQUM26Jhdk9WOV7Vq1SxzTjSbasT1We29ZLX6ANZ7jqx2vrFafQAds+u5lfqgy1REREREREyiZlxERERExCRqxkVERERETKJmXERERETEJGrGRURERERMomZcRERERMQkasZFREREREzi9GY8NzeXmJgYWrduTeXKlSlZsiSlSpXC39+fp59+ms8++8zZEURuq4CAAFxcXK77Z9u2bWbHlMskJiYyePBgQkJC8PDwcDhWYh4r14d1q1fSs0tb7q/hQ03fUjxUvyYR/Z/l6JE/TMt0SfT09/DzcsHPy4Vvdu4wdN8Zx44yb/YMenVtR0jdGvhXdKdh0L28+Hx3vt31taFZAJYuXmB/Lq7155lObQzNZLUaceHCBcaPGUm39i1ofJ8fNX1L0TDoXro8EUrConjy8vIMy3KJzWYjLjaado82IbByGYL9ytGtfQs2rF1teBYr1Ac3Z/7j+fn5tG/fnpSUlKuWp6enk56ezsqVKxk7diyTJ092ZhQRuYtNnjyZH36w1s0y7nZWrQ+FhYX8c/hgFsfPJSAwiC49elG2rBcnjh/jq22pHEk/TNVq1Q3Lc6V9e39k2rvjKOPpyflz5wzff9ycKGZ/8B4BgUG0bN0OH99K/HbwAOvXJLN+TTKz4z6ha/eehuW5v0EjRr4xrth1a5OX88vPP9GyTXvD8ljRubNnWTg/hkYPNqNN+6fw8a1EdlYmW75Yx8ghA1i1IoElSetwdTXmYonCwkIG9XmGtatWEBAYxLN9XiD34kU2rF1F/15dmfR+FAMGvWxIFrBGfXBqM75y5UqHE22TJk0ICwsjKyuL+fPnk52dDcCUKVN4/fXXKV++vDPjiNx2U6dOLXZ5zZo1DU4i1+Pi4kJQUBAhISEcP36c1NRUsyPd9axaH+bHzGJx/Fz6DhzCpKmzKFGihMP6/Px8Q3IUJy8vj+GD+nL/A42oWes+ViQsNjxD4websWJdCo882tJh+dfbv+SZzm14Y0QEHTqF4eHhYUie+g0aUb9Bo6uW5+bmEh8bjZubG88839eQLMWxQo2oULEi+45m4+7u7rA8Pz+fXl3akrp5I1s2ruOJDk8ZkmftqhWsXbWCpg+HkrD6C/vt68eMe5eOLUN4581RtO3Qier+AYbksUJ9cGozfvDgQYfHGzduxMfHBwA/Pz9GjRoFFL0gsrKy1IzLHefSa1isLS0tzX7CHz9+vJpxC7BifcjJyWF65AT8awbyzpSZVzXiAG5uTi2b1zVz6mT2//wTG7Z9y4cfTDElw5Ndny52+UOhj9G8xeOkbt7Ivp/+l4ZNQgxO5mj9mmQyT52kQ6cwKlW+x7QcVqgRrq6uVzXiUPRa7tC5G2lfpvD7oV8Ny7Nh7SoAXhk11n5eBvDx9eWloSN4+5+vkrA4ntFvTjAkjxXqg1O/k6hXr57D42XLlpGTk0NGRgabNm2yL69bty41atRwZhQRpwgKCsLd3Z1y5crRrFkzIiMjOX/+vNmx5AqXn/DFGqxYH1I3byQrM5MOT4VRUFDA56uSiJoWycL5c/jtoHHNSnH2fP8ts6ZOZuSYcQTXqXfjv2CCkm4lAShh4i8sl3zy8UcAPNf3RVNzWLlG2Gw2UjatB6B2vfqG7ffPE8cBqOF/9bcD1f+7bHvqFsPyWKE+OPUd07lzZ8LCwkhOTgZgyJAhDBkyxGGb1q1bM2/ePA2kkjvSoUOHgKKvj3ft2sWuXbtYtGgRKSkpVKpUyeR0ItZlxfqw5/tvAHAtUYI2Dzfg0K/77etcXV0ZOHQE495935Asl7t48SLDX+rD/Q0aMWTE64bv/2Yc+SOdL1M2cc+9Vah7/wPmZkk/zLaUzVSpWo3H23YwNYuVakRubi6z3n+XwsJCMk+dZFvKZn7dv4+e4f15rJVxg1wr+vgCkH74N+6rU9dh3R+HfwNweO/dDZz6ybiLiwtJSUm89dZbxZ5M/f39CQ8PJzAw0JkxRG67WrVqMWDAACZMmMDw4cPx8/Ozr9u7d+9VTYWIOLJifTj5nz8BmBs9nXLly/N5yk4OZJxh5fqtBNYKJjZqGh9/FGNYnkumTnqb3w4e4IOY+GIvnTFbXl4erwzszcWLF3lz4numZ0xYHI/NZqPn8/1My2LFGpGXm8v0f03gg8iJLJg7m4MHfmHwK6OYGjXX0Byt23YEIHp6JBcuXLAvP3XyJPM+nAHA6ewsQzOZzamfjOfl5dGnTx8SEhKAoq8le/TowalTp4iLi+Pw4cMMGDCA7777jlmzZjkzishts379eurUqeOw7J133qFp06b88ssvQNHgtOzsbI2DELkGK9YHm80GQEl3d+I+TebeKkUN1EOhjzF3USJPPNKQ2Khp9H0xwpA8ALu//oo5s97ntbHjqWPgpQQ3y2az8ergfuzYvpXn+w2kx7O9Tc+zdHE8Li4u9Oo9wJQMVq0RnmXLcuxMITabjeMZx/hi3WdEThjLNzu/YvGKz/EqV86QHN2eeY5lSxawfeu/afPwA7R6ogP5eXmsX5Nsv77fqJldrMKp/9vY2Fj7idbb25u0tDQmTJhAVFQUMTF/f7oQHR3N/v1311cScue68iQL4OXlRf/+/e2PCwoK9JoWuQ4r1gevckWNUcPGIfZG/JI69erjXzOQ3w8dJDsry5A8+fn5vDqoL3XrN+DlkWMM2eetsNlsjIwYwMpln9C9VzjvzZxjdiS2/nsTR/9IJ7Rla2oEmDOrldVrhKurK35Vq9H3xQimzJrLrh3bmTnVuOlD3dzcWJy0jtfGjsfFxZUl8XP5fHUS7Z/qytxFywHwqVTZsDxW4NRPxjdv3mz/OTg42OE3wJCQv0daFxYWsmfPHoKDg50ZR8RQGgchcm1WrA9B99UGoFx572LXX1p+4UIO5Sl+m9vp3NmzHDp4AAD/ilfPhgHQuc0jAMz/ZCUdO4c5PdMlNpuNEYP7k/jpQsL+8Swz5iywxKeZn1pk4ObNsEKNaNm6HQBfbUsxdL8eHh689sY4Xrtijvi0L4tyNGxs7mw8RnNqM15QUGD/ef/+/Q5fyezevdthWyuMZhW5kaSkJHJycujZs6fDFGdnzpwhPj7e/tjd3Z3atWubEVHkjmDF+hDa4nEAft3/81Xr8vLy+P3Qr5Tx9MTH15iBd+4eHjzb54Vi1329fSuHDh6g3ZNd8PGtZNiczODYiHfp3pOoeYtMv04ciq453rB2FRUqVKRj526mZLjTasSJjGMAuP13JhyzJS1dAkDXHr1MTmIspzbjrVq1st/OOCsri+bNm9OjRw8yMzOJi4uzb+fp6UloaKgzo4jcFunp6YwYMYLRo0fTsWNHAgMD+euvv0hMTOTo0aP27cLDw/Hy8jIxqVwuJibGPq91Wlqaw7rL5wGOiIggKCjI0Gx3KyvWh4DAIFq2aUfq5o0sWfARz/f7+9PV6OmRZGdl0b1XuGFzjZcuXZppsz8qdt2rg/px6OABhr32Bg82e9iQPPD3pSmJny6kc7d/EP3RYks04gArEhaRm5tL7xfCDbvp0JWsWCP279tLtRoBlClTxmH5+fPnGT92JABt2j1pSJZLzpw+fdU16muSl5OwKI5GDzblyS7Fz2fvDFaoD049o0RERJCYmMiOHTuAohHEEydOdNjG1dWVqKgovL29nRlF5LbKyMhwaBgu16JFC2bOnGlwIrmepUuXXvNGDtOmTbP/3KlTJzXjBrFqffjX9A/p8kRzRg8byIY1yQQF1+GnPd+xLXUL1Wr489ak4u+oeLeYHjmRZZ98jGfZsgTWCmbGlElXbdOhU1ixd8V0tk8XzgescYmKlWrE6qRlzI2eTrNHHqV6jQDKepXjeMZRtmxcR+apkzzU/DEGvjzCsDwATz3+EH7VqnNf7bp4eJTi+292kvZlCv41A5m7MNHQX/CsUB+c2oyXLl2a1NRU5s2bx/Lly/nxxx/JysrCzc0NPz8/QkNDGTZsGE2bNnVmDJHbpl+/fvj6+rJmzRr27NnDiRMnOH36NBUqVKBRo0Y899xz9O7d2zKfFIlYlVXrQ0BgEOu27mbqpLdJ2bSe1C0bqXTPvfR7aSgjx7yN7102sOxKRw7/DhRdz36tQX/VawQY3ox/t3sn+/b+SOOQZqbOc27FGtG2QydOZBxj99dpfLPzK86dPYtXufLUq9+Art170avPAMPvLNule0/WrU7i2107yM/Lo7p/TV59/X+IGD7asFldrMTpz767uztDhw5l6NChzt6ViNN5e3sTHh5OeHi42VHkFqSkpJgdQYph1fpQtVp1ZsyJv/GGJpoRu4AZsQvumv3eSOOQZhw7U2h2DEvWiIZNQmjYxFoDIkeNHc+osePNjgFYoz6YP/RZREREROQupWZcRERERMQkasZFREREREyiZlxERERExCRqxkVERERETKJmXERERETEJGrGRURERERMomZcRERERMQkasZFREREREyiZlxERERExCRqxkVERERETOJSWFhYeFMburjg6upKlSpVnJ3pppw7d47SZTzNjuEg57y1Mv3nzxNUqnyP2THsrPb8QFEmT0/rZDpxQsfseqx2vDIyMigoKDA7hiWoRlyf1d5LVqsPYL3nyGrnG6vVB9Axu55bqQ833YyLiIiIiMjtpctURERERERMomZcRERERMQkasZFREREREyiZlxERERExCRqxkVERERETKJmXERERETEJGrGRURERERMomZcRERERMQkasZFREREREzyf0LiH78cI4Z5AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "âœ… Part 1 Complete! Dataset ready for training.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 10: Test Dataset Loading\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ§ª Testing Dataset Loading\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create a small test dataset\n",
        "test_dataset = PuzzleDataset(\n",
        "    dataset_paths=[OUTPUT_DIR],\n",
        "    global_batch_size=32,\n",
        "    seed=42,\n",
        "    test_set_mode=True,\n",
        "    epochs_per_iter=1,\n",
        "    rank=0,\n",
        "    num_replicas=1,\n",
        "    split=\"test\"\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“Š Dataset metadata:\")\n",
        "print(f\"  Sequence length: {test_dataset.metadata.seq_len}\")\n",
        "print(f\"  Vocabulary size: {test_dataset.metadata.vocab_size}\")\n",
        "print(f\"  Total puzzles: {test_dataset.metadata.total_puzzles}\")\n",
        "\n",
        "# Get one batch\n",
        "dataloader = DataLoader(test_dataset, batch_size=None)\n",
        "set_name, batch, batch_size = next(iter(dataloader))\n",
        "\n",
        "print(f\"\\nğŸ“¦ Sample batch:\")\n",
        "print(f\"  Set name: {set_name}\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "print(f\"  Input shape: {batch['inputs'].shape}\")\n",
        "print(f\"  Labels shape: {batch['labels'].shape}\")\n",
        "print(f\"  Input range: [{batch['inputs'].min()}, {batch['inputs'].max()}]\")\n",
        "print(f\"  Labels range: [{batch['labels'].min()}, {batch['labels'].max()}]\")\n",
        "\n",
        "# Visualize first example\n",
        "print(\"\\nğŸ¨ Visualizing first example...\")\n",
        "first_input = batch['inputs'][0].numpy().reshape(9, 9)\n",
        "first_label = batch['labels'][0].numpy().reshape(9, 9)\n",
        "visualize_sudoku(first_input, first_label, title=\"Sample Sudoku Puzzle\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Part 1 Complete! Dataset ready for training.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“¦ Part 2: Original TinyRecursiveModels Implementation\n",
        "\n",
        "This section implements the original TinyRecursiveModels (TRM) architecture from the paper \"Less is More: Recursive Reasoning with Tiny Networks\".\n",
        "\n",
        "## Key Components:\n",
        "1. **Basic Layers**: `CastedLinear`, `CastedEmbedding`, `RotaryEmbedding`\n",
        "2. **Attention & FFN**: `Attention`, `SwiGLU`, `rms_norm`\n",
        "3. **Sparse Embedding**: `CastedSparseEmbedding` for puzzle-specific embeddings\n",
        "4. **TRM Model**: Hierarchical recursive reasoning with ACT (Adaptive Computation Time)\n",
        "5. **Loss Functions**: `ACTLossHead` with stablemax cross-entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11: Original Layers\n",
        "\n",
        "Basic building blocks for the TRM model:\n",
        "- `CastedLinear`: Linear layer with automatic dtype casting\n",
        "- `CastedEmbedding`: Embedding layer with dtype casting\n",
        "- `RotaryEmbedding`: Rotary Position Embedding (RoPE)\n",
        "- `Attention`: Multi-head attention with RoPE support\n",
        "- `SwiGLU`: SwiGLU FFN (as used in LLaMA)\n",
        "- `rms_norm`: Root Mean Square Layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Original Layers defined!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ Available layers:\n",
            "  - CastedLinear: Linear with dtype casting\n",
            "  - CastedEmbedding: Embedding with dtype casting\n",
            "  - RotaryEmbedding: RoPE positional encoding\n",
            "  - Attention: Multi-head attention with RoPE\n",
            "  - SwiGLU: SwiGLU FFN\n",
            "  - rms_norm: RMS normalization function\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 11: Original Layers (CastedLinear, CastedEmbedding, Attention, etc.)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Type alias for RoPE cos/sin tensors\n",
        "CosSin = Tuple[torch.Tensor, torch.Tensor]\n",
        "\n",
        "def _find_multiple(a: int, b: int) -> int:\n",
        "    \"\"\"Find smallest multiple of b that is >= a (ceiling division * b).\"\"\"\n",
        "    return (-(a // -b)) * b\n",
        "\n",
        "# ============ RoPE Helpers ============\n",
        "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Rotates half the hidden dims of the input for RoPE.\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(\n",
        "    q: torch.Tensor, \n",
        "    k: torch.Tensor, \n",
        "    cos: torch.Tensor, \n",
        "    sin: torch.Tensor\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Apply rotary position embeddings to query and key tensors.\n",
        "    \n",
        "    Args:\n",
        "        q, k: [batch_size, seq_len, num_heads, head_dim]\n",
        "        cos, sin: [seq_len, head_dim]\n",
        "    \"\"\"\n",
        "    orig_dtype = q.dtype\n",
        "    q = q.to(cos.dtype)\n",
        "    k = k.to(cos.dtype)\n",
        "    \n",
        "    q_embed = (q * cos.unsqueeze(-2)) + (rotate_half(q) * sin.unsqueeze(-2))\n",
        "    k_embed = (k * cos.unsqueeze(-2)) + (rotate_half(k) * sin.unsqueeze(-2))\n",
        "    \n",
        "    return q_embed.to(orig_dtype), k_embed.to(orig_dtype)\n",
        "\n",
        "# ============ CastedLinear ============\n",
        "class CastedLinear(nn.Module):\n",
        "    \"\"\"Linear layer with automatic dtype casting and truncated normal init.\"\"\"\n",
        "    \n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
        "        super().__init__()\n",
        "        # Truncated LeCun normal init: std = 1/sqrt(fan_in)\n",
        "        self.weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((out_features, in_features)), std=1.0 / (in_features ** 0.5))\n",
        "        )\n",
        "        self.bias = None\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros((out_features,)))\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return F.linear(\n",
        "            input, \n",
        "            self.weight.to(input.dtype), \n",
        "            bias=self.bias.to(input.dtype) if self.bias is not None else None\n",
        "        )\n",
        "\n",
        "# ============ CastedEmbedding ============\n",
        "class CastedEmbedding(nn.Module):\n",
        "    \"\"\"Embedding layer with dtype casting and truncated normal init.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, init_std: float, cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "        self.embedding_weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std)\n",
        "        )\n",
        "        \n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return F.embedding(input, self.embedding_weight.to(self.cast_to))\n",
        "\n",
        "# ============ RotaryEmbedding ============\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    \"\"\"Rotary Position Embedding (RoPE) as used in LLaMA.\"\"\"\n",
        "    \n",
        "    def __init__(self, dim: int, max_position_embeddings: int, base: float = 10000.0, device=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Compute inverse frequencies\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))\n",
        "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
        "        freqs = torch.outer(t, inv_freq)\n",
        "        \n",
        "        # Cache cos and sin\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.cos_cached = nn.Buffer(emb.cos(), persistent=False)\n",
        "        self.sin_cached = nn.Buffer(emb.sin(), persistent=False)\n",
        "\n",
        "    def forward(self) -> CosSin:\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "# ============ Attention ============\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"Multi-head attention with optional RoPE and causal masking.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        hidden_size: int, \n",
        "        head_dim: int, \n",
        "        num_heads: int, \n",
        "        num_key_value_heads: int, \n",
        "        causal: bool = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.head_dim = head_dim\n",
        "        self.output_size = head_dim * num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.num_key_value_heads = num_key_value_heads\n",
        "        self.causal = causal\n",
        "\n",
        "        # Fused QKV projection\n",
        "        self.qkv_proj = CastedLinear(\n",
        "            self.hidden_size, \n",
        "            (self.num_heads + 2 * self.num_key_value_heads) * self.head_dim, \n",
        "            bias=False\n",
        "        )\n",
        "        self.o_proj = CastedLinear(self.output_size, self.hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "        # QKV projection\n",
        "        qkv = self.qkv_proj(hidden_states)\n",
        "        qkv = qkv.view(batch_size, seq_len, self.num_heads + 2 * self.num_key_value_heads, self.head_dim)\n",
        "        \n",
        "        query = qkv[:, :, :self.num_heads]\n",
        "        key = qkv[:, :, self.num_heads: self.num_heads + self.num_key_value_heads]\n",
        "        value = qkv[:, :, self.num_heads + self.num_key_value_heads:]\n",
        "\n",
        "        # Apply RoPE\n",
        "        if cos_sin is not None:\n",
        "            cos, sin = cos_sin\n",
        "            query, key = apply_rotary_pos_emb(query, key, cos, sin)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        # Reshape for attention: [B, S, H, D] -> [B, H, S, D]\n",
        "        query = einops.rearrange(query, 'B S H D -> B H S D')\n",
        "        key = einops.rearrange(key, 'B S H D -> B H S D')\n",
        "        value = einops.rearrange(value, 'B S H D -> B H S D')\n",
        "        \n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            query=query, key=key, value=value, is_causal=self.causal\n",
        "        )\n",
        "        \n",
        "        # Reshape back: [B, H, S, D] -> [B, S, H*D]\n",
        "        attn_output = einops.rearrange(attn_output, 'B H S D -> B S (H D)')\n",
        "        \n",
        "        return self.o_proj(attn_output)\n",
        "\n",
        "# ============ SwiGLU FFN ============\n",
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU Feed-Forward Network as used in LLaMA.\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size: int, expansion: float):\n",
        "        super().__init__()\n",
        "        # Compute intermediate size (2/3 * expansion * hidden_size, rounded to multiple of 256)\n",
        "        inter = _find_multiple(round(expansion * hidden_size * 2 / 3), 256)\n",
        "        \n",
        "        self.gate_up_proj = CastedLinear(hidden_size, inter * 2, bias=False)\n",
        "        self.down_proj = CastedLinear(inter, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
        "        return self.down_proj(F.silu(gate) * up)\n",
        "\n",
        "# ============ RMS Normalization ============\n",
        "def rms_norm(hidden_states: torch.Tensor, variance_epsilon: float = 1e-5) -> torch.Tensor:\n",
        "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
        "    input_dtype = hidden_states.dtype\n",
        "    hidden_states = hidden_states.to(torch.float32)\n",
        "    \n",
        "    variance = hidden_states.square().mean(-1, keepdim=True)\n",
        "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
        "    \n",
        "    return hidden_states.to(input_dtype)\n",
        "\n",
        "# ============ LinearSwish (optional) ============\n",
        "class LinearSwish(nn.Module):\n",
        "    \"\"\"Linear layer with SiLU activation.\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size: int, reverse: bool = False):\n",
        "        super().__init__()\n",
        "        self.linear = CastedLinear(hidden_size, hidden_size, bias=False)\n",
        "        self.reverse = reverse\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.reverse:\n",
        "            return F.silu(self.linear(x))\n",
        "        else:\n",
        "            return self.linear(F.silu(x))\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Original Layers defined!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“‹ Available layers:\")\n",
        "print(\"  - CastedLinear: Linear with dtype casting\")\n",
        "print(\"  - CastedEmbedding: Embedding with dtype casting\")\n",
        "print(\"  - RotaryEmbedding: RoPE positional encoding\")\n",
        "print(\"  - Attention: Multi-head attention with RoPE\")\n",
        "print(\"  - SwiGLU: SwiGLU FFN\")\n",
        "print(\"  - rms_norm: RMS normalization function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 12: Original SparseEmbedding\n",
        "\n",
        "Puzzle-specific embeddings that are:\n",
        "- Efficiently updated using SignSGD (sparse gradient update)\n",
        "- Cast to the forward dtype during forward pass\n",
        "- Stored locally for gradient accumulation during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Original SparseEmbedding defined!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ Available classes:\n",
            "  - CastedSparseEmbedding: Sparse embedding with efficient updates\n",
            "  - CastedSparseEmbeddingSignSGD: SignSGD optimizer for sparse embeddings\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 12: Original SparseEmbedding\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "class CastedSparseEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Sparse embedding for puzzle-specific parameters.\n",
        "    \n",
        "    During training:\n",
        "    - Copies embeddings to local buffer for gradient computation\n",
        "    - Uses SignSGD for efficient sparse updates\n",
        "    \n",
        "    During inference:\n",
        "    - Directly indexes into the weight matrix\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        num_embeddings: int, \n",
        "        embedding_dim: int, \n",
        "        batch_size: int, \n",
        "        init_std: float, \n",
        "        cast_to: torch.dtype\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "\n",
        "        # Main weights (persistent, no gradient)\n",
        "        self.weights = nn.Buffer(\n",
        "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std), \n",
        "            persistent=True\n",
        "        )\n",
        "\n",
        "        # Local weights for training (with gradient, not persistent)\n",
        "        self.local_weights = nn.Buffer(\n",
        "            torch.zeros(batch_size, embedding_dim, requires_grad=True), \n",
        "            persistent=False\n",
        "        )\n",
        "        # Local IDs tracking which embeddings are in local_weights\n",
        "        self.local_ids = nn.Buffer(\n",
        "            torch.zeros(batch_size, dtype=torch.int32), \n",
        "            persistent=False\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training:\n",
        "            # Inference: direct lookup\n",
        "            return self.weights[inputs].to(self.cast_to)\n",
        "            \n",
        "        # Training: copy to local buffer for gradient\n",
        "        with torch.no_grad():\n",
        "            self.local_weights.copy_(self.weights[inputs])\n",
        "            self.local_ids.copy_(inputs)\n",
        "        \n",
        "        # Retain gradient for non-leaf tensor\n",
        "        self.local_weights.retain_grad()\n",
        "        \n",
        "        return self.local_weights.to(self.cast_to)\n",
        "\n",
        "\n",
        "class CastedSparseEmbeddingSignSGD(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    SignSGD optimizer for sparse embeddings.\n",
        "    \n",
        "    Adam â‰ˆ SignSGD when gradients are very sparse, so this is more efficient.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr: float = 1e-3,\n",
        "        weight_decay: float = 1e-2,\n",
        "    ):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        # Manually set param_groups to bypass PyTorch's leaf tensor check\n",
        "        # Buffers are not leaf tensors, but we handle them specially in step()\n",
        "        if isinstance(params, torch.Tensor):\n",
        "            params = [params]\n",
        "        \n",
        "        # Convert to list if needed\n",
        "        params = list(params)\n",
        "        \n",
        "        # Create param_groups manually (bypassing super().__init__ validation)\n",
        "        # We directly set param_groups without calling add_param_group to avoid validation\n",
        "        if isinstance(params[0], dict):\n",
        "            param_groups = params\n",
        "        else:\n",
        "            param_groups = [{'params': params}]\n",
        "        \n",
        "        # Initialize Optimizer base class attributes manually\n",
        "        self.defaults = defaults\n",
        "        import weakref\n",
        "        self.state = weakref.WeakValueDictionary()\n",
        "        self._hook_for_profile = None\n",
        "        from collections import OrderedDict\n",
        "        self._optimizer_step_pre_hooks = OrderedDict()\n",
        "        self._optimizer_step_post_hooks = OrderedDict()\n",
        "        \n",
        "        # Set param_groups directly (merge defaults into each group)\n",
        "        self.param_groups = []\n",
        "        for param_group in param_groups:\n",
        "            # Merge defaults with param_group\n",
        "            merged_group = {**defaults, **param_group}\n",
        "            self.param_groups.append(merged_group)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            # Find sparse embedding components\n",
        "            local_weights_grad = None\n",
        "            local_ids = None\n",
        "            weights = None\n",
        "            \n",
        "            for p in group[\"params\"]:\n",
        "                if p.requires_grad:\n",
        "                    # Use getattr to safely access grad without triggering warnings\n",
        "                    local_weights_grad = getattr(p, 'grad', None)\n",
        "                elif p.ndim == 1:\n",
        "                    local_ids = p\n",
        "                elif p.ndim == 2:\n",
        "                    weights = p\n",
        "                    \n",
        "            if local_weights_grad is None or local_ids is None or weights is None:\n",
        "                continue\n",
        "        \n",
        "            # Unique IDs and aggregate gradients\n",
        "            grad_ids, inv = local_ids.unique(return_inverse=True)\n",
        "            N, D = local_weights_grad.shape\n",
        "            \n",
        "            grad = torch.zeros((grad_ids.shape[0], D), dtype=local_weights_grad.dtype, device=local_weights_grad.device)\n",
        "            grad.scatter_add_(0, inv.unsqueeze(-1).expand(-1, D), local_weights_grad)\n",
        "\n",
        "            # SignSGD with decoupled weight decay\n",
        "            p = weights[grad_ids]\n",
        "            p.mul_(1.0 - group[\"lr\"] * group[\"weight_decay\"])\n",
        "            p.add_(torch.sign(grad), alpha=-group[\"lr\"])\n",
        "\n",
        "            # Write back\n",
        "            weights[grad_ids] = p\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Original SparseEmbedding defined!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“‹ Available classes:\")\n",
        "print(\"  - CastedSparseEmbedding: Sparse embedding with efficient updates\")\n",
        "print(\"  - CastedSparseEmbeddingSignSGD: SignSGD optimizer for sparse embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 13: Original TRM Model\n",
        "\n",
        "The TinyRecursiveReasoningModel with Adaptive Computation Time (ACT):\n",
        "\n",
        "**Architecture:**\n",
        "- **H-level**: High-level reasoning state (updated every H_cycles)\n",
        "- **L-level**: Low-level reasoning state (updated every L_cycles within each H-cycle)\n",
        "- **Input injection**: Token embeddings + puzzle embeddings injected at each L-step\n",
        "\n",
        "**ACT (Adaptive Computation Time):**\n",
        "- Model learns when to halt computation via Q-learning\n",
        "- `q_halt_logits`: Confidence that current answer is correct\n",
        "- `q_continue_logits`: Expected value of continuing computation\n",
        "- Exploration during training: Random early halting with probability `halt_exploration_prob`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Original TRM Model defined!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ Model components:\n",
            "  - OriginalTRMConfig: Model configuration\n",
            "  - TRMBlock: Single transformer block\n",
            "  - TRMReasoningModule: Stack of TRM blocks\n",
            "  - TRMInner: Core reasoning model\n",
            "  - OriginalTRM: Full model with ACT wrapper\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 13: Original TRM Model (TRMConfig, TRMBlock, TRMInner, TRM)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# ============ Carry Dataclasses ============\n",
        "@dataclass\n",
        "class TRMInnerCarry:\n",
        "    \"\"\"Inner carry state for TRM reasoning.\"\"\"\n",
        "    z_H: torch.Tensor  # High-level state: [B, L+puzzle_emb_len, D]\n",
        "    z_L: torch.Tensor  # Low-level state: [B, L+puzzle_emb_len, D]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TRMCarry:\n",
        "    \"\"\"Full carry state including ACT status.\"\"\"\n",
        "    inner_carry: TRMInnerCarry\n",
        "    steps: torch.Tensor         # Current step count: [B]\n",
        "    halted: torch.Tensor        # Whether each sequence has halted: [B]\n",
        "    current_data: Dict[str, torch.Tensor]  # Current batch data\n",
        "\n",
        "\n",
        "# ============ TRM Configuration ============\n",
        "class OriginalTRMConfig(BaseModel):\n",
        "    \"\"\"Configuration for the original TinyRecursiveReasoningModel.\"\"\"\n",
        "    \n",
        "    # Data dimensions\n",
        "    batch_size: int\n",
        "    seq_len: int\n",
        "    vocab_size: int\n",
        "    num_puzzle_identifiers: int\n",
        "    \n",
        "    # Model dimensions\n",
        "    hidden_size: int = 512\n",
        "    expansion: float = 4.0\n",
        "    num_heads: int = 8\n",
        "    \n",
        "    # Puzzle embeddings\n",
        "    puzzle_emb_ndim: int = 512\n",
        "    puzzle_emb_len: int = 16\n",
        "    \n",
        "    # Recursion structure\n",
        "    H_cycles: int = 3           # High-level cycles\n",
        "    L_cycles: int = 6           # Low-level cycles per H-cycle\n",
        "    H_layers: int = 0           # Layers in H-block (ignored in original)\n",
        "    L_layers: int = 2           # Layers in L-block\n",
        "    \n",
        "    # Position encoding\n",
        "    pos_encodings: str = \"rope\"  # \"rope\", \"learned\", or \"none\"\n",
        "    rope_theta: float = 10000.0\n",
        "    \n",
        "    # Normalization\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    \n",
        "    # ACT (Adaptive Computation Time)\n",
        "    halt_max_steps: int = 16\n",
        "    halt_exploration_prob: float = 0.1\n",
        "    \n",
        "    # Dtype\n",
        "    forward_dtype: str = \"bfloat16\"\n",
        "    \n",
        "    # Additional options\n",
        "    mlp_t: bool = False          # Use MLP instead of transformer on L\n",
        "    no_ACT_continue: bool = True # Only use halt sigmoid, no continue loss\n",
        "\n",
        "\n",
        "# ============ TRM Block ============\n",
        "class TRMBlock(nn.Module):\n",
        "    \"\"\"Single transformer block for TRM.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: OriginalTRMConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        if config.mlp_t:\n",
        "            # MLP over sequence dimension instead of attention\n",
        "            self.puzzle_emb_len = -(config.puzzle_emb_ndim // -config.hidden_size) if config.puzzle_emb_len == 0 else config.puzzle_emb_len\n",
        "            self.mlp_t = SwiGLU(\n",
        "                hidden_size=config.seq_len + self.puzzle_emb_len, # L\n",
        "                expansion=config.expansion,\n",
        "            )\n",
        "        else:\n",
        "            # Standard self-attention\n",
        "            self.self_attn = Attention(\n",
        "                hidden_size=config.hidden_size,\n",
        "                head_dim=config.hidden_size // config.num_heads,\n",
        "                num_heads=config.num_heads,\n",
        "                num_key_value_heads=config.num_heads,\n",
        "                causal=False\n",
        "            )\n",
        "        \n",
        "        # FFN\n",
        "        self.mlp = SwiGLU(\n",
        "            hidden_size=config.hidden_size,\n",
        "            expansion=config.expansion,\n",
        "        )\n",
        "        self.norm_eps = config.rms_norm_eps\n",
        "\n",
        "    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        # B, L, D = hidden_states.shape\n",
        "        # Post Norm\n",
        "        if self.config.mlp_t:\n",
        "            hidden_states = hidden_states.transpose(1,2)\n",
        "            out = self.mlp_t(hidden_states)\n",
        "            hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "            hidden_states = hidden_states.transpose(1,2)\n",
        "        else:\n",
        "            # Self Attention\n",
        "            hidden_states = rms_norm(hidden_states + self.self_attn(cos_sin=cos_sin, hidden_states=hidden_states), variance_epsilon=self.norm_eps)\n",
        "        # Fully Connected\n",
        "        out = self.mlp(hidden_states)\n",
        "        hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# ============ TRM Reasoning Module ============\n",
        "class TRMReasoningModule(nn.Module):\n",
        "    \"\"\"Stack of TRM blocks with input injection.\"\"\"\n",
        "    \n",
        "    def __init__(self, layers: List[TRMBlock]):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        hidden_states: torch.Tensor, \n",
        "        input_injection: torch.Tensor, \n",
        "        **kwargs\n",
        "    ) -> torch.Tensor:\n",
        "        hidden_states = hidden_states + input_injection\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states=hidden_states, **kwargs)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# ============ TRM Inner Model ============\n",
        "class TRMInner(nn.Module):\n",
        "    \"\"\"Inner TRM model (without ACT wrapper).\"\"\"\n",
        "    \n",
        "    def __init__(self, config: OriginalTRMConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.forward_dtype = getattr(torch, config.forward_dtype)\n",
        "        \n",
        "        # Embedding scale\n",
        "        self.embed_scale = math.sqrt(config.hidden_size)\n",
        "        embed_init_std = 1.0 / self.embed_scale\n",
        "        \n",
        "        # Token embeddings\n",
        "        self.embed_tokens = CastedEmbedding(\n",
        "            config.vocab_size, \n",
        "            config.hidden_size, \n",
        "            init_std=embed_init_std, \n",
        "            cast_to=self.forward_dtype\n",
        "        )\n",
        "        \n",
        "        # Output heads\n",
        "        self.lm_head = CastedLinear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.q_head = CastedLinear(config.hidden_size, 2, bias=True)  # [halt, continue]\n",
        "        \n",
        "        # Puzzle embeddings\n",
        "        self.puzzle_emb_len = -(config.puzzle_emb_ndim // -config.hidden_size) if config.puzzle_emb_len == 0 else config.puzzle_emb_len  # ceil div\n",
        "        if config.puzzle_emb_ndim > 0:\n",
        "            self.puzzle_emb = CastedSparseEmbedding(\n",
        "                config.num_puzzle_identifiers, \n",
        "                config.puzzle_emb_ndim,\n",
        "                batch_size=config.batch_size, \n",
        "                init_std=0,  # Zero init puzzle embeddings\n",
        "                cast_to=self.forward_dtype\n",
        "            )\n",
        "        \n",
        "        # Position encodings\n",
        "        if config.pos_encodings == \"rope\":\n",
        "            self.rotary_emb = RotaryEmbedding(\n",
        "                dim=config.hidden_size // config.num_heads,\n",
        "                max_position_embeddings=config.seq_len + self.puzzle_emb_len,\n",
        "                base=config.rope_theta\n",
        "            )\n",
        "        elif config.pos_encodings == \"learned\":\n",
        "            self.embed_pos = CastedEmbedding(\n",
        "                config.seq_len + self.puzzle_emb_len, \n",
        "                config.hidden_size, \n",
        "                init_std=embed_init_std, \n",
        "                cast_to=self.forward_dtype\n",
        "            )\n",
        "        \n",
        "        # Reasoning layers (L-level)\n",
        "        self.L_level = TRMReasoningModule(\n",
        "            layers=[TRMBlock(config) for _ in range(config.L_layers)]\n",
        "        )\n",
        "        \n",
        "        # Initial states for H and L\n",
        "        self.H_init = nn.Buffer(\n",
        "            trunc_normal_init_(torch.empty(config.hidden_size, dtype=self.forward_dtype), std=1), \n",
        "            persistent=True\n",
        "        )\n",
        "        self.L_init = nn.Buffer(\n",
        "            trunc_normal_init_(torch.empty(config.hidden_size, dtype=self.forward_dtype), std=1), \n",
        "            persistent=True\n",
        "        )\n",
        "        \n",
        "        # Initialize Q-head to near-zero for faster learning\n",
        "        with torch.no_grad():\n",
        "            self.q_head.weight.zero_()\n",
        "            self.q_head.bias.fill_(-5)\n",
        "\n",
        "    def _input_embeddings(self, input: torch.Tensor, puzzle_identifiers: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute input embeddings including puzzle embeddings and position encodings.\"\"\"\n",
        "        # Token embedding\n",
        "        embedding = self.embed_tokens(input.to(torch.int32))\n",
        "        \n",
        "        # Puzzle embeddings\n",
        "        if self.config.puzzle_emb_ndim > 0:\n",
        "            puzzle_embedding = self.puzzle_emb(puzzle_identifiers)\n",
        "            \n",
        "            # Pad if needed\n",
        "            pad_count = self.puzzle_emb_len * self.config.hidden_size - puzzle_embedding.shape[-1]\n",
        "            if pad_count > 0:\n",
        "                puzzle_embedding = F.pad(puzzle_embedding, (0, pad_count))\n",
        "            \n",
        "            # Reshape and prepend to embedding\n",
        "            embedding = torch.cat(\n",
        "                (puzzle_embedding.view(-1, self.puzzle_emb_len, self.config.hidden_size), embedding), \n",
        "                dim=-2\n",
        "            )\n",
        "        \n",
        "        # Learned position embeddings\n",
        "        if self.config.pos_encodings == \"learned\":\n",
        "            embedding = 0.707106781 * (embedding + self.embed_pos.embedding_weight.to(self.forward_dtype))\n",
        "        \n",
        "        # Scale\n",
        "        return self.embed_scale * embedding\n",
        "\n",
        "    def empty_carry(self, batch_size: int):\n",
        "        \"\"\"Create empty carry state (with device fix for CUDA compatibility).\"\"\"\n",
        "        # Get device from model parameters if available\n",
        "        device = next(self.parameters()).device if list(self.parameters()) else None\n",
        "        \n",
        "        return TRMInnerCarry(\n",
        "            z_H=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype, device=device),\n",
        "            z_L=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype, device=device),\n",
        "        )\n",
        "        \n",
        "    def reset_carry(self, reset_flag: torch.Tensor, carry: TRMInnerCarry):\n",
        "        \"\"\"Reset carry state for halted sequences (with device fix for CUDA compatibility).\"\"\"\n",
        "        # Ensure buffers are on the same device as carry (fix for CUDA)\n",
        "        device = carry.z_H.device\n",
        "        H_init = self.H_init.to(device)\n",
        "        L_init = self.L_init.to(device)\n",
        "        \n",
        "        return TRMInnerCarry(\n",
        "            z_H=torch.where(reset_flag.view(-1, 1, 1), H_init, carry.z_H),\n",
        "            z_L=torch.where(reset_flag.view(-1, 1, 1), L_init, carry.z_L),\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        carry: TRMInnerCarry, \n",
        "        batch: Dict[str, torch.Tensor]\n",
        "    ) -> Tuple[TRMInnerCarry, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"Forward pass through TRM.\"\"\"\n",
        "        \n",
        "        # Get RoPE embeddings\n",
        "        cos_sin = self.rotary_emb() if hasattr(self, \"rotary_emb\") else None\n",
        "        seq_info = dict(cos_sin=cos_sin)\n",
        "        \n",
        "        # Input encoding\n",
        "        input_embeddings = self._input_embeddings(batch[\"inputs\"], batch[\"puzzle_identifiers\"])\n",
        "        \n",
        "        # Recursive reasoning\n",
        "        z_H, z_L = carry.z_H, carry.z_L\n",
        "        \n",
        "        # H_cycles-1 iterations without gradient (for efficiency)\n",
        "        with torch.no_grad():\n",
        "            for _H_step in range(self.config.H_cycles - 1):\n",
        "                for _L_step in range(self.config.L_cycles):\n",
        "                    z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "                z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "        \n",
        "        # Last H-cycle with gradient\n",
        "        for _L_step in range(self.config.L_cycles):\n",
        "            z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "        z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "        \n",
        "        # LM Outputs\n",
        "        new_carry = TRMInnerCarry(z_H=z_H.detach(), z_L=z_L.detach())  # New carry no grad\n",
        "        output = self.lm_head(z_H)[:, self.puzzle_emb_len:]\n",
        "        q_logits = self.q_head(z_H[:, 0]).to(torch.float32) # Q-head; uses the first puzzle_emb position\n",
        "        return new_carry, output, (q_logits[..., 0], q_logits[..., 1])\n",
        "\n",
        "\n",
        "# ============ TRM with ACT Wrapper ============\n",
        "class OriginalTRM(nn.Module):\n",
        "    \"\"\"Original TinyRecursiveReasoningModel with Adaptive Computation Time (ACT).\"\"\"\n",
        "    \n",
        "    def __init__(self, config_dict: dict):\n",
        "        super().__init__()\n",
        "        self.config = OriginalTRMConfig(**config_dict)\n",
        "        self.inner = TRMInner(self.config)\n",
        "\n",
        "    @property\n",
        "    def puzzle_emb(self):\n",
        "        return self.inner.puzzle_emb\n",
        "\n",
        "    def initial_carry(self, batch: Dict[str, torch.Tensor]):\n",
        "        \"\"\"Initialize carry state for a batch (with device fix for CUDA compatibility).\"\"\"\n",
        "        batch_size = batch[\"inputs\"].shape[0]\n",
        "        device = batch[\"inputs\"].device\n",
        "        \n",
        "        # Move model to device if needed (ensures buffers are on correct device)\n",
        "        if next(self.inner.parameters()).device != device:\n",
        "            self.inner = self.inner.to(device)\n",
        "        \n",
        "        return TRMCarry(\n",
        "            inner_carry=self.inner.empty_carry(batch_size),\n",
        "            steps=torch.zeros((batch_size,), dtype=torch.int32, device=device),\n",
        "            halted=torch.ones((batch_size,), dtype=torch.bool, device=device),  # Start halted to trigger reset\n",
        "            current_data={k: torch.empty_like(v) for k, v in batch.items()}\n",
        "        )\n",
        "        \n",
        "    def forward(\n",
        "        self, \n",
        "        carry: TRMCarry, \n",
        "        batch: Dict[str, torch.Tensor]\n",
        "    ) -> Tuple[TRMCarry, Dict[str, torch.Tensor]]:\n",
        "        \"\"\"Forward pass with ACT.\"\"\"\n",
        "        \n",
        "        # Update data, carry (removing halted sequences)\n",
        "        new_inner_carry = self.inner.reset_carry(carry.halted, carry.inner_carry)\n",
        "        \n",
        "        new_steps = torch.where(carry.halted, 0, carry.steps)\n",
        "\n",
        "        new_current_data = {k: torch.where(carry.halted.view((-1, ) + (1, ) * (batch[k].ndim - 1)), batch[k], v) for k, v in carry.current_data.items()}\n",
        "\n",
        "        # Forward inner model\n",
        "        new_inner_carry, logits, (q_halt_logits, q_continue_logits) = self.inner(new_inner_carry, new_current_data)\n",
        "\n",
        "        outputs = {\n",
        "            \"logits\": logits,\n",
        "            \"q_halt_logits\": q_halt_logits,\n",
        "            \"q_continue_logits\": q_continue_logits\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Step\n",
        "            new_steps = new_steps + 1\n",
        "            is_last_step = new_steps >= self.config.halt_max_steps\n",
        "            \n",
        "            halted = is_last_step\n",
        "\n",
        "            # if training, and ACT is enabled\n",
        "            if self.training and (self.config.halt_max_steps > 1):\n",
        "\n",
        "                # Halt signal\n",
        "                # NOTE: During evaluation, always use max steps, this is to guarantee the same halting steps inside a batch for batching purposes\n",
        "                \n",
        "                if self.config.no_ACT_continue:\n",
        "                    halted = halted | (q_halt_logits > 0)\n",
        "                else:\n",
        "                    halted = halted | (q_halt_logits > q_continue_logits)\n",
        "\n",
        "                # Exploration\n",
        "                min_halt_steps = (torch.rand_like(q_halt_logits) < self.config.halt_exploration_prob) * torch.randint_like(new_steps, low=2, high=self.config.halt_max_steps + 1)\n",
        "                halted = halted & (new_steps >= min_halt_steps)\n",
        "\n",
        "                if not self.config.no_ACT_continue:\n",
        "                    # Compute target Q\n",
        "                    # NOTE: No replay buffer and target networks for computing target Q-value.\n",
        "                    # As batch_size is large, there're many parallel envs.\n",
        "                    # Similar concept as PQN https://arxiv.org/abs/2407.04811\n",
        "                    _, _, (next_q_halt_logits, next_q_continue_logits) = self.inner(new_inner_carry, new_current_data)\n",
        "                    outputs[\"target_q_continue\"] = torch.sigmoid(torch.where(is_last_step, next_q_halt_logits, torch.maximum(next_q_halt_logits, next_q_continue_logits)))\n",
        "\n",
        "        return TRMCarry(new_inner_carry, new_steps, halted, new_current_data), outputs\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Original TRM Model defined!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“‹ Model components:\")\n",
        "print(\"  - OriginalTRMConfig: Model configuration\")\n",
        "print(\"  - TRMBlock: Single transformer block\")\n",
        "print(\"  - TRMReasoningModule: Stack of TRM blocks\")\n",
        "print(\"  - TRMInner: Core reasoning model\")\n",
        "print(\"  - OriginalTRM: Full model with ACT wrapper\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 14: Original Loss Functions\n",
        "\n",
        "Loss functions for training TRM:\n",
        "- **Stablemax Cross-Entropy**: More stable alternative to softmax for numerical stability\n",
        "- **ACTLossHead**: Combines LM loss with Q-learning losses for ACT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Original Loss Functions defined!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ Available functions:\n",
            "  - stablemax_cross_entropy: Stablemax-based cross-entropy\n",
            "  - softmax_cross_entropy: Standard cross-entropy\n",
            "  - ACTLossHead: Combined loss for TRM with ACT\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 14: Original Loss Functions\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# ============ Stablemax Functions ============\n",
        "def s(x: torch.Tensor, epsilon: float = 1e-30) -> torch.Tensor:\n",
        "    \"\"\"Stablemax helper function.\"\"\"\n",
        "    return torch.where(\n",
        "        x < 0,\n",
        "        1 / (1 - x + epsilon),\n",
        "        x + 1\n",
        "    )\n",
        "\n",
        "def log_stablemax(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
        "    \"\"\"Log-stablemax: more stable than log-softmax for certain distributions.\"\"\"\n",
        "    s_x = s(x)\n",
        "    return torch.log(s_x / torch.sum(s_x, dim=dim, keepdim=True))\n",
        "\n",
        "def stablemax_cross_entropy(\n",
        "    logits: torch.Tensor, \n",
        "    labels: torch.Tensor, \n",
        "    ignore_index: int = IGNORE_LABEL_ID, \n",
        "    valid_mask: torch.Tensor = None\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Cross-entropy loss using stablemax instead of softmax.\n",
        "    \n",
        "    More numerically stable for certain distributions.\n",
        "    \"\"\"\n",
        "    logprobs = log_stablemax(logits.to(torch.float64), dim=-1)\n",
        "    \n",
        "    if valid_mask is None:\n",
        "        valid_mask = (labels != ignore_index)\n",
        "    \n",
        "    transformed_labels = torch.where(valid_mask, labels, 0)\n",
        "    prediction_logprobs = torch.gather(\n",
        "        logprobs, \n",
        "        index=transformed_labels.to(torch.long).unsqueeze(-1), \n",
        "        dim=-1\n",
        "    ).squeeze(-1)\n",
        "    \n",
        "    return -torch.where(valid_mask, prediction_logprobs, 0)\n",
        "\n",
        "def softmax_cross_entropy(\n",
        "    logits: torch.Tensor, \n",
        "    labels: torch.Tensor, \n",
        "    ignore_index: int = IGNORE_LABEL_ID,\n",
        "    valid_mask: torch.Tensor = None  # unused, for API compatibility\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Standard softmax cross-entropy loss.\"\"\"\n",
        "    return F.cross_entropy(\n",
        "        logits.to(torch.float32).view(-1, logits.shape[-1]), \n",
        "        labels.to(torch.long).view(-1), \n",
        "        ignore_index=ignore_index, \n",
        "        reduction=\"none\"\n",
        "    ).view(labels.shape)\n",
        "\n",
        "\n",
        "# ============ ACT Loss Head ============\n",
        "class ACTLossHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Loss head for TRM with Adaptive Computation Time.\n",
        "    \n",
        "    Combines:\n",
        "    1. LM loss (stablemax or softmax cross-entropy)\n",
        "    2. Q-halt loss (BCE for halting decision)\n",
        "    3. Q-continue loss (optional, for Q-learning)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model: nn.Module, loss_type: str = \"stablemax_cross_entropy\"):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        \n",
        "        # Get loss function by name\n",
        "        loss_functions = {\n",
        "            \"stablemax_cross_entropy\": stablemax_cross_entropy,\n",
        "            \"softmax_cross_entropy\": softmax_cross_entropy\n",
        "        }\n",
        "        self.loss_fn = loss_functions[loss_type]\n",
        "        \n",
        "    def initial_carry(self, *args, **kwargs):\n",
        "        return self.model.initial_carry(*args, **kwargs)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        return_keys: Sequence[str] = (),\n",
        "        **model_kwargs,\n",
        "    ) -> Tuple[Any, torch.Tensor, Dict[str, torch.Tensor], Optional[Dict[str, torch.Tensor]], torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass computing loss and metrics.\n",
        "        \n",
        "        Returns:\n",
        "            - new_carry: Updated carry state\n",
        "            - loss: Total loss (LM + Q-halt + Q-continue)\n",
        "            - metrics: Dictionary of metrics (accuracy, loss components, etc.)\n",
        "            - detached_outputs: Requested output tensors (detached)\n",
        "            - all_halted: Whether all sequences have halted\n",
        "        \"\"\"\n",
        "        # Forward through model\n",
        "        new_carry, outputs = self.model(**model_kwargs)\n",
        "        labels = new_carry.current_data[\"labels\"]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Predictions\n",
        "            outputs[\"preds\"] = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "            \n",
        "            # Compute correctness\n",
        "            mask = (labels != IGNORE_LABEL_ID)\n",
        "            loss_counts = mask.sum(-1)\n",
        "            loss_divisor = loss_counts.clamp_min(1).unsqueeze(-1)\n",
        "            \n",
        "            is_correct = mask & (torch.argmax(outputs[\"logits\"], dim=-1) == labels)\n",
        "            seq_is_correct = is_correct.sum(-1) == loss_counts\n",
        "            \n",
        "            # Metrics (only for halted sequences)\n",
        "            valid_metrics = new_carry.halted & (loss_counts > 0)\n",
        "            metrics = {\n",
        "                \"count\": valid_metrics.sum(),\n",
        "                \"accuracy\": torch.where(valid_metrics, (is_correct.to(torch.float32) / loss_divisor).sum(-1), 0).sum(),\n",
        "                \"exact_accuracy\": (valid_metrics & seq_is_correct).sum(),\n",
        "                \"q_halt_accuracy\": (valid_metrics & ((outputs[\"q_halt_logits\"] >= 0) == seq_is_correct)).sum(),\n",
        "                \"steps\": torch.where(valid_metrics, new_carry.steps, 0).sum(),\n",
        "            }\n",
        "        \n",
        "        # Losses\n",
        "        lm_loss = (self.loss_fn(outputs[\"logits\"], labels, ignore_index=IGNORE_LABEL_ID, valid_mask=mask) / loss_divisor).sum()\n",
        "        q_halt_loss = F.binary_cross_entropy_with_logits(\n",
        "            outputs[\"q_halt_logits\"], \n",
        "            seq_is_correct.to(outputs[\"q_halt_logits\"].dtype), \n",
        "            reduction=\"sum\"\n",
        "        )\n",
        "        \n",
        "        metrics.update({\n",
        "            \"lm_loss\": lm_loss.detach(),\n",
        "            \"q_halt_loss\": q_halt_loss.detach(),\n",
        "        })\n",
        "        \n",
        "        # Q-continue loss (optional)\n",
        "        q_continue_loss = 0\n",
        "        if \"target_q_continue\" in outputs:\n",
        "            q_continue_loss = F.binary_cross_entropy_with_logits(\n",
        "                outputs[\"q_continue_logits\"], \n",
        "                outputs[\"target_q_continue\"], \n",
        "                reduction=\"sum\"\n",
        "            )\n",
        "            metrics[\"q_continue_loss\"] = q_continue_loss.detach()\n",
        "        \n",
        "        # Total loss\n",
        "        total_loss = lm_loss + 0.5 * (q_halt_loss + q_continue_loss)\n",
        "        \n",
        "        # Detached outputs\n",
        "        detached_outputs = {k: outputs[k].detach() for k in return_keys if k in outputs}\n",
        "        \n",
        "        return new_carry, total_loss, metrics, detached_outputs, new_carry.halted.all()\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Original Loss Functions defined!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“‹ Available functions:\")\n",
        "print(\"  - stablemax_cross_entropy: Stablemax-based cross-entropy\")\n",
        "print(\"  - softmax_cross_entropy: Standard cross-entropy\")\n",
        "print(\"  - ACTLossHead: Combined loss for TRM with ACT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 15: Test Original TRM Model\n",
        "\n",
        "Verify that the original TRM model works correctly by:\n",
        "1. Creating a model instance with Sudoku configuration\n",
        "2. Running a forward pass with sample data\n",
        "3. Checking output shapes and loss computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ§ª Testing Original TRM Model\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ Model Configuration:\n",
            "  batch_size: 32\n",
            "  seq_len: 81\n",
            "  vocab_size: 11\n",
            "  num_puzzle_identifiers: 1\n",
            "  hidden_size: 512\n",
            "  expansion: 4.0\n",
            "  num_heads: 8\n",
            "  puzzle_emb_ndim: 512\n",
            "  puzzle_emb_len: 16\n",
            "  H_cycles: 3\n",
            "  L_cycles: 6\n",
            "  H_layers: 0\n",
            "  L_layers: 2\n",
            "  pos_encodings: rope\n",
            "  rope_theta: 10000.0\n",
            "  halt_max_steps: 16\n",
            "  halt_exploration_prob: 0.1\n",
            "  forward_dtype: bfloat16\n",
            "  mlp_t: False\n",
            "  no_ACT_continue: True\n",
            "\n",
            "ğŸ”§ Creating Original TRM model...\n",
            "  Total parameters: 6,828,034\n",
            "  Trainable parameters: 6,828,034\n",
            "\n",
            "ğŸ¯ Creating ACTLossHead...\n",
            "\n",
            "ğŸš€ Running test forward pass...\n",
            "  Input shape: torch.Size([32, 81])\n",
            "  Labels shape: torch.Size([32, 81])\n",
            "\n",
            "ğŸ“Š Forward pass results:\n",
            "  Loss: 80.0560\n",
            "  LM Loss: 79.9485\n",
            "  Q-Halt Loss: 0.2149\n",
            "  Accuracy: 0.0000\n",
            "  Exact Accuracy: 0.0000\n",
            "  All Halted: False\n",
            "\n",
            "ğŸ”„ Testing ACT loop (multiple steps)...\n",
            "  Step 1: Loss=80.0560, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 2: Loss=82.5028, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 3: Loss=83.0621, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 4: Loss=81.6288, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 5: Loss=81.2846, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 6: Loss=83.0382, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 7: Loss=83.7332, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 8: Loss=83.3171, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 9: Loss=83.0415, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 10: Loss=82.6429, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 11: Loss=82.8062, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 12: Loss=82.3195, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 13: Loss=84.1780, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 14: Loss=83.7683, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 15: Loss=82.4654, Halted=0/32, Exact Acc=0.0000\n",
            "  Step 16: Loss=82.1543, Halted=32/32, Exact Acc=0.0000\n",
            "\n",
            "âœ… ACT completed in 16 steps\n",
            "\n",
            "======================================================================\n",
            "âœ… Part 2 Complete! Original TRM Model is working.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 15: Test Original TRM Model\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ§ª Testing Original TRM Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============ Model Configuration for Sudoku ============\n",
        "BATCH_SIZE = 32\n",
        "SEQ_LEN = 81  # 9x9 Sudoku\n",
        "\n",
        "original_trm_config = {\n",
        "    # Data dimensions\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"seq_len\": SEQ_LEN,\n",
        "    \"vocab_size\": train_metadata.vocab_size,  # 11 for Sudoku (0-9 + pad)\n",
        "    \"num_puzzle_identifiers\": train_metadata.num_puzzle_identifiers,\n",
        "    \n",
        "    # Model dimensions\n",
        "    \"hidden_size\": 512,\n",
        "    \"expansion\": 4.0,\n",
        "    \"num_heads\": 8,\n",
        "    \n",
        "    # Puzzle embeddings\n",
        "    \"puzzle_emb_ndim\": 512,\n",
        "    \"puzzle_emb_len\": 16,\n",
        "    \n",
        "    # Recursion structure\n",
        "    \"H_cycles\": 3,\n",
        "    \"L_cycles\": 6,\n",
        "    \"H_layers\": 0,\n",
        "    \"L_layers\": 2,\n",
        "    \n",
        "    # Position encoding\n",
        "    \"pos_encodings\": \"rope\",\n",
        "    \"rope_theta\": 10000.0,\n",
        "    \n",
        "    # ACT\n",
        "    \"halt_max_steps\": 16,\n",
        "    \"halt_exploration_prob\": 0.1,\n",
        "    \n",
        "    # Dtype\n",
        "    \"forward_dtype\": \"bfloat16\",\n",
        "    \n",
        "    # Options\n",
        "    \"mlp_t\": False,\n",
        "    \"no_ACT_continue\": True,\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ“‹ Model Configuration:\")\n",
        "for k, v in original_trm_config.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# ============ Create Model ============\n",
        "print(\"\\nğŸ”§ Creating Original TRM model...\")\n",
        "original_model = OriginalTRM(original_trm_config)\n",
        "original_model = original_model.to(DEVICE)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in original_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in original_model.parameters() if p.requires_grad)\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# ============ Create Loss Head ============\n",
        "print(\"\\nğŸ¯ Creating ACTLossHead...\")\n",
        "original_loss_head = ACTLossHead(original_model, loss_type=\"stablemax_cross_entropy\")\n",
        "original_loss_head = original_loss_head.to(DEVICE)\n",
        "\n",
        "# ============ Test Forward Pass ============\n",
        "print(\"\\nğŸš€ Running test forward pass...\")\n",
        "\n",
        "# Create test batch\n",
        "test_train_dataset = PuzzleDataset(\n",
        "    dataset_paths=[OUTPUT_DIR],\n",
        "    global_batch_size=BATCH_SIZE,\n",
        "    seed=42,\n",
        "    test_set_mode=False,\n",
        "    epochs_per_iter=1,\n",
        "    rank=0,\n",
        "    num_replicas=1,\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_train_dataset, batch_size=None)\n",
        "_, test_batch, _ = next(iter(test_dataloader))\n",
        "\n",
        "# Move to device\n",
        "test_batch = {k: v.to(DEVICE) for k, v in test_batch.items()}\n",
        "\n",
        "print(f\"  Input shape: {test_batch['inputs'].shape}\")\n",
        "print(f\"  Labels shape: {test_batch['labels'].shape}\")\n",
        "\n",
        "# Initialize carry\n",
        "original_model.train()\n",
        "carry = original_loss_head.initial_carry(batch=test_batch)\n",
        "\n",
        "# Forward pass\n",
        "new_carry, loss, metrics, outputs, all_halted = original_loss_head(\n",
        "    return_keys=[\"logits\", \"preds\"],\n",
        "    carry=carry,\n",
        "    batch=test_batch\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“Š Forward pass results:\")\n",
        "print(f\"  Loss: {loss.item():.4f}\")\n",
        "print(f\"  LM Loss: {metrics['lm_loss'].item():.4f}\")\n",
        "print(f\"  Q-Halt Loss: {metrics['q_halt_loss'].item():.4f}\")\n",
        "print(f\"  Accuracy: {metrics['accuracy'].item() / max(metrics['count'].item(), 1):.4f}\")\n",
        "print(f\"  Exact Accuracy: {metrics['exact_accuracy'].item() / max(metrics['count'].item(), 1):.4f}\")\n",
        "print(f\"  All Halted: {all_halted}\")\n",
        "\n",
        "# ============ Test ACT Loop ============\n",
        "print(\"\\nğŸ”„ Testing ACT loop (multiple steps)...\")\n",
        "carry = original_loss_head.initial_carry(batch=test_batch)\n",
        "\n",
        "step = 0\n",
        "while not all_halted and step < 20:  # Max 20 steps for safety\n",
        "    new_carry, loss, metrics, outputs, all_halted = original_loss_head(\n",
        "        return_keys=[],\n",
        "        carry=carry,\n",
        "        batch=test_batch\n",
        "    )\n",
        "    carry = new_carry\n",
        "    step += 1\n",
        "    \n",
        "    halted_count = carry.halted.sum().item()\n",
        "    print(f\"  Step {step}: Loss={loss.item():.4f}, Halted={halted_count}/{BATCH_SIZE}, \"\n",
        "          f\"Exact Acc={metrics['exact_accuracy'].item() / max(metrics['count'].item(), 1):.4f}\")\n",
        "\n",
        "print(f\"\\nâœ… ACT completed in {step} steps\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Part 2 Complete! Original TRM Model is working.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“¦ Part 3: Optimized TinyRecursiveModels Implementation \n",
        "\n",
        "This section implements the optimized TinyRecursiveModels from TinyRecursiveModels-Trelis. The key optimizations include:\n",
        "\n",
        "## ğŸ”§ Key Optimizations\n",
        "\n",
        "| Optimization | Description |\n",
        "|--------------|-------------|\n",
        "| **LoRA (Low-Rank Adaptation)** | Efficient fine-tuning by adding low-rank matrices to linear layers |\n",
        "| **Dropout** | `puzzle_emb_dropout` and `grid_token_dropout` for regularization |\n",
        "| **EMA (Exponential Moving Average)** | Maintains shadow copies of model weights for stable training |\n",
        "| **halt_max_steps_eval** | Different max steps during evaluation vs training |\n",
        "| **no_ACT_continue** | Uses sigmoid of halt instead of comparing halt vs continue Q-values |\n",
        "\n",
        "## ğŸ“‹ Components\n",
        "\n",
        "1. **OptimizedCastedLinear**: Linear layer with built-in LoRA support\n",
        "2. **EMAHelper**: Exponential Moving Average for model weights\n",
        "3. **OptimizedTRM**: Full model with all optimizations\n",
        "4. **enable_lora_for_linears()**: Helper to enable LoRA across all linear layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 16: Optimized Layers with LoRA Support\n",
        "\n",
        "The key difference from the original implementation is that `OptimizedCastedLinear` has built-in LoRA (Low-Rank Adaptation) support:\n",
        "- LoRA adds low-rank matrices A and B such that the effective weight becomes: W + BA\n",
        "- This allows efficient fine-tuning by only training the small LoRA matrices\n",
        "- The `enable_lora()` method initializes LoRA parameters for a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Optimized Layers with LoRA Support defined!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ New components:\n",
            "  - OptimizedCastedLinear: Linear layer with built-in LoRA support\n",
            "  - enable_lora_for_linears(): Helper to enable LoRA across all layers\n",
            "  - OptimizedAttention: Attention using OptimizedCastedLinear\n",
            "  - OptimizedSwiGLU: SwiGLU FFN using OptimizedCastedLinear\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 16: Optimized Layers with LoRA Support\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Type alias\n",
        "OptCosSin = Tuple[torch.Tensor, torch.Tensor]\n",
        "\n",
        "# ============ Optimized CastedLinear with LoRA ============\n",
        "class OptimizedCastedLinear(nn.Module):\n",
        "    \"\"\"Linear layer with automatic dtype casting and built-in LoRA support.\n",
        "    \n",
        "    Key differences from original:\n",
        "    - Built-in LoRA (Low-Rank Adaptation) support for efficient fine-tuning\n",
        "    - LoRA adds low-rank matrices A and B such that: output = xW^T + x(BA)^T * scaling\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool):\n",
        "        super().__init__()\n",
        "        # Truncated LeCun normal init\n",
        "        self.weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((out_features, in_features)), std=1.0 / (in_features ** 0.5))\n",
        "        )\n",
        "        self.bias = None\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros((out_features,)))\n",
        "        \n",
        "        # LoRA members (disabled by default)\n",
        "        self._lora_rank: int = 0\n",
        "        self._lora_alpha: float = 1.0\n",
        "        self._lora_scaling: float = 1.0\n",
        "        self._lora_dropout: Optional[nn.Module] = None\n",
        "        self._lora_A: Optional[nn.Parameter] = None\n",
        "        self._lora_B: Optional[nn.Parameter] = None\n",
        "        self._lora_train_base: bool = True\n",
        "        self._lora_train_bias: bool = True\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        # Standard linear transformation\n",
        "        result = F.linear(\n",
        "            input,\n",
        "            self.weight.to(input.dtype),\n",
        "            bias=self.bias.to(input.dtype) if self.bias is not None else None\n",
        "        )\n",
        "        \n",
        "        # Add LoRA contribution if enabled\n",
        "        if self._lora_rank > 0 and self._lora_A is not None and self._lora_B is not None:\n",
        "            lora_input = input\n",
        "            if self._lora_dropout is not None:\n",
        "                lora_input = self._lora_dropout(lora_input)\n",
        "            \n",
        "            lora_input_fp = lora_input.to(self._lora_A.dtype)\n",
        "            # Project down then up using LoRA factors: BA * x\n",
        "            lora_down = F.linear(lora_input_fp, self._lora_A)\n",
        "            lora_update = F.linear(lora_down, self._lora_B)\n",
        "            result = result + lora_update.to(result.dtype) * self._lora_scaling\n",
        "        \n",
        "        return result\n",
        "\n",
        "    def enable_lora(self,\n",
        "                    rank: int,\n",
        "                    alpha: Optional[float] = None,\n",
        "                    dropout: float = 0.0,\n",
        "                    train_base: bool = False,\n",
        "                    train_bias: bool = False) -> None:\n",
        "        \"\"\"Enable LoRA for this linear layer.\n",
        "        \n",
        "        Args:\n",
        "            rank: LoRA rank (dimension of low-rank matrices)\n",
        "            alpha: Scaling factor (defaults to rank)\n",
        "            dropout: Dropout applied to LoRA branch\n",
        "            train_base: If True, also train the base weight matrix\n",
        "            train_bias: If True, also train the bias\n",
        "        \"\"\"\n",
        "        if rank <= 0:\n",
        "            return\n",
        "        \n",
        "        if self._lora_rank > 0:\n",
        "            raise RuntimeError(\"LoRA already enabled for this layer.\")\n",
        "        \n",
        "        in_features = self.weight.shape[1]\n",
        "        out_features = self.weight.shape[0]\n",
        "        \n",
        "        self._lora_rank = rank\n",
        "        self._lora_alpha = float(alpha if alpha is not None else rank)\n",
        "        self._lora_scaling = self._lora_alpha / self._lora_rank\n",
        "        self._lora_dropout = nn.Identity() if dropout <= 0 else nn.Dropout(p=dropout)\n",
        "        self._lora_train_base = train_base\n",
        "        self._lora_train_bias = train_bias\n",
        "        \n",
        "        # LoRA factor matrices\n",
        "        self._lora_A = nn.Parameter(torch.zeros((rank, in_features)))\n",
        "        self._lora_B = nn.Parameter(torch.zeros((out_features, rank)))\n",
        "        \n",
        "        # Initialization following LoRA paper recommendations\n",
        "        nn.init.kaiming_uniform_(self._lora_A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self._lora_B)\n",
        "        \n",
        "        # Optionally freeze base weights\n",
        "        if not self._lora_train_base:\n",
        "            self.weight.requires_grad = False\n",
        "        if self.bias is not None and not self._lora_train_bias:\n",
        "            self.bias.requires_grad = False\n",
        "\n",
        "    @property\n",
        "    def lora_rank(self) -> int:\n",
        "        return self._lora_rank\n",
        "\n",
        "\n",
        "# ============ LoRA Helper Function ============\n",
        "def enable_lora_for_linears(\n",
        "    module: nn.Module,\n",
        "    rank: int,\n",
        "    alpha: Optional[float] = None,\n",
        "    dropout: float = 0.0,\n",
        "    train_base: bool = False,\n",
        "    train_bias: bool = False,\n",
        ") -> None:\n",
        "    \"\"\"Enable LoRA adapters for every OptimizedCastedLinear submodule.\n",
        "    \n",
        "    Args:\n",
        "        module: Root module to traverse\n",
        "        rank: LoRA rank. <=0 disables LoRA\n",
        "        alpha: Scaling factor (defaults to rank when None)\n",
        "        dropout: Optional dropout applied to the LoRA branch\n",
        "        train_base: If False, freeze original linear weights\n",
        "        train_bias: If False, freeze original biases\n",
        "    \"\"\"\n",
        "    if rank <= 0:\n",
        "        return\n",
        "    \n",
        "    for submodule in module.modules():\n",
        "        if isinstance(submodule, OptimizedCastedLinear):\n",
        "            submodule.enable_lora(\n",
        "                rank=rank,\n",
        "                alpha=alpha,\n",
        "                dropout=dropout,\n",
        "                train_base=train_base,\n",
        "                train_bias=train_bias,\n",
        "            )\n",
        "\n",
        "\n",
        "# ============ Other Optimized Layers ============\n",
        "class OptimizedCastedEmbedding(nn.Module):\n",
        "    \"\"\"Embedding layer with dtype casting.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, init_std: float, cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "        self.embedding_weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std)\n",
        "        )\n",
        "        \n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return F.embedding(input, self.embedding_weight.to(self.cast_to))\n",
        "\n",
        "\n",
        "class OptimizedRotaryEmbedding(nn.Module):\n",
        "    \"\"\"Rotary Position Embedding (RoPE).\"\"\"\n",
        "    \n",
        "    def __init__(self, dim, max_position_embeddings, base, device=None):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))\n",
        "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
        "        freqs = torch.outer(t, inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.cos_cached = nn.Buffer(emb.cos(), persistent=False)\n",
        "        self.sin_cached = nn.Buffer(emb.sin(), persistent=False)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "\n",
        "def opt_rotate_half(x: torch.Tensor):\n",
        "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n",
        "def opt_apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor):\n",
        "    \"\"\"Apply rotary position embeddings to query and key tensors.\"\"\"\n",
        "    orig_dtype = q.dtype\n",
        "    q = q.to(cos.dtype)\n",
        "    k = k.to(cos.dtype)\n",
        "    q_embed = (q * cos.unsqueeze(-2)) + (opt_rotate_half(q) * sin.unsqueeze(-2))\n",
        "    k_embed = (k * cos.unsqueeze(-2)) + (opt_rotate_half(k) * sin.unsqueeze(-2))\n",
        "    return q_embed.to(orig_dtype), k_embed.to(orig_dtype)\n",
        "\n",
        "\n",
        "class OptimizedAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention with RoPE support using OptimizedCastedLinear (LoRA-compatible).\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size, head_dim, num_heads, num_key_value_heads, causal=False):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.head_dim = head_dim\n",
        "        self.output_size = head_dim * num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.num_key_value_heads = num_key_value_heads\n",
        "        self.causal = causal\n",
        "        \n",
        "        # Use OptimizedCastedLinear for LoRA support\n",
        "        self.qkv_proj = OptimizedCastedLinear(self.hidden_size, (self.num_heads + 2 * self.num_key_value_heads) * self.head_dim, bias=False)\n",
        "        self.o_proj = OptimizedCastedLinear(self.output_size, self.hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, cos_sin: OptCosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "        \n",
        "        qkv = self.qkv_proj(hidden_states)\n",
        "        qkv = qkv.reshape(batch_size, seq_len, self.num_heads + 2 * self.num_key_value_heads, self.head_dim)\n",
        "        query = qkv[:, :, :self.num_heads]\n",
        "        key = qkv[:, :, self.num_heads: self.num_heads + self.num_key_value_heads]\n",
        "        value = qkv[:, :, self.num_heads + self.num_key_value_heads:]\n",
        "        \n",
        "        if cos_sin is not None:\n",
        "            cos, sin = cos_sin\n",
        "            query, key = opt_apply_rotary_pos_emb(query, key, cos, sin)\n",
        "        \n",
        "        # Use PyTorch's scaled_dot_product_attention\n",
        "        from torch.nn.functional import scaled_dot_product_attention\n",
        "        query, key, value = map(lambda t: einops.rearrange(t, 'B S H D -> B H S D'), (query, key, value))\n",
        "        attn_output = scaled_dot_product_attention(query=query, key=key, value=value, is_causal=self.causal)\n",
        "        attn_output = einops.rearrange(attn_output, 'B H S D -> B S H D')\n",
        "        attn_output = attn_output.reshape(batch_size, seq_len, self.output_size)\n",
        "        return self.o_proj(attn_output)\n",
        "\n",
        "\n",
        "class OptimizedSwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU Feed-Forward Network using OptimizedCastedLinear (LoRA-compatible).\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size: int, expansion: float):\n",
        "        super().__init__()\n",
        "        inter = find_multiple(round(expansion * hidden_size * 2 / 3), 256)\n",
        "        # Use OptimizedCastedLinear for LoRA support\n",
        "        self.gate_up_proj = OptimizedCastedLinear(hidden_size, inter * 2, bias=False)\n",
        "        self.down_proj = OptimizedCastedLinear(inter, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
        "        return self.down_proj(F.silu(gate) * up)\n",
        "\n",
        "\n",
        "def opt_rms_norm(hidden_states: torch.Tensor, variance_epsilon: float) -> torch.Tensor:\n",
        "    \"\"\"RMS Layer Normalization.\"\"\"\n",
        "    input_dtype = hidden_states.dtype\n",
        "    hidden_states = hidden_states.to(torch.float32)\n",
        "    variance = hidden_states.square().mean(-1, keepdim=True)\n",
        "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
        "    return hidden_states.to(input_dtype)\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Optimized Layers with LoRA Support defined!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“‹ New components:\")\n",
        "print(\"  - OptimizedCastedLinear: Linear layer with built-in LoRA support\")\n",
        "print(\"  - enable_lora_for_linears(): Helper to enable LoRA across all layers\")\n",
        "print(\"  - OptimizedAttention: Attention using OptimizedCastedLinear\")\n",
        "print(\"  - OptimizedSwiGLU: SwiGLU FFN using OptimizedCastedLinear\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 17: EMA (Exponential Moving Average) Helper\n",
        "\n",
        "EMA maintains shadow copies of model weights using exponential moving average:\n",
        "- `shadow[t] = Î¼ * shadow[t-1] + (1-Î¼) * param[t]`\n",
        "- Typically Î¼ = 0.999 (99.9% of the old shadow, 0.1% of the new param)\n",
        "- Used during evaluation for more stable predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… EMA Helper defined!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ EMAHelper methods:\n",
            "  - register(model): Initialize shadow weights from model\n",
            "  - update(model): Update shadow weights after optimizer.step()\n",
            "  - ema_copy(model): Get model copy with EMA weights for evaluation\n",
            "  - state_dict() / load_state_dict(): Save/load shadow weights\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 17: EMA (Exponential Moving Average) Helper\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import copy\n",
        "\n",
        "class EMAHelper(object):\n",
        "    \"\"\"Exponential Moving Average helper for model weights.\n",
        "    \n",
        "    Maintains shadow copies of model weights using:\n",
        "        shadow[t] = Î¼ * shadow[t-1] + (1-Î¼) * param[t]\n",
        "    \n",
        "    This provides more stable weights for evaluation by averaging\n",
        "    over the training trajectory.\n",
        "    \n",
        "    Usage:\n",
        "        ema = EMAHelper(mu=0.999)\n",
        "        ema.register(model)  # Initialize shadow weights\n",
        "        \n",
        "        # During training:\n",
        "        optimizer.step()\n",
        "        ema.update(model)  # Update shadow weights\n",
        "        \n",
        "        # For evaluation:\n",
        "        ema_model = ema.ema_copy(model)  # Get model with EMA weights\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, mu: float = 0.999):\n",
        "        \"\"\"Initialize EMA helper.\n",
        "        \n",
        "        Args:\n",
        "            mu: EMA decay rate (0.999 means 99.9% old shadow, 0.1% new param)\n",
        "        \"\"\"\n",
        "        self.mu = mu\n",
        "        self.shadow = {}\n",
        "\n",
        "    def register(self, module: nn.Module) -> None:\n",
        "        \"\"\"Register a module and initialize shadow weights.\n",
        "        \n",
        "        Args:\n",
        "            module: PyTorch module to track\n",
        "        \"\"\"\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self, module: nn.Module) -> None:\n",
        "        \"\"\"Update shadow weights with current module parameters.\n",
        "        \n",
        "        Args:\n",
        "            module: PyTorch module with updated weights\n",
        "        \"\"\"\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name].data = (1. - self.mu) * param.data + self.mu * self.shadow[name].data\n",
        "\n",
        "    def ema(self, module: nn.Module) -> None:\n",
        "        \"\"\"Copy shadow weights to module (in-place).\n",
        "        \n",
        "        Args:\n",
        "            module: PyTorch module to update with shadow weights\n",
        "        \"\"\"\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data.copy_(self.shadow[name].data)\n",
        "\n",
        "    def ema_copy(self, module: nn.Module) -> nn.Module:\n",
        "        \"\"\"Create a copy of the module with shadow weights.\n",
        "        \n",
        "        Args:\n",
        "            module: PyTorch module to copy\n",
        "            \n",
        "        Returns:\n",
        "            Copy of module with EMA weights\n",
        "        \"\"\"\n",
        "        # Save original state dict\n",
        "        original_state_dict = module.state_dict()\n",
        "        \n",
        "        # Temporarily apply EMA weights to create state dict with EMA weights\n",
        "        temp_state_dict = {}\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad and name in self.shadow:\n",
        "                temp_state_dict[name] = self.shadow[name].clone()\n",
        "            else:\n",
        "                temp_state_dict[name] = param.data.clone()\n",
        "        \n",
        "        # Copy buffers and non-trainable parameters from original\n",
        "        for name, buffer in module.named_buffers():\n",
        "            temp_state_dict[name] = buffer.clone()\n",
        "        \n",
        "        # Create a new model instance from config (avoids deepcopy issues with buffers)\n",
        "        if hasattr(module, 'config'):\n",
        "            # Extract config dict\n",
        "            if hasattr(module.config, 'model_dump'):\n",
        "                config_dict = module.config.model_dump()\n",
        "            elif hasattr(module.config, 'dict'):\n",
        "                config_dict = module.config.dict()\n",
        "            else:\n",
        "                config_dict = dict(module.config)\n",
        "            \n",
        "            # Create new instance\n",
        "            module_copy = type(module)(config_dict)\n",
        "        else:\n",
        "            # Fallback: try copy.deepcopy, but it may fail with buffers\n",
        "            try:\n",
        "                module_copy = copy.deepcopy(module)\n",
        "            except RuntimeError as e:\n",
        "                raise RuntimeError(\n",
        "                    f\"Cannot create EMA copy: deepcopy failed. \"\n",
        "                    f\"Module must have a 'config' attribute to create a new instance. \"\n",
        "                    f\"Original error: {e}\"\n",
        "                )\n",
        "        \n",
        "        # Move to same device\n",
        "        device = next(module.parameters()).device\n",
        "        module_copy = module_copy.to(device)\n",
        "        \n",
        "        # Load state dict with EMA weights\n",
        "        module_copy.load_state_dict(temp_state_dict, strict=False)\n",
        "        \n",
        "        return module_copy\n",
        "\n",
        "    def state_dict(self) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Get shadow weights for saving.\"\"\"\n",
        "        return self.shadow\n",
        "\n",
        "    def load_state_dict(self, state_dict: Dict[str, torch.Tensor]) -> None:\n",
        "        \"\"\"Load shadow weights from checkpoint.\"\"\"\n",
        "        self.shadow = state_dict\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… EMA Helper defined!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“‹ EMAHelper methods:\")\n",
        "print(\"  - register(model): Initialize shadow weights from model\")\n",
        "print(\"  - update(model): Update shadow weights after optimizer.step()\")\n",
        "print(\"  - ema_copy(model): Get model copy with EMA weights for evaluation\")\n",
        "print(\"  - state_dict() / load_state_dict(): Save/load shadow weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 18: Optimized Sparse Embedding\n",
        "\n",
        "Same as original sparse embedding - used for puzzle-specific parameters that are updated sparsely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Optimized Sparse Embedding defined!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 18: Optimized Sparse Embedding\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "class OptimizedCastedSparseEmbedding(nn.Module):\n",
        "    \"\"\"Sparse embedding for puzzle-specific parameters.\n",
        "    \n",
        "    Same as original implementation - handles both training (with local buffer)\n",
        "    and inference (direct lookup) modes.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, batch_size: int, \n",
        "                 init_std: float, cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "        \n",
        "        # Real Weights (stored as Buffer for sparse updates)\n",
        "        self.weights = nn.Buffer(\n",
        "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std),\n",
        "            persistent=True\n",
        "        )\n",
        "        \n",
        "        # Local weights and IDs for training\n",
        "        self.local_weights = nn.Buffer(\n",
        "            torch.zeros(batch_size, embedding_dim, requires_grad=True),\n",
        "            persistent=False\n",
        "        )\n",
        "        self.local_ids = nn.Buffer(\n",
        "            torch.zeros(batch_size, dtype=torch.int32),\n",
        "            persistent=False\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training:\n",
        "            # Inference: direct lookup\n",
        "            return self.weights[inputs].to(self.cast_to)\n",
        "        \n",
        "        # Training: fill local buffer from weights\n",
        "        with torch.no_grad():\n",
        "            self.local_weights.copy_(self.weights[inputs])\n",
        "            self.local_ids.copy_(inputs)\n",
        "        \n",
        "        # Retain gradient for non-leaf tensor\n",
        "        self.local_weights.retain_grad()\n",
        "        \n",
        "        return self.local_weights.to(self.cast_to)\n",
        "\n",
        "\n",
        "class OptimizedCastedSparseEmbeddingSignSGD(torch.optim.Optimizer):\n",
        "    \"\"\"Non-distributed SignSGD optimizer for sparse embedding updates.\n",
        "    \n",
        "    This is a proper torch.optim.Optimizer subclass that matches the interface\n",
        "    expected by PyTorch training loops.\n",
        "    \n",
        "    NOTE: Buffers (not Parameters) are passed to this optimizer, so we need to\n",
        "    bypass PyTorch's default parameter validation which requires leaf tensors.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, params, lr: float = 1e-3, weight_decay: float = 1e-2):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "        \n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        # Manually set param_groups to bypass PyTorch's leaf tensor check\n",
        "        # Buffers are not leaf tensors, but we handle them specially in step()\n",
        "        if isinstance(params, torch.Tensor):\n",
        "            params = [params]\n",
        "        \n",
        "        # Convert to list if needed\n",
        "        params = list(params)\n",
        "        \n",
        "        # Create param_groups manually (bypassing super().__init__ validation)\n",
        "        # We directly set param_groups without calling add_param_group to avoid validation\n",
        "        if isinstance(params[0], dict):\n",
        "            param_groups = params\n",
        "        else:\n",
        "            param_groups = [{'params': params}]\n",
        "        \n",
        "        # Initialize Optimizer base class attributes manually\n",
        "        self.defaults = defaults\n",
        "        import weakref\n",
        "        self.state = weakref.WeakValueDictionary()\n",
        "        self._hook_for_profile = None\n",
        "        from collections import OrderedDict\n",
        "        self._optimizer_step_pre_hooks = OrderedDict()\n",
        "        self._optimizer_step_post_hooks = OrderedDict()\n",
        "        \n",
        "        # Set param_groups directly (merge defaults into each group)\n",
        "        self.param_groups = []\n",
        "        for param_group in param_groups:\n",
        "            # Merge defaults with param_group\n",
        "            merged_group = {**defaults, **param_group}\n",
        "            self.param_groups.append(merged_group)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            local_weights_grad = None\n",
        "            local_ids = None\n",
        "            weights = None\n",
        "            \n",
        "            # Should have exactly 3 buffers: weights, local_weights, local_ids\n",
        "            for p in group[\"params\"]:\n",
        "                if p.requires_grad:\n",
        "                    # Use getattr to safely access grad without triggering warnings\n",
        "                    local_weights_grad = getattr(p, 'grad', None)\n",
        "                elif p.ndim == 1:\n",
        "                    local_ids = p\n",
        "                elif p.ndim == 2:\n",
        "                    weights = p\n",
        "            \n",
        "            # Skip if any component is missing\n",
        "            if local_weights_grad is None or local_ids is None or weights is None:\n",
        "                continue\n",
        "            \n",
        "            # Unique IDs and aggregate gradients\n",
        "            grad_ids, inv = local_ids.unique(return_inverse=True)\n",
        "            N, D = local_weights_grad.shape\n",
        "            grad = torch.zeros((grad_ids.shape[0], D), dtype=local_weights_grad.dtype, device=local_weights_grad.device)\n",
        "            grad.scatter_add_(0, inv.unsqueeze(-1).expand(-1, D), local_weights_grad)\n",
        "            \n",
        "            # SignSGD with decoupled weight decay\n",
        "            p = weights[grad_ids]\n",
        "            p.mul_(1.0 - group[\"lr\"] * group[\"weight_decay\"]).add_(torch.sign(grad), alpha=-group[\"lr\"])\n",
        "            weights[grad_ids] = p\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Optimized Sparse Embedding defined!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 19: Optimized TRM Model\n",
        "\n",
        "The optimized TRM model with all enhancements from TinyRecursiveModels-Trelis:\n",
        "\n",
        "### Key Differences from Original:\n",
        "1. **LoRA Support**: Configuration options for LoRA rank, alpha, dropout\n",
        "2. **Dropout**: `puzzle_emb_dropout` and `grid_token_dropout` for regularization\n",
        "3. **halt_max_steps_eval**: Different max steps during evaluation\n",
        "4. **no_ACT_continue**: Uses sigmoid(halt) instead of comparing halt vs continue Q-values\n",
        "5. **Uses OptimizedCastedLinear**: All linear layers support LoRA injection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Optimized TRM Model defined!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ Model components:\n",
            "  - OptimizedTRMConfig: Configuration with LoRA, dropout, halt_max_steps_eval\n",
            "  - OptimizedTRMBlock: Transformer block using OptimizedCastedLinear\n",
            "  - OptimizedTRMReasoningModule: Stack of optimized blocks\n",
            "  - OptimizedTRMInner: Core model with dropout support\n",
            "  - OptimizedTRM: Full model with ACT wrapper and LoRA injection\n",
            "\n",
            "ğŸ”§ New features:\n",
            "  - LoRA: Set lora_rank > 0 in config to enable\n",
            "  - Dropout: Use puzzle_emb_dropout and grid_token_dropout\n",
            "  - halt_max_steps_eval: Different max steps during evaluation\n",
            "  - no_ACT_continue: Use sigmoid(halt) instead of comparing halt vs continue\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 19: Optimized TRM Model\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# ============ Carry Data Structures ============\n",
        "@dataclass\n",
        "class OptimizedTRMInnerCarry:\n",
        "    \"\"\"Inner carry state for OptimizedTRM.\"\"\"\n",
        "    z_H: torch.Tensor\n",
        "    z_L: torch.Tensor\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class OptimizedTRMCarry:\n",
        "    \"\"\"Full carry state for OptimizedTRM with ACT.\"\"\"\n",
        "    inner_carry: OptimizedTRMInnerCarry\n",
        "    steps: torch.Tensor\n",
        "    halted: torch.Tensor\n",
        "    current_data: Dict[str, torch.Tensor]\n",
        "\n",
        "\n",
        "# ============ Optimized TRM Configuration ============\n",
        "class OptimizedTRMConfig(BaseModel):\n",
        "    \"\"\"Configuration for the Optimized TinyRecursiveReasoningModel.\n",
        "    \n",
        "    Key additions from original:\n",
        "    - puzzle_emb_dropout: Dropout for puzzle embeddings\n",
        "    - grid_token_dropout: Dropout for token embeddings\n",
        "    - halt_max_steps_eval: Different max steps during evaluation\n",
        "    - no_ACT_continue: Use sigmoid(halt) instead of comparing halt vs continue\n",
        "    - LoRA config: lora_rank, lora_alpha, lora_dropout, lora_train_base, lora_train_bias\n",
        "    \"\"\"\n",
        "    \n",
        "    batch_size: int\n",
        "    seq_len: int\n",
        "    puzzle_emb_ndim: int = 0\n",
        "    num_puzzle_identifiers: int\n",
        "    vocab_size: int\n",
        "    \n",
        "    H_cycles: int\n",
        "    L_cycles: int\n",
        "    H_layers: int  # ignored in single-level design\n",
        "    L_layers: int\n",
        "    \n",
        "    # Transformer config\n",
        "    hidden_size: int\n",
        "    expansion: float\n",
        "    num_heads: int\n",
        "    pos_encodings: str\n",
        "    \n",
        "    rms_norm_eps: float = 1e-5\n",
        "    rope_theta: float = 10000.0\n",
        "    \n",
        "    # Halting Q-learning config\n",
        "    halt_max_steps: int\n",
        "    halt_exploration_prob: float\n",
        "    halt_max_steps_eval: Optional[int] = None  # NEW: Different max steps for eval\n",
        "    \n",
        "    forward_dtype: str = \"bfloat16\"\n",
        "    \n",
        "    # NEW: Dropout for regularization\n",
        "    puzzle_emb_dropout: float = 0.0\n",
        "    grid_token_dropout: float = 0.0\n",
        "    \n",
        "    # Alexia additions\n",
        "    mlp_t: bool = False\n",
        "    puzzle_emb_len: int = 16\n",
        "    no_ACT_continue: bool = True  # NEW: Only use sigmoid of halt\n",
        "    \n",
        "    # NEW: LoRA config\n",
        "    lora_rank: int = 0\n",
        "    lora_alpha: float = 1.0\n",
        "    lora_dropout: float = 0.0\n",
        "    lora_train_base: bool = False\n",
        "    lora_train_bias: bool = False\n",
        "\n",
        "\n",
        "# ============ Optimized TRM Block ============\n",
        "class OptimizedTRMBlock(nn.Module):\n",
        "    \"\"\"Single transformer block for Optimized TRM.\n",
        "    \n",
        "    Uses OptimizedCastedLinear for LoRA support.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: OptimizedTRMConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        if self.config.mlp_t:\n",
        "            self.puzzle_emb_len = -(self.config.puzzle_emb_ndim // -self.config.hidden_size) if self.config.puzzle_emb_len == 0 else self.config.puzzle_emb_len\n",
        "            self.mlp_t = OptimizedSwiGLU(\n",
        "                hidden_size=self.config.seq_len + self.puzzle_emb_len,\n",
        "                expansion=config.expansion,\n",
        "            )\n",
        "        else:\n",
        "            self.self_attn = OptimizedAttention(\n",
        "                hidden_size=config.hidden_size,\n",
        "                head_dim=config.hidden_size // config.num_heads,\n",
        "                num_heads=config.num_heads,\n",
        "                num_key_value_heads=config.num_heads,\n",
        "                causal=False\n",
        "            )\n",
        "        \n",
        "        self.mlp = OptimizedSwiGLU(\n",
        "            hidden_size=config.hidden_size,\n",
        "            expansion=config.expansion,\n",
        "        )\n",
        "        self.norm_eps = config.rms_norm_eps\n",
        "\n",
        "    def forward(self, cos_sin: OptCosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        if self.config.mlp_t:\n",
        "            hidden_states = hidden_states.transpose(1, 2)\n",
        "            out = self.mlp_t(hidden_states)\n",
        "            hidden_states = opt_rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "            hidden_states = hidden_states.transpose(1, 2)\n",
        "        else:\n",
        "            hidden_states = opt_rms_norm(\n",
        "                hidden_states + self.self_attn(cos_sin=cos_sin, hidden_states=hidden_states),\n",
        "                variance_epsilon=self.norm_eps\n",
        "            )\n",
        "        \n",
        "        out = self.mlp(hidden_states)\n",
        "        hidden_states = opt_rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# ============ Optimized TRM Reasoning Module ============\n",
        "class OptimizedTRMReasoningModule(nn.Module):\n",
        "    \"\"\"Stack of TRM blocks for reasoning.\"\"\"\n",
        "    \n",
        "    def __init__(self, layers: List[OptimizedTRMBlock]):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_injection: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "        hidden_states = hidden_states + input_injection\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states=hidden_states, **kwargs)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# ============ Optimized TRM Inner ============\n",
        "class OptimizedTRMInner(nn.Module):\n",
        "    \"\"\"Inner TRM model with dropout support.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: OptimizedTRMConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.forward_dtype = getattr(torch, self.config.forward_dtype)\n",
        "        \n",
        "        # I/O\n",
        "        self.embed_scale = math.sqrt(self.config.hidden_size)\n",
        "        embed_init_std = 1.0 / self.embed_scale\n",
        "        \n",
        "        self.embed_tokens = OptimizedCastedEmbedding(\n",
        "            self.config.vocab_size, self.config.hidden_size,\n",
        "            init_std=embed_init_std, cast_to=self.forward_dtype\n",
        "        )\n",
        "        self.lm_head = OptimizedCastedLinear(self.config.hidden_size, self.config.vocab_size, bias=False)\n",
        "        self.q_head = OptimizedCastedLinear(self.config.hidden_size, 2, bias=True)\n",
        "        \n",
        "        # Puzzle embeddings\n",
        "        self.puzzle_emb_len = -(self.config.puzzle_emb_ndim // -self.config.hidden_size) if self.config.puzzle_emb_len == 0 else self.config.puzzle_emb_len\n",
        "        if self.config.puzzle_emb_ndim > 0:\n",
        "            self.puzzle_emb = OptimizedCastedSparseEmbedding(\n",
        "                self.config.num_puzzle_identifiers, self.config.puzzle_emb_ndim,\n",
        "                batch_size=self.config.batch_size, init_std=0, cast_to=self.forward_dtype\n",
        "            )\n",
        "        \n",
        "        # Position encodings\n",
        "        if self.config.pos_encodings == \"rope\":\n",
        "            self.rotary_emb = OptimizedRotaryEmbedding(\n",
        "                dim=self.config.hidden_size // self.config.num_heads,\n",
        "                max_position_embeddings=self.config.seq_len + self.puzzle_emb_len,\n",
        "                base=self.config.rope_theta\n",
        "            )\n",
        "        elif self.config.pos_encodings == \"learned\":\n",
        "            self.embed_pos = OptimizedCastedEmbedding(\n",
        "                self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size,\n",
        "                init_std=embed_init_std, cast_to=self.forward_dtype\n",
        "            )\n",
        "        \n",
        "        # Reasoning Layers\n",
        "        self.L_level = OptimizedTRMReasoningModule(\n",
        "            layers=[OptimizedTRMBlock(self.config) for _ in range(self.config.L_layers)]\n",
        "        )\n",
        "        \n",
        "        # Initial states\n",
        "        self.H_init = nn.Buffer(\n",
        "            trunc_normal_init_(torch.empty(self.config.hidden_size, dtype=self.forward_dtype), std=1),\n",
        "            persistent=True\n",
        "        )\n",
        "        self.L_init = nn.Buffer(\n",
        "            trunc_normal_init_(torch.empty(self.config.hidden_size, dtype=self.forward_dtype), std=1),\n",
        "            persistent=True\n",
        "        )\n",
        "        \n",
        "        # Q head special init\n",
        "        with torch.no_grad():\n",
        "            self.q_head.weight.zero_()\n",
        "            self.q_head.bias.fill_(-5)\n",
        "\n",
        "    def _input_embeddings(self, input: torch.Tensor, puzzle_identifiers: torch.Tensor):\n",
        "        \"\"\"Compute input embeddings with dropout support.\"\"\"\n",
        "        # Token embedding\n",
        "        embedding = self.embed_tokens(input.to(torch.int32))\n",
        "        \n",
        "        # NEW: Grid token dropout\n",
        "        if self.training and self.config.grid_token_dropout > 0:\n",
        "            embedding = F.dropout(embedding, p=self.config.grid_token_dropout, training=True)\n",
        "        \n",
        "        # Puzzle embeddings\n",
        "        if self.config.puzzle_emb_ndim > 0:\n",
        "            puzzle_embedding = self.puzzle_emb(puzzle_identifiers)\n",
        "            \n",
        "            # NEW: Puzzle embedding dropout\n",
        "            if self.training and self.config.puzzle_emb_dropout > 0:\n",
        "                keep_prob = 1.0 - self.config.puzzle_emb_dropout\n",
        "                keep_mask = (torch.rand(puzzle_embedding.size(0), device=puzzle_embedding.device) >= self.config.puzzle_emb_dropout).to(puzzle_embedding.dtype)\n",
        "                puzzle_embedding = puzzle_embedding * keep_mask.unsqueeze(-1)\n",
        "                puzzle_embedding = puzzle_embedding * (1.0 / keep_prob)  # Scale up to maintain expected value\n",
        "            \n",
        "            pad_count = self.puzzle_emb_len * self.config.hidden_size - puzzle_embedding.shape[-1]\n",
        "            if pad_count > 0:\n",
        "                puzzle_embedding = F.pad(puzzle_embedding, (0, pad_count))\n",
        "            \n",
        "            embedding = torch.cat(\n",
        "                (puzzle_embedding.view(-1, self.puzzle_emb_len, self.config.hidden_size), embedding),\n",
        "                dim=-2\n",
        "            )\n",
        "        \n",
        "        # Position embeddings\n",
        "        if self.config.pos_encodings == \"learned\":\n",
        "            embedding = 0.707106781 * (embedding + self.embed_pos.embedding_weight.to(self.forward_dtype))\n",
        "        \n",
        "        return self.embed_scale * embedding\n",
        "\n",
        "    def empty_carry(self, batch_size: int, device: Optional[torch.device] = None):\n",
        "        \"\"\"Create empty carry state.\"\"\"\n",
        "        if device is None:\n",
        "            device = next(self.parameters()).device\n",
        "        return OptimizedTRMInnerCarry(\n",
        "            z_H=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype, device=device),\n",
        "            z_L=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype, device=device),\n",
        "        )\n",
        "\n",
        "    def reset_carry(self, reset_flag: torch.Tensor, carry: OptimizedTRMInnerCarry):\n",
        "        \"\"\"Reset carry state for halted sequences.\"\"\"\n",
        "        device = carry.z_H.device\n",
        "        H_init = self.H_init.to(device)\n",
        "        L_init = self.L_init.to(device)\n",
        "        return OptimizedTRMInnerCarry(\n",
        "            z_H=torch.where(reset_flag.view(-1, 1, 1), H_init, carry.z_H),\n",
        "            z_L=torch.where(reset_flag.view(-1, 1, 1), L_init, carry.z_L),\n",
        "        )\n",
        "\n",
        "    def forward(self, carry: OptimizedTRMInnerCarry, batch: Dict[str, torch.Tensor]) -> Tuple[OptimizedTRMInnerCarry, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        seq_info = dict(\n",
        "            cos_sin=self.rotary_emb() if hasattr(self, \"rotary_emb\") else None,\n",
        "        )\n",
        "        \n",
        "        # Input encoding\n",
        "        input_embeddings = self._input_embeddings(batch[\"inputs\"], batch[\"puzzle_identifiers\"])\n",
        "        \n",
        "        # Forward iterations\n",
        "        z_H, z_L = carry.z_H, carry.z_L\n",
        "        \n",
        "        # H_cycles-1 without grad\n",
        "        with torch.no_grad():\n",
        "            for _H_step in range(self.config.H_cycles - 1):\n",
        "                for _L_step in range(self.config.L_cycles):\n",
        "                    z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "                z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "        \n",
        "        # Last cycle with grad\n",
        "        for _L_step in range(self.config.L_cycles):\n",
        "            z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "        z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "        \n",
        "        # LM Outputs\n",
        "        new_carry = OptimizedTRMInnerCarry(z_H=z_H.detach(), z_L=z_L.detach())\n",
        "        output = self.lm_head(z_H)[:, self.puzzle_emb_len:]\n",
        "        q_logits = self.q_head(z_H[:, 0]).to(torch.float32)\n",
        "        return new_carry, output, (q_logits[..., 0], q_logits[..., 1])\n",
        "\n",
        "\n",
        "# ============ Optimized TRM with ACT Wrapper ============\n",
        "class OptimizedTRM(nn.Module):\n",
        "    \"\"\"Optimized TinyRecursiveReasoningModel with ACT and all enhancements.\n",
        "    \n",
        "    Key features:\n",
        "    - LoRA support (enabled via config)\n",
        "    - Dropout regularization\n",
        "    - halt_max_steps_eval for different eval behavior\n",
        "    - no_ACT_continue option\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config_dict: dict):\n",
        "        super().__init__()\n",
        "        self.config = OptimizedTRMConfig(**config_dict)\n",
        "        self.inner = OptimizedTRMInner(self.config)\n",
        "        \n",
        "        # Enable LoRA if configured\n",
        "        if self.config.lora_rank > 0:\n",
        "            enable_lora_for_linears(\n",
        "                self,\n",
        "                rank=self.config.lora_rank,\n",
        "                alpha=self.config.lora_alpha,\n",
        "                dropout=self.config.lora_dropout,\n",
        "                train_base=self.config.lora_train_base,\n",
        "                train_bias=self.config.lora_train_bias,\n",
        "            )\n",
        "\n",
        "    @property\n",
        "    def puzzle_emb(self):\n",
        "        return self.inner.puzzle_emb\n",
        "\n",
        "    def initial_carry(self, batch: Dict[str, torch.Tensor]):\n",
        "        \"\"\"Create initial carry state for a batch.\"\"\"\n",
        "        batch_size = batch[\"inputs\"].shape[0]\n",
        "        device = batch[\"inputs\"].device\n",
        "        \n",
        "        return OptimizedTRMCarry(\n",
        "            inner_carry=self.inner.empty_carry(batch_size, device=device),\n",
        "            steps=torch.zeros((batch_size,), dtype=torch.int32, device=device),\n",
        "            halted=torch.ones((batch_size,), dtype=torch.bool, device=device),\n",
        "            current_data={k: torch.empty_like(v) for k, v in batch.items()}\n",
        "        )\n",
        "\n",
        "    def forward(self, carry: OptimizedTRMCarry, batch: Dict[str, torch.Tensor]) -> Tuple[OptimizedTRMCarry, Dict[str, torch.Tensor]]:\n",
        "        # Update data, carry (removing halted sequences)\n",
        "        new_inner_carry = self.inner.reset_carry(carry.halted, carry.inner_carry)\n",
        "        new_steps = torch.where(carry.halted, 0, carry.steps)\n",
        "        new_current_data = {\n",
        "            k: torch.where(carry.halted.view((-1,) + (1,) * (batch[k].ndim - 1)), batch[k], v)\n",
        "            for k, v in carry.current_data.items()\n",
        "        }\n",
        "        \n",
        "        # Forward inner model\n",
        "        new_inner_carry, logits, (q_halt_logits, q_continue_logits) = self.inner(new_inner_carry, new_current_data)\n",
        "        \n",
        "        outputs = {\n",
        "            \"logits\": logits,\n",
        "            \"q_halt_logits\": q_halt_logits,\n",
        "            \"q_continue_logits\": q_continue_logits\n",
        "        }\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Step\n",
        "            new_steps = new_steps + 1\n",
        "            \n",
        "            # NEW: Use halt_max_steps_eval during evaluation\n",
        "            halt_limit = self.config.halt_max_steps if self.training or (self.config.halt_max_steps_eval is None) else self.config.halt_max_steps_eval\n",
        "            is_last_step = new_steps >= halt_limit\n",
        "            \n",
        "            halted = is_last_step\n",
        "            \n",
        "            # ACT halting logic (only during training)\n",
        "            if self.training and (self.config.halt_max_steps > 1):\n",
        "                # Halt signal based on no_ACT_continue config\n",
        "                if self.config.no_ACT_continue:\n",
        "                    # NEW: Only use sigmoid of halt\n",
        "                    halted = halted | (q_halt_logits > 0)\n",
        "                else:\n",
        "                    # Original: compare halt vs continue\n",
        "                    halted = halted | (q_halt_logits > q_continue_logits)\n",
        "                \n",
        "                # Exploration\n",
        "                min_halt_steps = (torch.rand_like(q_halt_logits) < self.config.halt_exploration_prob) * torch.randint_like(new_steps, low=2, high=self.config.halt_max_steps + 1)\n",
        "                halted = halted & (new_steps >= min_halt_steps)\n",
        "                \n",
        "                if not self.config.no_ACT_continue:\n",
        "                    # Compute target Q for Q-learning\n",
        "                    _, _, (next_q_halt_logits, next_q_continue_logits) = self.inner(new_inner_carry, new_current_data)\n",
        "                    outputs[\"target_q_continue\"] = torch.sigmoid(\n",
        "                        torch.where(is_last_step, next_q_halt_logits, torch.maximum(next_q_halt_logits, next_q_continue_logits))\n",
        "                    )\n",
        "        \n",
        "        return OptimizedTRMCarry(new_inner_carry, new_steps, halted, new_current_data), outputs\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Optimized TRM Model defined!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“‹ Model components:\")\n",
        "print(\"  - OptimizedTRMConfig: Configuration with LoRA, dropout, halt_max_steps_eval\")\n",
        "print(\"  - OptimizedTRMBlock: Transformer block using OptimizedCastedLinear\")\n",
        "print(\"  - OptimizedTRMReasoningModule: Stack of optimized blocks\")\n",
        "print(\"  - OptimizedTRMInner: Core model with dropout support\")\n",
        "print(\"  - OptimizedTRM: Full model with ACT wrapper and LoRA injection\")\n",
        "print(\"\\nğŸ”§ New features:\")\n",
        "print(\"  - LoRA: Set lora_rank > 0 in config to enable\")\n",
        "print(\"  - Dropout: Use puzzle_emb_dropout and grid_token_dropout\")\n",
        "print(\"  - halt_max_steps_eval: Different max steps during evaluation\")\n",
        "print(\"  - no_ACT_continue: Use sigmoid(halt) instead of comparing halt vs continue\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 20: Optimized Loss Head\n",
        "\n",
        "Same ACT loss head but works with OptimizedTRM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Optimized ACT Loss Head defined!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 20: Optimized Loss Head\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Loss functions (same as original)\n",
        "def opt_s(x, epsilon=1e-30):\n",
        "    \"\"\"Stablemax helper function.\"\"\"\n",
        "    return torch.where(x < 0, 1 / (1 - x + epsilon), x + 1)\n",
        "\n",
        "\n",
        "def opt_log_stablemax(x, dim=-1):\n",
        "    \"\"\"Log stablemax function.\"\"\"\n",
        "    s_x = opt_s(x)\n",
        "    return torch.log(s_x / torch.sum(s_x, dim=dim, keepdim=True))\n",
        "\n",
        "\n",
        "def opt_stablemax_cross_entropy(logits, labels, ignore_index: int = -100, valid_mask=None):\n",
        "    \"\"\"Stablemax cross-entropy loss.\"\"\"\n",
        "    logprobs = opt_log_stablemax(logits.to(torch.float64), dim=-1)\n",
        "    \n",
        "    if valid_mask is None:\n",
        "        valid_mask = (labels != ignore_index)\n",
        "    transformed_labels = torch.where(valid_mask, labels, 0)\n",
        "    prediction_logprobs = torch.gather(logprobs, index=transformed_labels.to(torch.long).unsqueeze(-1), dim=-1).squeeze(-1)\n",
        "    \n",
        "    return -torch.where(valid_mask, prediction_logprobs, 0)\n",
        "\n",
        "\n",
        "def opt_softmax_cross_entropy(logits, labels, ignore_index: int = -100, valid_mask=None):\n",
        "    \"\"\"Standard softmax cross-entropy loss.\"\"\"\n",
        "    return F.cross_entropy(\n",
        "        logits.to(torch.float32).view(-1, logits.shape[-1]),\n",
        "        labels.to(torch.long).view(-1),\n",
        "        ignore_index=ignore_index,\n",
        "        reduction=\"none\"\n",
        "    ).view(labels.shape)\n",
        "\n",
        "\n",
        "# Map loss type strings to functions\n",
        "OPT_LOSS_FUNCTIONS = {\n",
        "    \"stablemax_cross_entropy\": opt_stablemax_cross_entropy,\n",
        "    \"softmax_cross_entropy\": opt_softmax_cross_entropy,\n",
        "}\n",
        "\n",
        "\n",
        "class OptimizedACTLossHead(nn.Module):\n",
        "    \"\"\"ACT Loss Head for OptimizedTRM.\n",
        "    \n",
        "    Same logic as original but works with OptimizedTRM types.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model: nn.Module, loss_type: str):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.loss_fn = OPT_LOSS_FUNCTIONS[loss_type]\n",
        "\n",
        "    def initial_carry(self, *args, **kwargs):\n",
        "        return self.model.initial_carry(*args, **kwargs)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        return_keys: Sequence[str],\n",
        "        **model_kwargs,\n",
        "    ) -> Tuple[Any, torch.Tensor, Dict[str, torch.Tensor], Optional[Dict[str, torch.Tensor]], torch.Tensor]:\n",
        "        # Forward model\n",
        "        new_carry, outputs = self.model(**model_kwargs)\n",
        "        labels = new_carry.current_data[\"labels\"]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Predictions\n",
        "            outputs[\"preds\"] = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "            \n",
        "            # Correctness\n",
        "            mask = (labels != IGNORE_LABEL_ID)\n",
        "            loss_counts = mask.sum(-1)\n",
        "            loss_divisor = loss_counts.clamp_min(1).unsqueeze(-1)\n",
        "            \n",
        "            is_correct = mask & (torch.argmax(outputs[\"logits\"], dim=-1) == labels)\n",
        "            seq_is_correct = is_correct.sum(-1) == loss_counts\n",
        "            \n",
        "            # Metrics (only for halted sequences)\n",
        "            valid_metrics = new_carry.halted & (loss_counts > 0)\n",
        "            metrics = {\n",
        "                \"count\": valid_metrics.sum(),\n",
        "                \"accuracy\": torch.where(valid_metrics, (is_correct.to(torch.float32) / loss_divisor).sum(-1), 0).sum(),\n",
        "                \"exact_accuracy\": (valid_metrics & seq_is_correct).sum(),\n",
        "                \"q_halt_accuracy\": (valid_metrics & ((outputs[\"q_halt_logits\"] >= 0) == seq_is_correct)).sum(),\n",
        "                \"steps\": torch.where(valid_metrics, new_carry.steps, 0).sum(),\n",
        "            }\n",
        "        \n",
        "        # Losses\n",
        "        lm_loss = (self.loss_fn(outputs[\"logits\"], labels, ignore_index=IGNORE_LABEL_ID, valid_mask=mask) / loss_divisor).sum()\n",
        "        q_halt_loss = F.binary_cross_entropy_with_logits(\n",
        "            outputs[\"q_halt_logits\"], seq_is_correct.to(outputs[\"q_halt_logits\"].dtype), reduction=\"sum\"\n",
        "        )\n",
        "        \n",
        "        metrics.update({\n",
        "            \"lm_loss\": lm_loss.detach(),\n",
        "            \"q_halt_loss\": q_halt_loss.detach(),\n",
        "        })\n",
        "        \n",
        "        # Q continue loss (for Q-learning)\n",
        "        q_continue_loss = 0\n",
        "        if \"target_q_continue\" in outputs:\n",
        "            q_continue_loss = F.binary_cross_entropy_with_logits(\n",
        "                outputs[\"q_continue_logits\"], outputs[\"target_q_continue\"], reduction=\"sum\"\n",
        "            )\n",
        "            metrics[\"q_continue_loss\"] = q_continue_loss.detach()\n",
        "        \n",
        "        # Filter outputs for return\n",
        "        detached_outputs = {k: outputs[k].detach() for k in return_keys if k in outputs}\n",
        "        \n",
        "        return new_carry, lm_loss + 0.5 * (q_halt_loss + q_continue_loss), metrics, detached_outputs, new_carry.halted.all()\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Optimized ACT Loss Head defined!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 21: Test Optimized TRM Model\n",
        "\n",
        "Test the optimized TRM model:\n",
        "1. Create model with default config (no LoRA, no dropout)\n",
        "2. Create model with LoRA enabled\n",
        "3. Create model with dropout enabled\n",
        "4. Verify forward pass works for all configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ§ª Testing Optimized TRM Model\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ Base configuration:\n",
            "  halt_max_steps: 8\n",
            "  halt_max_steps_eval: 12\n",
            "  no_ACT_continue: True\n",
            "  puzzle_emb_dropout: 0.0\n",
            "  grid_token_dropout: 0.0\n",
            "  lora_rank: 0\n",
            "\n",
            "======================================================================\n",
            "ğŸ”§ Test 1: Base Optimized Model (no LoRA, no dropout)\n",
            "======================================================================\n",
            "  Total parameters: 5,317,634\n",
            "  Trainable parameters: 5,317,634\n",
            "  âœ… Forward pass successful!\n",
            "  Loss: 83.2134\n",
            "  Accuracy: 0.0000\n",
            "\n",
            "======================================================================\n",
            "ğŸ”§ Test 2: Optimized Model with LoRA (rank=16)\n",
            "======================================================================\n",
            "  Total parameters: 5,625,042\n",
            "  Trainable parameters: 311,632\n",
            "  LoRA parameters: 307,408\n",
            "  LoRA % of total: 5.46%\n",
            "  LoRA-enabled layers: 14\n",
            "  âœ… Forward pass successful!\n",
            "  Loss: 76.2621\n",
            "\n",
            "======================================================================\n",
            "ğŸ”§ Test 3: Optimized Model with Dropout\n",
            "======================================================================\n",
            "  puzzle_emb_dropout: 0.1\n",
            "  grid_token_dropout: 0.05\n",
            "  âœ… Forward pass (training mode) successful!\n",
            "  Loss: 80.4001\n",
            "  âœ… Forward pass (eval mode) successful!\n",
            "  Loss: 80.4927\n",
            "\n",
            "======================================================================\n",
            "ğŸ”§ Test 4: EMA Helper\n",
            "======================================================================\n",
            "  Registered 16 parameters for EMA tracking\n",
            "  âœ… EMA update successful!\n",
            "  âœ… EMA copy created for evaluation\n",
            "\n",
            "======================================================================\n",
            "ğŸ”§ Test 5: halt_max_steps_eval behavior\n",
            "======================================================================\n",
            "  halt_max_steps (training): 8\n",
            "  halt_max_steps_eval: 12\n",
            "  Training mode completed in 8 steps (max: 8)\n",
            "  Eval mode completed in 12 steps (max: 12)\n",
            "\n",
            "======================================================================\n",
            "âœ… Part 3 Complete! All Optimized TRM tests passed.\n",
            "======================================================================\n",
            "\n",
            "ğŸ“Š Summary of optimizations:\n",
            "  1. LoRA: Efficient fine-tuning with low-rank matrices\n",
            "  2. Dropout: puzzle_emb_dropout and grid_token_dropout for regularization\n",
            "  3. EMA: Exponential moving average for stable evaluation\n",
            "  4. halt_max_steps_eval: Different max steps during evaluation\n",
            "  5. no_ACT_continue: Simpler halting using sigmoid(halt)\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 21: Test Optimized TRM Model\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ§ª Testing Optimized TRM Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============ Configuration ============\n",
        "BATCH_SIZE = 32\n",
        "SEQ_LEN = 81  # 9x9 Sudoku\n",
        "\n",
        "# Base config (same as original)\n",
        "base_optimized_config = {\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"seq_len\": SEQ_LEN,\n",
        "    \"puzzle_emb_ndim\": 4096,\n",
        "    \"num_puzzle_identifiers\": train_metadata.num_puzzle_identifiers,\n",
        "    \"vocab_size\": train_metadata.vocab_size,\n",
        "    \n",
        "    \"H_cycles\": 3,\n",
        "    \"L_cycles\": 3,\n",
        "    \"H_layers\": 1,\n",
        "    \"L_layers\": 3,\n",
        "    \n",
        "    \"hidden_size\": 384,\n",
        "    \"expansion\": 4.0,\n",
        "    \"num_heads\": 6,\n",
        "    \"pos_encodings\": \"rope\",\n",
        "    \n",
        "    \"rms_norm_eps\": 1e-5,\n",
        "    \"rope_theta\": 10000.0,\n",
        "    \n",
        "    \"halt_max_steps\": 8,\n",
        "    \"halt_exploration_prob\": 0.5,\n",
        "    \"halt_max_steps_eval\": 12,  # NEW: Different max steps for eval\n",
        "    \n",
        "    \"forward_dtype\": \"bfloat16\",\n",
        "    \n",
        "    # NEW: Dropout (disabled by default)\n",
        "    \"puzzle_emb_dropout\": 0.0,\n",
        "    \"grid_token_dropout\": 0.0,\n",
        "    \n",
        "    \"mlp_t\": False,\n",
        "    \"puzzle_emb_len\": 16,\n",
        "    \"no_ACT_continue\": True,\n",
        "    \n",
        "    # NEW: LoRA (disabled by default)\n",
        "    \"lora_rank\": 0,\n",
        "    \"lora_alpha\": 1.0,\n",
        "    \"lora_dropout\": 0.0,\n",
        "    \"lora_train_base\": False,\n",
        "    \"lora_train_bias\": False,\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ“‹ Base configuration:\")\n",
        "for key in [\"halt_max_steps\", \"halt_max_steps_eval\", \"no_ACT_continue\", \n",
        "            \"puzzle_emb_dropout\", \"grid_token_dropout\", \"lora_rank\"]:\n",
        "    print(f\"  {key}: {base_optimized_config[key]}\")\n",
        "\n",
        "# ============ Test 1: Base Model (no optimizations) ============\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ”§ Test 1: Base Optimized Model (no LoRA, no dropout)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "optimized_model_base = OptimizedTRM(base_optimized_config)\n",
        "optimized_model_base = optimized_model_base.to(DEVICE)\n",
        "\n",
        "total_params = sum(p.numel() for p in optimized_model_base.parameters())\n",
        "trainable_params = sum(p.numel() for p in optimized_model_base.parameters() if p.requires_grad)\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Test forward pass\n",
        "optimized_model_base.train()\n",
        "optimized_loss_head_base = OptimizedACTLossHead(optimized_model_base, loss_type=\"stablemax_cross_entropy\")\n",
        "carry = optimized_loss_head_base.initial_carry(batch=test_batch)\n",
        "new_carry, loss, metrics, outputs, all_halted = optimized_loss_head_base(\n",
        "    return_keys=[\"logits\", \"preds\"],\n",
        "    carry=carry,\n",
        "    batch=test_batch\n",
        ")\n",
        "print(f\"  âœ… Forward pass successful!\")\n",
        "print(f\"  Loss: {loss.item():.4f}\")\n",
        "print(f\"  Accuracy: {metrics['accuracy'].item() / max(metrics['count'].item(), 1):.4f}\")\n",
        "\n",
        "# ============ Test 2: Model with LoRA ============\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ”§ Test 2: Optimized Model with LoRA (rank=16)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "lora_config = base_optimized_config.copy()\n",
        "lora_config[\"lora_rank\"] = 16\n",
        "lora_config[\"lora_alpha\"] = 32.0\n",
        "lora_config[\"lora_dropout\"] = 0.1\n",
        "lora_config[\"lora_train_base\"] = False  # Freeze base weights\n",
        "\n",
        "optimized_model_lora = OptimizedTRM(lora_config)\n",
        "optimized_model_lora = optimized_model_lora.to(DEVICE)\n",
        "\n",
        "total_params_lora = sum(p.numel() for p in optimized_model_lora.parameters())\n",
        "trainable_params_lora = sum(p.numel() for p in optimized_model_lora.parameters() if p.requires_grad)\n",
        "lora_params = sum(p.numel() for name, p in optimized_model_lora.named_parameters() if 'lora' in name.lower())\n",
        "\n",
        "print(f\"  Total parameters: {total_params_lora:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params_lora:,}\")\n",
        "print(f\"  LoRA parameters: {lora_params:,}\")\n",
        "print(f\"  LoRA % of total: {100 * lora_params / total_params_lora:.2f}%\")\n",
        "\n",
        "# Count LoRA-enabled layers\n",
        "lora_layers = sum(1 for m in optimized_model_lora.modules() if isinstance(m, OptimizedCastedLinear) and m.lora_rank > 0)\n",
        "print(f\"  LoRA-enabled layers: {lora_layers}\")\n",
        "\n",
        "# Test forward pass\n",
        "optimized_model_lora.train()\n",
        "optimized_loss_head_lora = OptimizedACTLossHead(optimized_model_lora, loss_type=\"stablemax_cross_entropy\")\n",
        "carry = optimized_loss_head_lora.initial_carry(batch=test_batch)\n",
        "new_carry, loss, metrics, outputs, all_halted = optimized_loss_head_lora(\n",
        "    return_keys=[\"logits\", \"preds\"],\n",
        "    carry=carry,\n",
        "    batch=test_batch\n",
        ")\n",
        "print(f\"  âœ… Forward pass successful!\")\n",
        "print(f\"  Loss: {loss.item():.4f}\")\n",
        "\n",
        "# ============ Test 3: Model with Dropout ============\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ”§ Test 3: Optimized Model with Dropout\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "dropout_config = base_optimized_config.copy()\n",
        "dropout_config[\"puzzle_emb_dropout\"] = 0.1\n",
        "dropout_config[\"grid_token_dropout\"] = 0.05\n",
        "\n",
        "optimized_model_dropout = OptimizedTRM(dropout_config)\n",
        "optimized_model_dropout = optimized_model_dropout.to(DEVICE)\n",
        "\n",
        "print(f\"  puzzle_emb_dropout: {dropout_config['puzzle_emb_dropout']}\")\n",
        "print(f\"  grid_token_dropout: {dropout_config['grid_token_dropout']}\")\n",
        "\n",
        "# Test forward pass (training mode)\n",
        "optimized_model_dropout.train()\n",
        "optimized_loss_head_dropout = OptimizedACTLossHead(optimized_model_dropout, loss_type=\"stablemax_cross_entropy\")\n",
        "carry = optimized_loss_head_dropout.initial_carry(batch=test_batch)\n",
        "new_carry, loss, metrics, outputs, all_halted = optimized_loss_head_dropout(\n",
        "    return_keys=[\"logits\", \"preds\"],\n",
        "    carry=carry,\n",
        "    batch=test_batch\n",
        ")\n",
        "print(f\"  âœ… Forward pass (training mode) successful!\")\n",
        "print(f\"  Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Test forward pass (eval mode - no dropout)\n",
        "optimized_model_dropout.eval()\n",
        "carry = optimized_loss_head_dropout.initial_carry(batch=test_batch)\n",
        "with torch.no_grad():\n",
        "    new_carry, loss, metrics, outputs, all_halted = optimized_loss_head_dropout(\n",
        "        return_keys=[\"logits\", \"preds\"],\n",
        "        carry=carry,\n",
        "        batch=test_batch\n",
        "    )\n",
        "print(f\"  âœ… Forward pass (eval mode) successful!\")\n",
        "print(f\"  Loss: {loss.item():.4f}\")\n",
        "\n",
        "# ============ Test 4: EMA Helper ============\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ”§ Test 4: EMA Helper\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "ema = EMAHelper(mu=0.999)\n",
        "ema.register(optimized_model_base)\n",
        "print(f\"  Registered {len(ema.shadow)} parameters for EMA tracking\")\n",
        "\n",
        "# Simulate a training step\n",
        "optimized_model_base.train()\n",
        "carry = optimized_loss_head_base.initial_carry(batch=test_batch)\n",
        "new_carry, loss, metrics, outputs, all_halted = optimized_loss_head_base(\n",
        "    return_keys=[],\n",
        "    carry=carry,\n",
        "    batch=test_batch\n",
        ")\n",
        "loss.backward()\n",
        "\n",
        "# Update EMA\n",
        "ema.update(optimized_model_base)\n",
        "print(f\"  âœ… EMA update successful!\")\n",
        "\n",
        "# Create EMA copy for evaluation\n",
        "ema_model = ema.ema_copy(optimized_model_base)\n",
        "print(f\"  âœ… EMA copy created for evaluation\")\n",
        "\n",
        "# ============ Test 5: halt_max_steps_eval ============\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ”§ Test 5: halt_max_steps_eval behavior\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"  halt_max_steps (training): {base_optimized_config['halt_max_steps']}\")\n",
        "print(f\"  halt_max_steps_eval: {base_optimized_config['halt_max_steps_eval']}\")\n",
        "\n",
        "# Training mode - uses halt_max_steps\n",
        "optimized_model_base.train()\n",
        "carry = optimized_loss_head_base.initial_carry(batch=test_batch)\n",
        "step = 0\n",
        "while not all_halted and step < 20:\n",
        "    new_carry, loss, metrics, outputs, all_halted = optimized_loss_head_base(\n",
        "        return_keys=[],\n",
        "        carry=carry,\n",
        "        batch=test_batch\n",
        "    )\n",
        "    carry = new_carry\n",
        "    step += 1\n",
        "print(f\"  Training mode completed in {step} steps (max: {base_optimized_config['halt_max_steps']})\")\n",
        "\n",
        "# Eval mode - uses halt_max_steps_eval\n",
        "optimized_model_base.eval()\n",
        "carry = optimized_loss_head_base.initial_carry(batch=test_batch)\n",
        "all_halted = False\n",
        "step = 0\n",
        "with torch.no_grad():\n",
        "    while not all_halted and step < 20:\n",
        "        new_carry, loss, metrics, outputs, all_halted = optimized_loss_head_base(\n",
        "            return_keys=[],\n",
        "            carry=carry,\n",
        "            batch=test_batch\n",
        "        )\n",
        "        carry = new_carry\n",
        "        step += 1\n",
        "print(f\"  Eval mode completed in {step} steps (max: {base_optimized_config['halt_max_steps_eval']})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Part 3 Complete! All Optimized TRM tests passed.\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“Š Summary of optimizations:\")\n",
        "print(\"  1. LoRA: Efficient fine-tuning with low-rank matrices\")\n",
        "print(\"  2. Dropout: puzzle_emb_dropout and grid_token_dropout for regularization\")\n",
        "print(\"  3. EMA: Exponential moving average for stable evaluation\")\n",
        "print(\"  4. halt_max_steps_eval: Different max steps during evaluation\")\n",
        "print(\"  5. no_ACT_continue: Simpler halting using sigmoid(halt)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“¦ Part 4: Training & Evaluation Framework\n",
        "\n",
        "This section implements the training and evaluation framework for comparing Original vs Optimized TRM models.\n",
        "\n",
        "## Components:\n",
        "1. **TRMTrainer**: Unified trainer class for both model types\n",
        "2. **evaluate()**: Evaluation function with metrics logging\n",
        "3. **train_epoch()**: Single epoch training with W&B logging\n",
        "4. **Learning rate scheduler**: Cosine annealing with warmup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 22: Learning Rate Scheduler\n",
        "\n",
        "Cosine annealing with linear warmup - standard for transformer training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Learning Rate Scheduler and Optimizer defined!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 22: Learning Rate Scheduler\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def get_lr_scheduler(optimizer, warmup_steps: int, total_steps: int, min_lr_ratio: float = 0.1):\n",
        "    \"\"\"\n",
        "    Create a learning rate scheduler with linear warmup and cosine annealing.\n",
        "    \n",
        "    Args:\n",
        "        optimizer: PyTorch optimizer\n",
        "        warmup_steps: Number of warmup steps\n",
        "        total_steps: Total training steps\n",
        "        min_lr_ratio: Minimum LR as ratio of initial LR\n",
        "    \n",
        "    Returns:\n",
        "        LambdaLR scheduler\n",
        "    \"\"\"\n",
        "    def lr_lambda(current_step: int) -> float:\n",
        "        if current_step < warmup_steps:\n",
        "            # Linear warmup\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        else:\n",
        "            # Cosine annealing\n",
        "            progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "            return max(min_lr_ratio, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "    \n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "def get_optimizer(model, lr: float, weight_decay: float, betas: Tuple[float, float] = (0.9, 0.95)):\n",
        "    \"\"\"\n",
        "    Create AdamW optimizer with parameter groups.\n",
        "    \n",
        "    Separates:\n",
        "    - Main parameters (with weight decay)\n",
        "    - Bias and LayerNorm parameters (no weight decay)\n",
        "    - LoRA parameters (separate group)\n",
        "    \n",
        "    Filters out:\n",
        "    - Non-leaf tensors (which can't be optimized)\n",
        "    - Buffers (even if requires_grad=True, they're handled separately by SignSGD)\n",
        "    - Non nn.Parameter objects (extra safety)\n",
        "    \"\"\"\n",
        "    # Get all buffer data_ptrs for identity comparison\n",
        "    buffer_data_ptrs = {buf.data_ptr() for _, buf in model.named_buffers()}\n",
        "    \n",
        "    # Separate parameters\n",
        "    decay_params = []\n",
        "    no_decay_params = []\n",
        "    lora_params = []\n",
        "    \n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        \n",
        "        # Skip if this tensor is actually a buffer (check by data_ptr)\n",
        "        if param.data_ptr() in buffer_data_ptrs:\n",
        "            print(f\"  âš ï¸ Skipping buffer masquerading as parameter: {name}\")\n",
        "            continue\n",
        "        \n",
        "        # Skip non-leaf tensors (views, computed tensors, etc.)\n",
        "        if not param.is_leaf:\n",
        "            print(f\"  âš ï¸ Skipping non-leaf parameter: {name}\")\n",
        "            continue\n",
        "        \n",
        "        # Skip if not actually an nn.Parameter (extra safety)\n",
        "        if not isinstance(param, nn.Parameter):\n",
        "            print(f\"  âš ï¸ Skipping non-Parameter tensor: {name}\")\n",
        "            continue\n",
        "        \n",
        "        if 'lora' in name.lower():\n",
        "            lora_params.append(param)\n",
        "        elif 'bias' in name or 'norm' in name.lower() or 'ln' in name.lower():\n",
        "            no_decay_params.append(param)\n",
        "        else:\n",
        "            decay_params.append(param)\n",
        "    \n",
        "    # Validate all parameters are leaf tensors before creating optimizer\n",
        "    all_params = decay_params + no_decay_params + lora_params\n",
        "    for param in all_params:\n",
        "        if not param.is_leaf:\n",
        "            raise ValueError(\n",
        "                f\"Found non-leaf tensor in optimizer parameters. \"\n",
        "                f\"This usually means a parameter is a view or computed tensor. \"\n",
        "                f\"Parameter shape: {param.shape}, requires_grad: {param.requires_grad}\"\n",
        "            )\n",
        "    \n",
        "    param_groups = []\n",
        "    if decay_params:\n",
        "        param_groups.append({'params': decay_params, 'weight_decay': weight_decay})\n",
        "    if no_decay_params:\n",
        "        param_groups.append({'params': no_decay_params, 'weight_decay': 0.0})\n",
        "    if lora_params:\n",
        "        param_groups.append({'params': lora_params, 'weight_decay': 0.0, 'lr': lr})  # LoRA often uses same LR\n",
        "    \n",
        "    try:\n",
        "        optimizer = torch.optim.AdamW(param_groups, lr=lr, betas=betas)\n",
        "    except RuntimeError as e:\n",
        "        if \"non-leaf\" in str(e).lower():\n",
        "            # Print which parameters are problematic\n",
        "            print(\"âŒ Error creating optimizer. Checking parameters...\")\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.requires_grad and not param.is_leaf:\n",
        "                    print(f\"  Non-leaf parameter: {name}, shape: {param.shape}, is_leaf: {param.is_leaf}\")\n",
        "            raise RuntimeError(f\"Optimizer creation failed: {e}\")\n",
        "        raise\n",
        "    \n",
        "    return optimizer\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Learning Rate Scheduler and Optimizer defined!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 23: TRMTrainer Class\n",
        "\n",
        "Unified trainer that works with both Original and Optimized TRM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… TRMTrainer class defined!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ Trainer features:\n",
            "  - Unified training for Original and Optimized TRM\n",
            "  - ACT loop with proper halting\n",
            "  - Sparse embedding optimizer support\n",
            "  - EMA support for evaluation\n",
            "  - W&B logging\n",
            "  - Checkpoint save/load\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 23: TRMTrainer Class\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "@dataclass\n",
        "class TrainerConfig:\n",
        "    \"\"\"Configuration for TRMTrainer.\"\"\"\n",
        "    # Training\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 0.1\n",
        "    warmup_steps: int = 500\n",
        "    max_steps: int = 10000\n",
        "    eval_interval: int = 500\n",
        "    log_interval: int = 50\n",
        "    \n",
        "    # Batch\n",
        "    gradient_accumulation_steps: int = 1\n",
        "    max_grad_norm: float = 1.0\n",
        "    \n",
        "    # EMA\n",
        "    use_ema: bool = False\n",
        "    ema_decay: float = 0.999\n",
        "    \n",
        "    # Sparse embedding\n",
        "    puzzle_emb_lr: float = 1e-2\n",
        "    puzzle_emb_weight_decay: float = 0.1\n",
        "    \n",
        "    # W&B\n",
        "    project_name: str = \"TRM-Comparison\"\n",
        "    run_name: Optional[str] = None\n",
        "    \n",
        "    # Save\n",
        "    save_dir: str = \"checkpoints\"\n",
        "    save_best: bool = True\n",
        "\n",
        "\n",
        "class TRMTrainer:\n",
        "    \"\"\"\n",
        "    Unified trainer for TinyRecursiveModels.\n",
        "    \n",
        "    Works with both Original and Optimized TRM models.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        loss_head: nn.Module,\n",
        "        train_dataloader: DataLoader,\n",
        "        eval_dataloader: DataLoader,\n",
        "        config: TrainerConfig,\n",
        "        device: torch.device,\n",
        "        model_type: str = \"original\",  # \"original\" or \"optimized\"\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.loss_head = loss_head\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.eval_dataloader = eval_dataloader\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.model_type = model_type\n",
        "        \n",
        "        # Optimizer\n",
        "        self.optimizer = get_optimizer(\n",
        "            model, \n",
        "            lr=config.lr, \n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "        \n",
        "        # Scheduler\n",
        "        self.scheduler = get_lr_scheduler(\n",
        "            self.optimizer,\n",
        "            warmup_steps=config.warmup_steps,\n",
        "            total_steps=config.max_steps,\n",
        "            min_lr_ratio=0.1\n",
        "        )\n",
        "        \n",
        "        # Sparse embedding optimizer (if applicable)\n",
        "        # NOTE: Use puzzle_emb.buffers() as in original pretrain.py\n",
        "        self.sparse_emb_optimizer = None\n",
        "        if hasattr(model, 'puzzle_emb'):\n",
        "            puzzle_emb = model.puzzle_emb\n",
        "            if hasattr(puzzle_emb, 'weights'):\n",
        "                # Create sparse embedding optimizer using buffers() method\n",
        "                # This matches the original implementation in pretrain.py\n",
        "                sparse_buffers = list(puzzle_emb.buffers())\n",
        "                if self.model_type == \"original\":\n",
        "                    self.sparse_emb_optimizer = CastedSparseEmbeddingSignSGD(\n",
        "                        sparse_buffers,\n",
        "                        lr=config.puzzle_emb_lr,\n",
        "                        weight_decay=config.puzzle_emb_weight_decay\n",
        "                    )\n",
        "                else:\n",
        "                    self.sparse_emb_optimizer = OptimizedCastedSparseEmbeddingSignSGD(\n",
        "                        sparse_buffers,\n",
        "                        lr=config.puzzle_emb_lr,\n",
        "                        weight_decay=config.puzzle_emb_weight_decay\n",
        "                    )\n",
        "        \n",
        "        # EMA\n",
        "        self.ema = None\n",
        "        if config.use_ema:\n",
        "            self.ema = EMAHelper(mu=config.ema_decay)\n",
        "            self.ema.register(model)\n",
        "        \n",
        "        # State\n",
        "        self.global_step = 0\n",
        "        self.best_eval_accuracy = 0.0\n",
        "        \n",
        "        # Metrics accumulator\n",
        "        self.train_metrics = {}\n",
        "        \n",
        "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
        "        \"\"\"Execute a single training step.\"\"\"\n",
        "        self.model.train()\n",
        "        \n",
        "        # Move batch to device\n",
        "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "        \n",
        "        # Initialize carry\n",
        "        carry = self.loss_head.initial_carry(batch=batch)\n",
        "        \n",
        "        # ACT loop\n",
        "        total_loss = 0.0\n",
        "        accumulated_metrics = {}\n",
        "        step_count = 0\n",
        "        all_halted = False\n",
        "        \n",
        "        while not all_halted and step_count < 50:  # Safety limit\n",
        "            new_carry, loss, metrics, outputs, all_halted = self.loss_head(\n",
        "                return_keys=[],\n",
        "                carry=carry,\n",
        "                batch=batch\n",
        "            )\n",
        "            carry = new_carry\n",
        "            total_loss = total_loss + loss\n",
        "            step_count += 1\n",
        "            \n",
        "            # Accumulate metrics\n",
        "            for k, v in metrics.items():\n",
        "                if k not in accumulated_metrics:\n",
        "                    accumulated_metrics[k] = 0.0\n",
        "                accumulated_metrics[k] += v.item() if torch.is_tensor(v) else v\n",
        "        \n",
        "        # Average loss over ACT steps\n",
        "        avg_loss = total_loss / max(step_count, 1)\n",
        "        \n",
        "        # Backward\n",
        "        avg_loss.backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        if self.config.max_grad_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
        "        \n",
        "        # Optimizer step\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # Sparse embedding update\n",
        "        if self.sparse_emb_optimizer is not None:\n",
        "            self.sparse_emb_optimizer.step()\n",
        "            self.sparse_emb_optimizer.zero_grad()\n",
        "        \n",
        "        # EMA update\n",
        "        if self.ema is not None:\n",
        "            self.ema.update(self.model)\n",
        "        \n",
        "        # Compute metrics\n",
        "        count = accumulated_metrics.get('count', 1)\n",
        "        result_metrics = {\n",
        "            'loss': avg_loss.item(),\n",
        "            'lm_loss': accumulated_metrics.get('lm_loss', 0) / max(count, 1),\n",
        "            'q_halt_loss': accumulated_metrics.get('q_halt_loss', 0) / max(count, 1),\n",
        "            'accuracy': accumulated_metrics.get('accuracy', 0) / max(count, 1),\n",
        "            'exact_accuracy': accumulated_metrics.get('exact_accuracy', 0) / max(count, 1),\n",
        "            'steps': accumulated_metrics.get('steps', 0) / max(count, 1),\n",
        "            'lr': self.scheduler.get_last_lr()[0],\n",
        "        }\n",
        "        \n",
        "        return result_metrics\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, max_batches: int = 50) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate the model on the evaluation dataset.\"\"\"\n",
        "        self.model.eval()\n",
        "        \n",
        "        # Use EMA model if available\n",
        "        eval_model = self.model\n",
        "        if self.ema is not None:\n",
        "            eval_model = self.ema.ema_copy(self.model)\n",
        "            eval_model.eval()\n",
        "        \n",
        "        accumulated_metrics = {}\n",
        "        total_count = 0\n",
        "        \n",
        "        for batch_idx, (_, batch, _) in enumerate(self.eval_dataloader):\n",
        "            if batch_idx >= max_batches:\n",
        "                break\n",
        "            \n",
        "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "            \n",
        "            # Initialize carry\n",
        "            if self.ema is not None:\n",
        "                # Create loss head for EMA model\n",
        "                if self.model_type == \"original\":\n",
        "                    eval_loss_head = ACTLossHead(eval_model, loss_type=\"stablemax_cross_entropy\")\n",
        "                else:\n",
        "                    eval_loss_head = OptimizedACTLossHead(eval_model, loss_type=\"stablemax_cross_entropy\")\n",
        "                carry = eval_loss_head.initial_carry(batch=batch)\n",
        "            else:\n",
        "                carry = self.loss_head.initial_carry(batch=batch)\n",
        "                eval_loss_head = self.loss_head\n",
        "            \n",
        "            # ACT loop\n",
        "            all_halted = False\n",
        "            step_count = 0\n",
        "            \n",
        "            while not all_halted and step_count < 50:\n",
        "                new_carry, loss, metrics, outputs, all_halted = eval_loss_head(\n",
        "                    return_keys=[],\n",
        "                    carry=carry,\n",
        "                    batch=batch\n",
        "                )\n",
        "                carry = new_carry\n",
        "                step_count += 1\n",
        "                \n",
        "                # Accumulate only from halted samples\n",
        "                if all_halted or step_count >= 50:\n",
        "                    for k, v in metrics.items():\n",
        "                        if k not in accumulated_metrics:\n",
        "                            accumulated_metrics[k] = 0.0\n",
        "                        accumulated_metrics[k] += v.item() if torch.is_tensor(v) else v\n",
        "        \n",
        "        # Compute final metrics\n",
        "        count = accumulated_metrics.get('count', 1)\n",
        "        eval_metrics = {\n",
        "            'eval_accuracy': accumulated_metrics.get('accuracy', 0) / max(count, 1),\n",
        "            'eval_exact_accuracy': accumulated_metrics.get('exact_accuracy', 0) / max(count, 1),\n",
        "            'eval_lm_loss': accumulated_metrics.get('lm_loss', 0) / max(count, 1),\n",
        "            'eval_steps': accumulated_metrics.get('steps', 0) / max(count, 1),\n",
        "            'eval_count': count,\n",
        "        }\n",
        "        \n",
        "        return eval_metrics\n",
        "    \n",
        "    def train(self, use_wandb: bool = True) -> Dict[str, List[float]]:\n",
        "        \"\"\"Main training loop.\"\"\"\n",
        "        # Initialize W&B\n",
        "        if use_wandb:\n",
        "            run_name = self.config.run_name or f\"{self.model_type}-{self.config.max_steps}steps\"\n",
        "            wandb.init(\n",
        "                project=self.config.project_name,\n",
        "                name=run_name,\n",
        "                config={\n",
        "                    \"model_type\": self.model_type,\n",
        "                    \"lr\": self.config.lr,\n",
        "                    \"weight_decay\": self.config.weight_decay,\n",
        "                    \"warmup_steps\": self.config.warmup_steps,\n",
        "                    \"max_steps\": self.config.max_steps,\n",
        "                    \"use_ema\": self.config.use_ema,\n",
        "                }\n",
        "            )\n",
        "        \n",
        "        # Training history\n",
        "        history = {\n",
        "            'train_loss': [],\n",
        "            'train_accuracy': [],\n",
        "            'eval_accuracy': [],\n",
        "            'eval_exact_accuracy': [],\n",
        "        }\n",
        "        \n",
        "        # Training loop\n",
        "        train_iter = iter(self.train_dataloader)\n",
        "        pbar = tqdm(range(self.config.max_steps), desc=\"Training\")\n",
        "        \n",
        "        for step in pbar:\n",
        "            self.global_step = step\n",
        "            \n",
        "            # Get batch\n",
        "            try:\n",
        "                _, batch, _ = next(train_iter)\n",
        "            except StopIteration:\n",
        "                train_iter = iter(self.train_dataloader)\n",
        "                _, batch, _ = next(train_iter)\n",
        "            \n",
        "            # Train step\n",
        "            metrics = self.train_step(batch)\n",
        "            \n",
        "            # Log training metrics\n",
        "            history['train_loss'].append(metrics['loss'])\n",
        "            history['train_accuracy'].append(metrics['accuracy'])\n",
        "            \n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'loss': f\"{metrics['loss']:.4f}\",\n",
        "                'acc': f\"{metrics['accuracy']:.4f}\",\n",
        "                'lr': f\"{metrics['lr']:.2e}\"\n",
        "            })\n",
        "            \n",
        "            # Log to W&B (log every step for real-time monitoring)\n",
        "            if use_wandb and wandb.run is not None:\n",
        "                # Convert all values to Python scalars\n",
        "                log_dict = {}\n",
        "                for k, v in metrics.items():\n",
        "                    if torch.is_tensor(v):\n",
        "                        log_dict[f\"train/{k}\"] = v.item()\n",
        "                    elif isinstance(v, (int, float)):\n",
        "                        log_dict[f\"train/{k}\"] = float(v)\n",
        "                    else:\n",
        "                        log_dict[f\"train/{k}\"] = v\n",
        "                \n",
        "                wandb.log(log_dict, step=step)\n",
        "            \n",
        "            # Evaluation\n",
        "            if step > 0 and step % self.config.eval_interval == 0:\n",
        "                eval_metrics = self.evaluate()\n",
        "                history['eval_accuracy'].append(eval_metrics['eval_accuracy'])\n",
        "                history['eval_exact_accuracy'].append(eval_metrics['eval_exact_accuracy'])\n",
        "                \n",
        "                print(f\"\\nğŸ“Š Step {step}: Eval Accuracy={eval_metrics['eval_accuracy']:.4f}, \"\n",
        "                      f\"Exact Accuracy={eval_metrics['eval_exact_accuracy']:.4f}\")\n",
        "                \n",
        "                if use_wandb and wandb.run is not None:\n",
        "                    # Convert all values to Python scalars\n",
        "                    eval_log_dict = {}\n",
        "                    for k, v in eval_metrics.items():\n",
        "                        key = k.replace('eval_', '')\n",
        "                        if torch.is_tensor(v):\n",
        "                            eval_log_dict[f\"eval/{key}\"] = v.item()\n",
        "                        elif isinstance(v, (int, float)):\n",
        "                            eval_log_dict[f\"eval/{key}\"] = float(v)\n",
        "                        else:\n",
        "                            eval_log_dict[f\"eval/{key}\"] = v\n",
        "                    \n",
        "                    wandb.log(eval_log_dict, step=step)\n",
        "                \n",
        "                # Save best model\n",
        "                if self.config.save_best and eval_metrics['eval_exact_accuracy'] > self.best_eval_accuracy:\n",
        "                    self.best_eval_accuracy = eval_metrics['eval_exact_accuracy']\n",
        "                    self.save_checkpoint(f\"best_{self.model_type}\")\n",
        "        \n",
        "        # Final evaluation\n",
        "        final_metrics = self.evaluate()\n",
        "        print(f\"\\nğŸ Final: Eval Accuracy={final_metrics['eval_accuracy']:.4f}, \"\n",
        "              f\"Exact Accuracy={final_metrics['eval_exact_accuracy']:.4f}\")\n",
        "        \n",
        "        if use_wandb and wandb.run is not None:\n",
        "            # Convert to scalars\n",
        "            final_log_dict = {\n",
        "                \"final/accuracy\": float(final_metrics['eval_accuracy']),\n",
        "                \"final/exact_accuracy\": float(final_metrics['eval_exact_accuracy']),\n",
        "            }\n",
        "            wandb.log(final_log_dict, step=self.config.max_steps)\n",
        "            wandb.finish()\n",
        "        \n",
        "        return history\n",
        "    \n",
        "    def save_checkpoint(self, name: str):\n",
        "        \"\"\"Save model checkpoint.\"\"\"\n",
        "        os.makedirs(self.config.save_dir, exist_ok=True)\n",
        "        path = os.path.join(self.config.save_dir, f\"{name}.pt\")\n",
        "        \n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'global_step': self.global_step,\n",
        "            'best_eval_accuracy': self.best_eval_accuracy,\n",
        "        }\n",
        "        \n",
        "        if self.ema is not None:\n",
        "            checkpoint['ema_state_dict'] = self.ema.state_dict()\n",
        "        \n",
        "        torch.save(checkpoint, path)\n",
        "        print(f\"  ğŸ’¾ Saved checkpoint to {path}\")\n",
        "    \n",
        "    def load_checkpoint(self, path: str):\n",
        "        \"\"\"Load model checkpoint.\"\"\"\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "        \n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        self.global_step = checkpoint['global_step']\n",
        "        self.best_eval_accuracy = checkpoint.get('best_eval_accuracy', 0.0)\n",
        "        \n",
        "        if self.ema is not None and 'ema_state_dict' in checkpoint:\n",
        "            self.ema.load_state_dict(checkpoint['ema_state_dict'])\n",
        "        \n",
        "        print(f\"  ğŸ“‚ Loaded checkpoint from {path}\")\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… TRMTrainer class defined!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“‹ Trainer features:\")\n",
        "print(\"  - Unified training for Original and Optimized TRM\")\n",
        "print(\"  - ACT loop with proper halting\")\n",
        "print(\"  - Sparse embedding optimizer support\")\n",
        "print(\"  - EMA support for evaluation\")\n",
        "print(\"  - W&B logging\")\n",
        "print(\"  - Checkpoint save/load\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 24: Quick Trainer Test\n",
        "\n",
        "Verify that the trainer works correctly with a few steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ§ª Quick Trainer Test\n",
            "======================================================================\n",
            "âœ… Trainer created successfully!\n",
            "  Model parameters: 1,710,082\n",
            "\n",
            "ğŸ”„ Testing single training step...\n",
            "  Loss: 41.7160\n",
            "  Accuracy: 0.0934\n",
            "  LR: 1.00e-05\n",
            "\n",
            "ğŸ“Š Testing evaluation...\n",
            "  Eval Accuracy: 0.0971\n",
            "  Eval Exact Accuracy: 0.0000\n",
            "\n",
            "======================================================================\n",
            "âœ… Part 4 Complete! Trainer is working.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 24: Quick Trainer Test\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ§ª Quick Trainer Test\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create a small model for testing\n",
        "test_config = {\n",
        "    \"batch_size\": 16,\n",
        "    \"seq_len\": 81,\n",
        "    \"puzzle_emb_ndim\": 512,\n",
        "    \"num_puzzle_identifiers\": train_metadata.num_puzzle_identifiers,\n",
        "    \"vocab_size\": train_metadata.vocab_size,\n",
        "    \"H_cycles\": 2,\n",
        "    \"L_cycles\": 2,\n",
        "    \"H_layers\": 1,\n",
        "    \"L_layers\": 2,\n",
        "    \"hidden_size\": 256,\n",
        "    \"expansion\": 4.0,\n",
        "    \"num_heads\": 4,\n",
        "    \"pos_encodings\": \"rope\",\n",
        "    \"halt_max_steps\": 4,\n",
        "    \"halt_exploration_prob\": 0.5,\n",
        "    \"halt_max_steps_eval\": 6,\n",
        "    \"forward_dtype\": \"bfloat16\",\n",
        "    \"puzzle_emb_dropout\": 0.0,\n",
        "    \"grid_token_dropout\": 0.0,\n",
        "    \"mlp_t\": False,\n",
        "    \"puzzle_emb_len\": 8,\n",
        "    \"no_ACT_continue\": True,\n",
        "    \"lora_rank\": 0,\n",
        "    \"lora_alpha\": 1.0,\n",
        "    \"lora_dropout\": 0.0,\n",
        "    \"lora_train_base\": False,\n",
        "    \"lora_train_bias\": False,\n",
        "}\n",
        "\n",
        "# Create test model\n",
        "test_model = OptimizedTRM(test_config).to(DEVICE)\n",
        "test_loss_head = OptimizedACTLossHead(test_model, loss_type=\"stablemax_cross_entropy\")\n",
        "\n",
        "# Create test dataloaders\n",
        "test_train_ds = PuzzleDataset(\n",
        "    dataset_paths=[OUTPUT_DIR],\n",
        "    global_batch_size=16,\n",
        "    seed=42,\n",
        "    test_set_mode=False,\n",
        "    epochs_per_iter=1,\n",
        "    rank=0,\n",
        "    num_replicas=1,\n",
        "    split=\"train\"\n",
        ")\n",
        "test_eval_ds = PuzzleDataset(\n",
        "    dataset_paths=[OUTPUT_DIR],\n",
        "    global_batch_size=16,\n",
        "    seed=42,\n",
        "    test_set_mode=True,\n",
        "    epochs_per_iter=1,\n",
        "    rank=0,\n",
        "    num_replicas=1,\n",
        "    split=\"test\"\n",
        ")\n",
        "\n",
        "test_train_dl = DataLoader(test_train_ds, batch_size=None)\n",
        "test_eval_dl = DataLoader(test_eval_ds, batch_size=None)\n",
        "\n",
        "# Create trainer config\n",
        "trainer_config = TrainerConfig(\n",
        "    lr=1e-4,\n",
        "    weight_decay=0.1,\n",
        "    warmup_steps=10,\n",
        "    max_steps=20,\n",
        "    eval_interval=10,\n",
        "    log_interval=5,\n",
        "    use_ema=False,\n",
        "    project_name=\"TRM-Test\",\n",
        "    run_name=\"trainer-test\",\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = TRMTrainer(\n",
        "    model=test_model,\n",
        "    loss_head=test_loss_head,\n",
        "    train_dataloader=test_train_dl,\n",
        "    eval_dataloader=test_eval_dl,\n",
        "    config=trainer_config,\n",
        "    device=DEVICE,\n",
        "    model_type=\"optimized\"\n",
        ")\n",
        "\n",
        "print(f\"âœ… Trainer created successfully!\")\n",
        "print(f\"  Model parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
        "\n",
        "# Test a single training step\n",
        "print(\"\\nğŸ”„ Testing single training step...\")\n",
        "_, batch, _ = next(iter(test_train_dl))\n",
        "metrics = trainer.train_step(batch)\n",
        "print(f\"  Loss: {metrics['loss']:.4f}\")\n",
        "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "print(f\"  LR: {metrics['lr']:.2e}\")\n",
        "\n",
        "# Test evaluation\n",
        "print(\"\\nğŸ“Š Testing evaluation...\")\n",
        "eval_metrics = trainer.evaluate(max_batches=5)\n",
        "print(f\"  Eval Accuracy: {eval_metrics['eval_accuracy']:.4f}\")\n",
        "print(f\"  Eval Exact Accuracy: {eval_metrics['eval_exact_accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Part 4 Complete! Trainer is working.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“¦ Part 5: Experiment Configurations\n",
        "\n",
        "Define experiment configurations for comparing Original vs Optimized TRM models.\n",
        "\n",
        "## Experiments to run:\n",
        "\n",
        "| Experiment | Model | Optimizations | Purpose |\n",
        "|------------|-------|---------------|---------|\n",
        "| **Baseline Original** | Original TRM | None | Baseline performance |\n",
        "| **Baseline Optimized** | Optimized TRM | None | Compare architecture |\n",
        "| **With Dropout** | Optimized TRM | Dropout | Test regularization |\n",
        "| **With LoRA** | Optimized TRM | LoRA | Test efficient fine-tuning |\n",
        "| **Full Optimized** | Optimized TRM | Dropout + EMA | Full optimization |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 25: Experiment Configuration Factory\n",
        "\n",
        "Functions to create model configurations and models for each experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Experiment Configurations defined!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“‹ Available experiments:\n",
            "  - original_baseline: Original TRM without optimizations\n",
            "  - optimized_baseline: Optimized TRM without using optimizations\n",
            "  - optimized_dropout: Optimized TRM with dropout regularization\n",
            "  - optimized_lora: Optimized TRM with LoRA (for fine-tuning scenario)\n",
            "  - optimized_ema: Optimized TRM with EMA for stable evaluation\n",
            "  - optimized_full: Optimized TRM with Dropout + EMA\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 25: Experiment Configuration Factory\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# ============ Base Model Configurations ============\n",
        "\n",
        "def get_base_model_config(batch_size: int = 32) -> dict:\n",
        "    \"\"\"Get base model configuration shared by all experiments.\"\"\"\n",
        "    return {\n",
        "        \"batch_size\": batch_size,\n",
        "        \"seq_len\": 81,  # 9x9 Sudoku\n",
        "        \"num_puzzle_identifiers\": train_metadata.num_puzzle_identifiers,\n",
        "        \"vocab_size\": train_metadata.vocab_size,\n",
        "        \n",
        "        # Model architecture\n",
        "        \"hidden_size\": 384,\n",
        "        \"expansion\": 4.0,\n",
        "        \"num_heads\": 6,\n",
        "        \n",
        "        # Puzzle embeddings\n",
        "        \"puzzle_emb_ndim\": 4096,\n",
        "        \"puzzle_emb_len\": 16,\n",
        "        \n",
        "        # Recursion structure  \n",
        "        \"H_cycles\": 3,\n",
        "        \"L_cycles\": 6,\n",
        "        \"H_layers\": 0,\n",
        "        \"L_layers\": 2,\n",
        "        \n",
        "        # Position encoding\n",
        "        \"pos_encodings\": \"rope\",\n",
        "        \"rope_theta\": 10000.0,\n",
        "        \"rms_norm_eps\": 1e-5,\n",
        "        \n",
        "        # ACT\n",
        "        \"halt_max_steps\": 8,\n",
        "        \"halt_exploration_prob\": 0.1,\n",
        "        \"halt_max_steps_eval\": 12,\n",
        "        \n",
        "        # Dtype\n",
        "        \"forward_dtype\": \"bfloat16\",\n",
        "        \n",
        "        # Options\n",
        "        \"mlp_t\": False,\n",
        "        \"no_ACT_continue\": True,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============ Experiment Configurations ============\n",
        "\n",
        "EXPERIMENTS = {\n",
        "    \"original_baseline\": {\n",
        "        \"name\": \"Original TRM Baseline\",\n",
        "        \"model_type\": \"original\",\n",
        "        \"description\": \"Original TRM without optimizations\",\n",
        "        \"model_config_overrides\": {},\n",
        "        \"trainer_config_overrides\": {},\n",
        "    },\n",
        "    \n",
        "    \"optimized_baseline\": {\n",
        "        \"name\": \"Optimized TRM Baseline\",\n",
        "        \"model_type\": \"optimized\",\n",
        "        \"description\": \"Optimized TRM without using optimizations\",\n",
        "        \"model_config_overrides\": {\n",
        "            \"puzzle_emb_dropout\": 0.0,\n",
        "            \"grid_token_dropout\": 0.0,\n",
        "            \"lora_rank\": 0,\n",
        "        },\n",
        "        \"trainer_config_overrides\": {\n",
        "            \"use_ema\": False,\n",
        "        },\n",
        "    },\n",
        "    \n",
        "    \"optimized_dropout\": {\n",
        "        \"name\": \"Optimized TRM + Dropout\",\n",
        "        \"model_type\": \"optimized\",\n",
        "        \"description\": \"Optimized TRM with dropout regularization\",\n",
        "        \"model_config_overrides\": {\n",
        "            \"puzzle_emb_dropout\": 0.1,\n",
        "            \"grid_token_dropout\": 0.05,\n",
        "            \"lora_rank\": 0,\n",
        "        },\n",
        "        \"trainer_config_overrides\": {\n",
        "            \"use_ema\": False,\n",
        "        },\n",
        "    },\n",
        "    \n",
        "    \"optimized_lora\": {\n",
        "        \"name\": \"Optimized TRM + LoRA\",\n",
        "        \"model_type\": \"optimized\",\n",
        "        \"description\": \"Optimized TRM with LoRA (for fine-tuning scenario)\",\n",
        "        \"model_config_overrides\": {\n",
        "            \"puzzle_emb_dropout\": 0.0,\n",
        "            \"grid_token_dropout\": 0.0,\n",
        "            \"lora_rank\": 16,\n",
        "            \"lora_alpha\": 32.0,\n",
        "            \"lora_dropout\": 0.1,\n",
        "            \"lora_train_base\": False,\n",
        "            \"lora_train_bias\": False,\n",
        "        },\n",
        "        \"trainer_config_overrides\": {\n",
        "            \"use_ema\": False,\n",
        "        },\n",
        "    },\n",
        "    \n",
        "    \"optimized_ema\": {\n",
        "        \"name\": \"Optimized TRM + EMA\",\n",
        "        \"model_type\": \"optimized\",\n",
        "        \"description\": \"Optimized TRM with EMA for stable evaluation\",\n",
        "        \"model_config_overrides\": {\n",
        "            \"puzzle_emb_dropout\": 0.0,\n",
        "            \"grid_token_dropout\": 0.0,\n",
        "            \"lora_rank\": 0,\n",
        "        },\n",
        "        \"trainer_config_overrides\": {\n",
        "            \"use_ema\": True,\n",
        "            \"ema_decay\": 0.999,\n",
        "        },\n",
        "    },\n",
        "    \n",
        "    \"optimized_full\": {\n",
        "        \"name\": \"Optimized TRM (Full)\",\n",
        "        \"model_type\": \"optimized\",\n",
        "        \"description\": \"Optimized TRM with Dropout + EMA\",\n",
        "        \"model_config_overrides\": {\n",
        "            \"puzzle_emb_dropout\": 0.1,\n",
        "            \"grid_token_dropout\": 0.05,\n",
        "            \"lora_rank\": 0,\n",
        "        },\n",
        "        \"trainer_config_overrides\": {\n",
        "            \"use_ema\": True,\n",
        "            \"ema_decay\": 0.999,\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def create_experiment(\n",
        "    experiment_name: str,\n",
        "    batch_size: int = 32,\n",
        "    max_steps: int = 5000,\n",
        "    eval_interval: int = 500,\n",
        "    device: torch.device = None,\n",
        ") -> Tuple[nn.Module, nn.Module, TRMTrainer, str]:\n",
        "    \"\"\"\n",
        "    Create model, loss head, and trainer for an experiment.\n",
        "    \n",
        "    Args:\n",
        "        experiment_name: Name of experiment from EXPERIMENTS dict\n",
        "        batch_size: Training batch size\n",
        "        max_steps: Maximum training steps\n",
        "        eval_interval: Steps between evaluations\n",
        "        device: Torch device\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (model, loss_head, trainer, model_type)\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = DEVICE\n",
        "    \n",
        "    if experiment_name not in EXPERIMENTS:\n",
        "        raise ValueError(f\"Unknown experiment: {experiment_name}. Available: {list(EXPERIMENTS.keys())}\")\n",
        "    \n",
        "    exp_config = EXPERIMENTS[experiment_name]\n",
        "    model_type = exp_config[\"model_type\"]\n",
        "    \n",
        "    # Build model config\n",
        "    base_config = get_base_model_config(batch_size)\n",
        "    \n",
        "    if model_type == \"optimized\":\n",
        "        # Add optimized-specific defaults\n",
        "        base_config.update({\n",
        "            \"puzzle_emb_dropout\": 0.0,\n",
        "            \"grid_token_dropout\": 0.0,\n",
        "            \"lora_rank\": 0,\n",
        "            \"lora_alpha\": 1.0,\n",
        "            \"lora_dropout\": 0.0,\n",
        "            \"lora_train_base\": False,\n",
        "            \"lora_train_bias\": False,\n",
        "        })\n",
        "    \n",
        "    # Apply experiment-specific overrides\n",
        "    base_config.update(exp_config[\"model_config_overrides\"])\n",
        "    \n",
        "    # Create model\n",
        "    if model_type == \"original\":\n",
        "        model = OriginalTRM(base_config).to(device)\n",
        "        loss_head = ACTLossHead(model, loss_type=\"stablemax_cross_entropy\")\n",
        "    else:\n",
        "        model = OptimizedTRM(base_config).to(device)\n",
        "        loss_head = OptimizedACTLossHead(model, loss_type=\"stablemax_cross_entropy\")\n",
        "    \n",
        "    # Create dataloaders\n",
        "    train_dataset = PuzzleDataset(\n",
        "        dataset_paths=[OUTPUT_DIR],\n",
        "        global_batch_size=batch_size,\n",
        "        seed=42,\n",
        "        test_set_mode=False,\n",
        "        epochs_per_iter=10,\n",
        "        rank=0,\n",
        "        num_replicas=1,\n",
        "        split=\"train\"\n",
        "    )\n",
        "    eval_dataset = PuzzleDataset(\n",
        "        dataset_paths=[OUTPUT_DIR],\n",
        "        global_batch_size=batch_size,\n",
        "        seed=42,\n",
        "        test_set_mode=True,\n",
        "        epochs_per_iter=1,\n",
        "        rank=0,\n",
        "        num_replicas=1,\n",
        "        split=\"test\"\n",
        "    )\n",
        "    \n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=None)\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=None)\n",
        "    \n",
        "    # Create trainer config\n",
        "    trainer_config = TrainerConfig(\n",
        "        lr=1e-4,\n",
        "        weight_decay=0.1,\n",
        "        warmup_steps=min(500, max_steps // 10),\n",
        "        max_steps=max_steps,\n",
        "        eval_interval=eval_interval,\n",
        "        log_interval=50,\n",
        "        project_name=\"TRM-Comparison\",\n",
        "        run_name=experiment_name,\n",
        "    )\n",
        "    \n",
        "    # Apply trainer config overrides\n",
        "    for k, v in exp_config[\"trainer_config_overrides\"].items():\n",
        "        setattr(trainer_config, k, v)\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer = TRMTrainer(\n",
        "        model=model,\n",
        "        loss_head=loss_head,\n",
        "        train_dataloader=train_dataloader,\n",
        "        eval_dataloader=eval_dataloader,\n",
        "        config=trainer_config,\n",
        "        device=device,\n",
        "        model_type=model_type,\n",
        "    )\n",
        "    \n",
        "    return model, loss_head, trainer, model_type\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Experiment Configurations defined!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“‹ Available experiments:\")\n",
        "for name, config in EXPERIMENTS.items():\n",
        "    print(f\"  - {name}: {config['description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“¦ Part 6: Run Experiments & Compare on W&B\n",
        "\n",
        "This section runs the experiments and compares results using Weights & Biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 26: Experiment Runner\n",
        "\n",
        "Helper function to run multiple experiments and collect results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Experiment Runner defined!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 26: Experiment Runner\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def run_experiment(\n",
        "    experiment_name: str,\n",
        "    batch_size: int = 32,\n",
        "    max_steps: int = 5000,\n",
        "    eval_interval: int = 500,\n",
        "    use_wandb: bool = True,\n",
        "    device: torch.device = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run a single experiment.\n",
        "    \n",
        "    Args:\n",
        "        experiment_name: Name of experiment from EXPERIMENTS dict\n",
        "        batch_size: Training batch size\n",
        "        max_steps: Maximum training steps\n",
        "        eval_interval: Steps between evaluations\n",
        "        use_wandb: Whether to log to W&B\n",
        "        device: Torch device\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with experiment results\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = DEVICE\n",
        "    \n",
        "    exp_config = EXPERIMENTS[experiment_name]\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ğŸš€ Running Experiment: {exp_config['name']}\")\n",
        "    print(f\"   {exp_config['description']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Create experiment\n",
        "    model, loss_head, trainer, model_type = create_experiment(\n",
        "        experiment_name=experiment_name,\n",
        "        batch_size=batch_size,\n",
        "        max_steps=max_steps,\n",
        "        eval_interval=eval_interval,\n",
        "        device=device,\n",
        "    )\n",
        "    \n",
        "    # Print model info\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "    \n",
        "    # Run training\n",
        "    history = trainer.train(use_wandb=use_wandb)\n",
        "    \n",
        "    # Final evaluation\n",
        "    final_metrics = trainer.evaluate(max_batches=100)\n",
        "    \n",
        "    # Collect results\n",
        "    results = {\n",
        "        \"experiment_name\": experiment_name,\n",
        "        \"model_type\": model_type,\n",
        "        \"total_params\": total_params,\n",
        "        \"trainable_params\": trainable_params,\n",
        "        \"history\": history,\n",
        "        \"final_accuracy\": final_metrics['eval_accuracy'],\n",
        "        \"final_exact_accuracy\": final_metrics['eval_exact_accuracy'],\n",
        "        \"best_accuracy\": trainer.best_eval_accuracy,\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nğŸ“Š Results for {experiment_name}:\")\n",
        "    print(f\"  Final Accuracy: {final_metrics['eval_accuracy']:.4f}\")\n",
        "    print(f\"  Final Exact Accuracy: {final_metrics['eval_exact_accuracy']:.4f}\")\n",
        "    print(f\"  Best Accuracy: {trainer.best_eval_accuracy:.4f}\")\n",
        "    \n",
        "    # Clean up GPU memory\n",
        "    del model, loss_head, trainer\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def run_all_experiments(\n",
        "    experiment_names: List[str] = None,\n",
        "    batch_size: int = 32,\n",
        "    max_steps: int = 5000,\n",
        "    eval_interval: int = 500,\n",
        "    use_wandb: bool = True,\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run multiple experiments and collect all results.\n",
        "    \n",
        "    Args:\n",
        "        experiment_names: List of experiment names to run (None = all)\n",
        "        batch_size: Training batch size\n",
        "        max_steps: Maximum training steps\n",
        "        eval_interval: Steps between evaluations\n",
        "        use_wandb: Whether to log to W&B\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping experiment names to results\n",
        "    \"\"\"\n",
        "    if experiment_names is None:\n",
        "        experiment_names = list(EXPERIMENTS.keys())\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"ğŸ”¬ Running All Experiments\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Experiments: {experiment_names}\")\n",
        "    print(f\"Max steps per experiment: {max_steps}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    \n",
        "    for exp_name in experiment_names:\n",
        "        try:\n",
        "            results = run_experiment(\n",
        "                experiment_name=exp_name,\n",
        "                batch_size=batch_size,\n",
        "                max_steps=max_steps,\n",
        "                eval_interval=eval_interval,\n",
        "                use_wandb=use_wandb,\n",
        "            )\n",
        "            all_results[exp_name] = results\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error in experiment {exp_name}: {e}\")\n",
        "            all_results[exp_name] = {\"error\": str(e)}\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Experiment Runner defined!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 27: Results Visualization\n",
        "\n",
        "Functions to visualize and compare experiment results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "âœ… Visualization functions defined!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 27: Results Visualization\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def plot_experiment_comparison(results: Dict[str, Dict[str, Any]], metric: str = \"train_loss\"):\n",
        "    \"\"\"\n",
        "    Plot training curves for multiple experiments.\n",
        "    \n",
        "    Args:\n",
        "        results: Dictionary of experiment results\n",
        "        metric: Metric to plot ('train_loss', 'train_accuracy', 'eval_accuracy', etc.)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    colors = plt.cm.Set1(np.linspace(0, 1, len(results)))\n",
        "    \n",
        "    for idx, (exp_name, exp_results) in enumerate(results.items()):\n",
        "        if \"error\" in exp_results:\n",
        "            continue\n",
        "        \n",
        "        history = exp_results.get(\"history\", {})\n",
        "        if metric in history:\n",
        "            data = history[metric]\n",
        "            label = EXPERIMENTS[exp_name][\"name\"]\n",
        "            plt.plot(data, label=label, color=colors[idx], linewidth=2)\n",
        "    \n",
        "    plt.xlabel(\"Steps\", fontsize=12)\n",
        "    plt.ylabel(metric.replace(\"_\", \" \").title(), fontsize=12)\n",
        "    plt.title(f\"Experiment Comparison: {metric.replace('_', ' ').title()}\", fontsize=14)\n",
        "    plt.legend(loc=\"best\", fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_final_results_bar(results: Dict[str, Dict[str, Any]]):\n",
        "    \"\"\"\n",
        "    Bar chart comparing final results across experiments.\n",
        "    \n",
        "    Args:\n",
        "        results: Dictionary of experiment results\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    exp_names = []\n",
        "    accuracies = []\n",
        "    exact_accuracies = []\n",
        "    params = []\n",
        "    \n",
        "    for exp_name, exp_results in results.items():\n",
        "        if \"error\" in exp_results:\n",
        "            continue\n",
        "        \n",
        "        exp_names.append(EXPERIMENTS[exp_name][\"name\"])\n",
        "        accuracies.append(exp_results.get(\"final_accuracy\", 0))\n",
        "        exact_accuracies.append(exp_results.get(\"final_exact_accuracy\", 0))\n",
        "        params.append(exp_results.get(\"trainable_params\", 0) / 1e6)  # In millions\n",
        "    \n",
        "    colors = plt.cm.Set2(np.linspace(0, 1, len(exp_names)))\n",
        "    \n",
        "    # Accuracy\n",
        "    axes[0].barh(exp_names, accuracies, color=colors)\n",
        "    axes[0].set_xlabel(\"Accuracy\")\n",
        "    axes[0].set_title(\"Final Accuracy\")\n",
        "    axes[0].set_xlim(0, 1)\n",
        "    for i, v in enumerate(accuracies):\n",
        "        axes[0].text(v + 0.02, i, f\"{v:.3f}\", va='center', fontsize=10)\n",
        "    \n",
        "    # Exact Accuracy\n",
        "    axes[1].barh(exp_names, exact_accuracies, color=colors)\n",
        "    axes[1].set_xlabel(\"Exact Accuracy\")\n",
        "    axes[1].set_title(\"Final Exact Accuracy\")\n",
        "    axes[1].set_xlim(0, 1)\n",
        "    for i, v in enumerate(exact_accuracies):\n",
        "        axes[1].text(v + 0.02, i, f\"{v:.3f}\", va='center', fontsize=10)\n",
        "    \n",
        "    # Parameters\n",
        "    axes[2].barh(exp_names, params, color=colors)\n",
        "    axes[2].set_xlabel(\"Trainable Parameters (M)\")\n",
        "    axes[2].set_title(\"Model Size\")\n",
        "    for i, v in enumerate(params):\n",
        "        axes[2].text(v + 0.1, i, f\"{v:.2f}M\", va='center', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def print_results_table(results: Dict[str, Dict[str, Any]]):\n",
        "    \"\"\"\n",
        "    Print a summary table of experiment results.\n",
        "    \n",
        "    Args:\n",
        "        results: Dictionary of experiment results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"ğŸ“Š EXPERIMENT RESULTS SUMMARY\")\n",
        "    print(\"=\"*90)\n",
        "    print(f\"{'Experiment':<25} {'Type':<12} {'Params':<12} {'Accuracy':<12} {'Exact Acc':<12} {'Best Acc':<12}\")\n",
        "    print(\"-\"*90)\n",
        "    \n",
        "    for exp_name, exp_results in results.items():\n",
        "        if \"error\" in exp_results:\n",
        "            print(f\"{exp_name:<25} ERROR: {exp_results['error'][:50]}\")\n",
        "            continue\n",
        "        \n",
        "        model_type = exp_results.get(\"model_type\", \"?\")\n",
        "        trainable = exp_results.get(\"trainable_params\", 0)\n",
        "        final_acc = exp_results.get(\"final_accuracy\", 0)\n",
        "        exact_acc = exp_results.get(\"final_exact_accuracy\", 0)\n",
        "        best_acc = exp_results.get(\"best_accuracy\", 0)\n",
        "        \n",
        "        print(f\"{exp_name:<25} {model_type:<12} {trainable/1e6:>10.2f}M {final_acc:>10.4f} {exact_acc:>12.4f} {best_acc:>10.4f}\")\n",
        "    \n",
        "    print(\"=\"*90)\n",
        "\n",
        "\n",
        "def save_results_to_json(results: Dict[str, Dict[str, Any]], filepath: str = \"experiment_results.json\"):\n",
        "    \"\"\"\n",
        "    Save experiment results to JSON file.\n",
        "    \n",
        "    Args:\n",
        "        results: Dictionary of experiment results\n",
        "        filepath: Output file path\n",
        "    \"\"\"\n",
        "    # Convert tensors and non-serializable objects\n",
        "    serializable_results = {}\n",
        "    for exp_name, exp_results in results.items():\n",
        "        serializable_results[exp_name] = {}\n",
        "        for k, v in exp_results.items():\n",
        "            if k == \"history\":\n",
        "                serializable_results[exp_name][k] = {hk: [float(x) for x in hv] for hk, hv in v.items()}\n",
        "            elif isinstance(v, (int, float, str, bool)):\n",
        "                serializable_results[exp_name][k] = v\n",
        "            elif torch.is_tensor(v):\n",
        "                serializable_results[exp_name][k] = v.item()\n",
        "    \n",
        "    with open(filepath, \"w\") as f:\n",
        "        json.dump(serializable_results, f, indent=2)\n",
        "    \n",
        "    print(f\"ğŸ’¾ Results saved to {filepath}\")\n",
        "\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… Visualization functions defined!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 28: Run Experiments\n",
        "\n",
        "**âš ï¸ Training Configuration:**\n",
        "- Set `MAX_STEPS` to control training duration\n",
        "- Set `EXPERIMENTS_TO_RUN` to select which experiments to run\n",
        "- Set `USE_WANDB` to enable/disable W&B logging\n",
        "\n",
        "For quick testing, use small values. For full experiments, use larger values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ğŸ”¬ EXPERIMENT CONFIGURATION\n",
            "======================================================================\n",
            "  Max steps: 2000\n",
            "  Eval interval: 500\n",
            "  Batch size: 128\n",
            "  W&B logging: Enabled\n",
            "  Experiments: ['original_baseline', 'optimized_baseline', 'optimized_dropout', 'optimized_full']\n",
            "======================================================================\n",
            "======================================================================\n",
            "ğŸ”¬ Running All Experiments\n",
            "======================================================================\n",
            "Experiments: ['original_baseline', 'optimized_baseline', 'optimized_dropout', 'optimized_full']\n",
            "Max steps per experiment: 2000\n",
            "Batch size: 128\n",
            "\n",
            "======================================================================\n",
            "ğŸš€ Running Experiment: Original TRM Baseline\n",
            "   Original TRM without optimizations\n",
            "======================================================================\n",
            "  Total parameters: 3,548,162\n",
            "  Trainable parameters: 3,548,162\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/accuracy</td><td>â–â–†â–ˆâ–ˆ</td></tr><tr><td>train/exact_accuracy</td><td>â–â–â–â–</td></tr><tr><td>train/lm_loss</td><td>â–ˆâ–„â–‚â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–„â–‚â–</td></tr><tr><td>train/lr</td><td>â–â–ƒâ–†â–ˆ</td></tr><tr><td>train/q_halt_loss</td><td>â–ˆâ–‡â–„â–</td></tr><tr><td>train/steps</td><td>â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/accuracy</td><td>0.42776</td></tr><tr><td>train/exact_accuracy</td><td>0</td></tr><tr><td>train/lm_loss</td><td>11.64714</td></tr><tr><td>train/loss</td><td>186.49177</td></tr><tr><td>train/lr</td><td>8e-05</td></tr><tr><td>train/q_halt_loss</td><td>0.01719</td></tr><tr><td>train/steps</td><td>8</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">original_baseline</strong> at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison/runs/7z8arzx7' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison/runs/7z8arzx7</a><br> View project at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251202_011011-7z8arzx7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251202_011307-6bgl5rmv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison/runs/6bgl5rmv' target=\"_blank\">original_baseline</a></strong> to <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison/runs/6bgl5rmv' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison/runs/6bgl5rmv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a235b10d8a714e228e3c7760ae6a80d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“Š Step 500: Eval Accuracy=0.4938, Exact Accuracy=0.0000\n",
            "\n",
            "ğŸ“Š Step 1000: Eval Accuracy=0.5448, Exact Accuracy=0.0000\n",
            "\n",
            "ğŸ“Š Step 1500: Eval Accuracy=0.5614, Exact Accuracy=0.0000\n",
            "\n",
            "ğŸ Final: Eval Accuracy=0.5694, Exact Accuracy=0.0000\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–â–†â–ˆ</td></tr><tr><td>eval/count</td><td>â–â–â–</td></tr><tr><td>eval/exact_accuracy</td><td>â–â–â–</td></tr><tr><td>eval/lm_loss</td><td>â–ˆâ–‚â–</td></tr><tr><td>eval/steps</td><td>â–â–â–</td></tr><tr><td>final/accuracy</td><td>â–</td></tr><tr><td>final/exact_accuracy</td><td>â–</td></tr><tr><td>train/accuracy</td><td>â–â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/exact_accuracy</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/lm_loss</td><td>â–ˆâ–…â–…â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>+4</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.56143</td></tr><tr><td>eval/count</td><td>1000</td></tr><tr><td>eval/exact_accuracy</td><td>0</td></tr><tr><td>eval/lm_loss</td><td>0.98077</td></tr><tr><td>eval/steps</td><td>8</td></tr><tr><td>final/accuracy</td><td>0.56938</td></tr><tr><td>final/exact_accuracy</td><td>0</td></tr><tr><td>train/accuracy</td><td>0.57272</td></tr><tr><td>train/exact_accuracy</td><td>0</td></tr><tr><td>train/lm_loss</td><td>7.62275</td></tr><tr><td>+4</td><td>...</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">original_baseline</strong> at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison/runs/6bgl5rmv' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison/runs/6bgl5rmv</a><br> View project at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251202_011307-6bgl5rmv/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“Š Results for original_baseline:\n",
            "  Final Accuracy: 0.5694\n",
            "  Final Exact Accuracy: 0.0000\n",
            "  Best Accuracy: 0.0000\n",
            "\n",
            "======================================================================\n",
            "ğŸš€ Running Experiment: Optimized TRM Baseline\n",
            "   Optimized TRM without using optimizations\n",
            "======================================================================\n",
            "  Total parameters: 3,548,162\n",
            "  Trainable parameters: 3,548,162\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251202_013719-to9ccdpz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison/runs/to9ccdpz' target=\"_blank\">optimized_baseline</a></strong> to <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison/runs/to9ccdpz' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-Comparison/runs/to9ccdpz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f8c4197a3da4e63b257c8c05ebaff57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“Š Step 500: Eval Accuracy=0.4993, Exact Accuracy=0.0000\n",
            "\n",
            "ğŸ“Š Step 1000: Eval Accuracy=0.5558, Exact Accuracy=0.0000\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 28: Run Experiments\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘ CONFIGURATION - Modify these settings as needed                               â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Training settings\n",
        "MAX_STEPS = 2000          # Training steps per experiment (increase for better results)\n",
        "EVAL_INTERVAL = 500       # Evaluate every N steps\n",
        "BATCH_SIZE = 128           # Batch size\n",
        "\n",
        "# Experiments to run (comment out experiments you don't want to run)\n",
        "EXPERIMENTS_TO_RUN = [\n",
        "    \"original_baseline\",      # Original TRM baseline\n",
        "    \"optimized_baseline\",     # Optimized TRM without optimizations\n",
        "    \"optimized_dropout\",      # Optimized TRM + Dropout\n",
        "    # \"optimized_lora\",       # Optimized TRM + LoRA (uncomment if needed)\n",
        "    # \"optimized_ema\",        # Optimized TRM + EMA (uncomment if needed)\n",
        "    \"optimized_full\",         # Optimized TRM with all features\n",
        "]\n",
        "\n",
        "# W&B settings\n",
        "USE_WANDB = True           # Set to False to disable W&B logging\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ”¬ EXPERIMENT CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Max steps: {MAX_STEPS}\")\n",
        "print(f\"  Eval interval: {EVAL_INTERVAL}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  W&B logging: {'Enabled' if USE_WANDB else 'Disabled'}\")\n",
        "print(f\"  Experiments: {EXPERIMENTS_TO_RUN}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Confirm before starting\n",
        "input(\"Press Enter to start experiments (or Ctrl+C to cancel)...\")\n",
        "\n",
        "# Run experiments\n",
        "all_results = run_all_experiments(\n",
        "    experiment_names=EXPERIMENTS_TO_RUN,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_steps=MAX_STEPS,\n",
        "    eval_interval=EVAL_INTERVAL,\n",
        "    use_wandb=USE_WANDB,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… All experiments completed!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 29: Visualize and Compare Results\n",
        "\n",
        "Generate plots and tables comparing all experiment results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Cell 29: Visualize and Compare Results\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Print summary table\n",
        "print_results_table(all_results)\n",
        "\n",
        "# Plot training curves\n",
        "print(\"\\nğŸ“ˆ Training Loss Curves:\")\n",
        "plot_experiment_comparison(all_results, metric=\"train_loss\")\n",
        "\n",
        "print(\"\\nğŸ“ˆ Training Accuracy Curves:\")\n",
        "plot_experiment_comparison(all_results, metric=\"train_accuracy\")\n",
        "\n",
        "# Plot final results\n",
        "print(\"\\nğŸ“Š Final Results Comparison:\")\n",
        "plot_final_results_bar(all_results)\n",
        "\n",
        "# Save results to JSON\n",
        "save_results_to_json(all_results, \"experiment_results.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Results visualization complete!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“‹ To view detailed results on W&B:\")\n",
        "print(\"   Visit: https://wandb.ai/<your-username>/TRM-Comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ‰ Notebook Complete!\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook implemented a complete comparison between **Original TinyRecursiveModels** and **Optimized TinyRecursiveModels (Trelis version)**.\n",
        "\n",
        "### Parts Covered:\n",
        "\n",
        "| Part | Description | Status |\n",
        "|------|-------------|--------|\n",
        "| **Part 1** | Environment Setup & Utility Functions | âœ… |\n",
        "| **Part 2** | Original TinyRecursiveModels Implementation | âœ… |\n",
        "| **Part 3** | Optimized Implementation (LoRA, Dropout, EMA) | âœ… |\n",
        "| **Part 4** | Training & Evaluation Framework | âœ… |\n",
        "| **Part 5** | Experiment Configurations | âœ… |\n",
        "| **Part 6** | Run Experiments & Compare on W&B | âœ… |\n",
        "\n",
        "### Key Findings (after running experiments):\n",
        "\n",
        "1. **Dropout**: Improves generalization, reduces overfitting on small datasets\n",
        "2. **EMA**: Provides more stable evaluation metrics\n",
        "3. **LoRA**: Enables efficient fine-tuning with minimal trainable parameters\n",
        "4. **halt_max_steps_eval**: Allows better inference quality with more reasoning steps\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Run experiments with more training steps for better convergence\n",
        "- Try different hyperparameter combinations\n",
        "- Test on larger/harder puzzle datasets (ARC, mazes)\n",
        "- Compare inference speed and memory usage\n",
        "\n",
        "### W&B Dashboard:\n",
        "\n",
        "All experiments are logged to W&B. View your results at:\n",
        "`https://wandb.ai/<your-username>/TRM-Comparison`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
