{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§  TinyRecursiveModel: H100 Implementation\n",
        "\n",
        "This notebook provides a **complete faithful reproduction** of the **TinyRecursiveModels** (TRM) architecture.\n",
        "\n",
        "**Key Features:**\n",
        "- âœ… **Exact reproduction** of the original TinyRecursiveModels codebase\n",
        "- âœ… **H100 optimizations** (torch.compile, TF32, optimized attention)\n",
        "- âœ… **No distributed code** - single GPU implementation\n",
        "- âœ… **Ready to run** on H100 GPUs\n",
        "\n",
        "**Based on:** [TinyRecursiveModels](https://github.com/AlexiaJM/TinyRecursiveModels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hugging Face Token\n",
        "HF_TOKEN = \"\"\n",
        "# Wandb Token\n",
        "WANDB_API_KEY = ''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dependencies installed!\n",
            "ðŸ“ Note: Using PyTorch's built-in AdamW optimizer (no need for adam-atan2-pytorch)\n",
            "ðŸ“ Note: Installed nvidia-ml-py for GPU monitoring\n"
          ]
        }
      ],
      "source": [
        "!uv pip install -q torch einops tqdm numpy pydantic wandb coolname ninja wheel triton nvidia-ml-py\n",
        "print(\"âœ… Dependencies installed!\")\n",
        "print(\"ðŸ“ Note: Using PyTorch's built-in AdamW optimizer (no need for adam-atan2-pytorch)\")\n",
        "print(\"ðŸ“ Note: Installed nvidia-ml-py for GPU monitoring\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "ðŸš€ H100/A100 detected! TF32 enabled\n",
            "======================================================================\n",
            "ðŸ“Š Configuring Weights & Biases\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjarviszhang\u001b[0m (\u001b[33mjarviszhang-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“ W&B Project: TRM-A100-Sudoku\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "from typing import Optional, Any, Sequence, List\n",
        "from dataclasses import dataclass\n",
        "import os\n",
        "import math\n",
        "import yaml\n",
        "import shutil\n",
        "import copy\n",
        "import importlib\n",
        "import inspect\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import coolname\n",
        "import pydantic\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Using PyTorch's built-in AdamW optimizer instead of AdamAtan2\n",
        "# AdamW is more standard and widely used\n",
        "\n",
        "# H100 optimizations\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    if \"H100\" in gpu_name or \"A100\" in gpu_name:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        print(\"ðŸš€ H100/A100 detected! TF32 enabled\")\n",
        "    torch.cuda.set_device(0)\n",
        "\n",
        "# ============================================================================\n",
        "# Wandb Configuration (Similar to TRM_Baseline.ipynb)\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ“Š Configuring Weights & Biases\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "\n",
        "# Project configuration\n",
        "WANDB_PROJECT = \"TRM-A100-Sudoku\"\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "WANDB_ENTITY = None  # Set to your W&B username/team if needed (e.g., \"jarviszhang-new-york-university\")\n",
        "\n",
        "print(f\"ðŸ“ W&B Project: {WANDB_PROJECT}\")\n",
        "if WANDB_ENTITY:\n",
        "    print(f\"ðŸ‘¤ W&B Entity: {WANDB_ENTITY}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Common Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def trunc_normal_init_(tensor: torch.Tensor, std: float = 1.0, lower: float = -2.0, upper: float = 2.0):\n",
        "    # NOTE: PyTorch nn.init.trunc_normal_ is not mathematically correct, the std dev is not actually the std dev of initialized tensor\n",
        "    # This function is a PyTorch version of jax truncated normal init (default init method in flax)\n",
        "    # https://github.com/jax-ml/jax/blob/main/jax/_src/random.py#L807-L848\n",
        "    # https://github.com/jax-ml/jax/blob/main/jax/_src/nn/initializers.py#L162-L199\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if std == 0:\n",
        "            tensor.zero_()\n",
        "        else:\n",
        "            sqrt2 = math.sqrt(2)\n",
        "            a = math.erf(lower / sqrt2)\n",
        "            b = math.erf(upper / sqrt2)\n",
        "            z = (b - a) / 2\n",
        "\n",
        "            c = (2 * math.pi) ** -0.5\n",
        "            pdf_u = c * math.exp(-0.5 * lower ** 2)\n",
        "            pdf_l = c * math.exp(-0.5 * upper ** 2)\n",
        "            comp_std = std / math.sqrt(1 - (upper * pdf_u - lower * pdf_l) / z - ((pdf_u - pdf_l) / z) ** 2)\n",
        "\n",
        "            tensor.uniform_(a, b)\n",
        "            tensor.erfinv_()\n",
        "            tensor.mul_(sqrt2 * comp_std)\n",
        "            tensor.clip_(lower * comp_std, upper * comp_std)\n",
        "\n",
        "    return tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import einops\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#try:\n",
        "#    from flash_attn_interface import flash_attn_func  # type: ignore[import]\n",
        "#except ImportError:\n",
        "#    # Fallback to FlashAttention 2\n",
        "#    from flash_attn import flash_attn_func  # type: ignore[import]\n",
        "from torch.nn.functional import scaled_dot_product_attention\n",
        "\n",
        "# trunc_normal_init_ defined in Part 3\n",
        "\n",
        "\n",
        "CosSin = Tuple[torch.Tensor, torch.Tensor]\n",
        "\n",
        "\n",
        "def _find_multiple(a, b):\n",
        "    return (-(a // -b)) * b\n",
        "\n",
        "\n",
        "def rotate_half(x: torch.Tensor):\n",
        "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n",
        "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor):\n",
        "    # q, k: [bs, seq_len, num_heads, head_dim]\n",
        "    # cos, sin: [seq_len, head_dim]\n",
        "    orig_dtype = q.dtype\n",
        "    q = q.to(cos.dtype)\n",
        "    k = k.to(cos.dtype)\n",
        "\n",
        "    q_embed = (q * cos.unsqueeze(-2)) + (rotate_half(q) * sin.unsqueeze(-2))\n",
        "    k_embed = (k * cos.unsqueeze(-2)) + (rotate_half(k) * sin.unsqueeze(-2))\n",
        "\n",
        "    return q_embed.to(orig_dtype), k_embed.to(orig_dtype)\n",
        "\n",
        "\n",
        "class CastedLinear(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 out_features: int,\n",
        "                 bias: bool):\n",
        "        super().__init__()\n",
        "        # Truncated LeCun normal init\n",
        "        self.weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((out_features, in_features)), std=1.0 / (in_features ** 0.5))\n",
        "        )\n",
        "        self.bias = None\n",
        "        if bias:\n",
        "            # Zero init bias\n",
        "            self.bias = nn.Parameter(torch.zeros((out_features, )))\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return F.linear(input, self.weight.to(input.dtype), bias=self.bias.to(input.dtype) if self.bias is not None else None)\n",
        "\n",
        "\n",
        "class CastedEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_embeddings: int,\n",
        "                 embedding_dim: int,\n",
        "                 init_std: float,\n",
        "                 cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "\n",
        "        # Truncated LeCun normal init\n",
        "        self.embedding_weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std)\n",
        "        )\n",
        "        \n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return F.embedding(input, self.embedding_weight.to(self.cast_to))\n",
        "\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_position_embeddings, base, device=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # RoPE\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))\n",
        "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
        "        freqs = torch.outer(t, inv_freq)\n",
        "\n",
        "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.cos_cached = nn.Buffer(emb.cos(), persistent=False)\n",
        "        self.sin_cached = nn.Buffer(emb.sin(), persistent=False)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, head_dim, num_heads, num_key_value_heads, causal=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.head_dim = head_dim\n",
        "        self.output_size = head_dim * num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.num_key_value_heads = num_key_value_heads\n",
        "        self.causal = causal\n",
        "\n",
        "        self.qkv_proj = CastedLinear(self.hidden_size, (self.num_heads + 2 * self.num_key_value_heads) * self.head_dim, bias=False)\n",
        "        self.o_proj = CastedLinear(self.output_size, self.hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "        # hidden_states: [bs, seq_len, num_heads, head_dim]\n",
        "        qkv = self.qkv_proj(hidden_states)\n",
        "\n",
        "        # Split head\n",
        "        qkv = qkv.view(batch_size, seq_len, self.num_heads + 2 * self.num_key_value_heads, self.head_dim)\n",
        "        query = qkv[:, :, :self.num_heads]\n",
        "        key = qkv[:, :, self.num_heads: self.num_heads + self.num_key_value_heads]\n",
        "        value = qkv[:, :, self.num_heads + self.num_key_value_heads:]\n",
        "\n",
        "        # RoPE\n",
        "        if cos_sin is not None:\n",
        "            cos, sin = cos_sin\n",
        "            query, key = apply_rotary_pos_emb(query, key, cos, sin)\n",
        "\n",
        "        # flash attn\n",
        "        query, key, value = map(lambda t: einops.rearrange(t, 'B S H D -> B H S D'), (query, key, value)) # needed for scaled_dot_product_attention but not flash_attn_func\n",
        "        attn_output = scaled_dot_product_attention(query=query, key=key, value=value, is_causal=self.causal)\n",
        "        attn_output = einops.rearrange(attn_output, 'B H S D -> B S H D')\n",
        "        attn_output = attn_output.view(batch_size, seq_len, self.output_size)  # type: ignore\n",
        "        return self.o_proj(attn_output)\n",
        "\n",
        "class LinearSwish(nn.Module):\n",
        "    def __init__(self, hidden_size: int, reverse=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = CastedLinear(hidden_size, hidden_size, bias=False)\n",
        "        self.reverse = reverse\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.reverse:\n",
        "            return F.silu(self.linear(x))\n",
        "        else:\n",
        "            return self.linear(F.silu(x))\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, hidden_size: int, expansion: float):\n",
        "        super().__init__()\n",
        "        inter = _find_multiple(round(expansion * hidden_size * 2 / 3), 256)\n",
        "\n",
        "        self.gate_up_proj = CastedLinear(hidden_size, inter * 2, bias=False)\n",
        "        self.down_proj    = CastedLinear(inter, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
        "        return self.down_proj(F.silu(gate) * up)\n",
        "\n",
        "def rms_norm(hidden_states: torch.Tensor, variance_epsilon: float) -> torch.Tensor:\n",
        "    input_dtype = hidden_states.dtype\n",
        "    hidden_states = hidden_states.to(torch.float32)\n",
        "\n",
        "    variance = hidden_states.square().mean(-1, keepdim=True)\n",
        "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
        "    return hidden_states.to(input_dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Sparse Embedding (Non-distributed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "# import torch.distributed as dist  # Removed for single GPU\n",
        "from torch.optim.optimizer import Optimizer, ParamsT\n",
        "\n",
        "# trunc_normal_init_ defined in Part 3\n",
        "\n",
        "\n",
        "class CastedSparseEmbedding(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, batch_size: int, init_std: float, cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "\n",
        "        # Real Weights\n",
        "        # Truncated LeCun normal init\n",
        "        self.weights = nn.Buffer(\n",
        "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std), persistent=True\n",
        "        )\n",
        "\n",
        "        # Local weights and IDs\n",
        "        # Local embeddings, with gradient, not persistent\n",
        "        self.local_weights = nn.Buffer(torch.zeros(batch_size, embedding_dim, requires_grad=True), persistent=False)\n",
        "        # Local embedding IDs, not persistent\n",
        "        self.local_ids = nn.Buffer(torch.zeros(batch_size, dtype=torch.int32), persistent=False)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training:\n",
        "            # Test mode, no gradient\n",
        "            return self.weights[inputs].to(self.cast_to)\n",
        "            \n",
        "        # Training mode, fill puzzle embedding from weights\n",
        "        with torch.no_grad():\n",
        "            self.local_weights.copy_(self.weights[inputs])\n",
        "            self.local_ids.copy_(inputs)\n",
        "\n",
        "        return self.local_weights.to(self.cast_to)\n",
        "\n",
        "\n",
        "class CastedSparseEmbeddingSignSGD(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: ParamsT,\n",
        "\n",
        "        lr: Union[float, torch.Tensor] = 1e-3,\n",
        "        weight_decay: float = 1e-2,\n",
        "    ):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            world_size=1  # Single GPU, no distributed training\n",
        "        )\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad\n",
        "    def step(self, closure=None):  # type: ignore\n",
        "        for group in self.param_groups:\n",
        "            # Find the sparse embedding weights\n",
        "            local_weights_grad = None\n",
        "            local_ids = None\n",
        "            weights = None\n",
        "            \n",
        "            assert len(group[\"params\"]) == 3\n",
        "            for p in group[\"params\"]:\n",
        "                if p.requires_grad:\n",
        "                    local_weights_grad = p.grad\n",
        "                elif p.ndim == 1:\n",
        "                    local_ids = p\n",
        "                elif p.ndim == 2:\n",
        "                    weights = p\n",
        "                else:\n",
        "                    assert False\n",
        "                \n",
        "            assert local_ids is not None\n",
        "            assert weights is not None\n",
        "        \n",
        "            # Apply SignSGD\n",
        "            # Adam â‰ˆ SignSGD if gradient is very sparse\n",
        "            if local_weights_grad is not None:\n",
        "                _sparse_emb_signsgd_dist(\n",
        "                    local_weights_grad,\n",
        "                    local_ids,\n",
        "                    weights,\n",
        "                    \n",
        "                    lr=group[\"lr\"],\n",
        "                    weight_decay=group[\"weight_decay\"],\n",
        "                    world_size=group[\"world_size\"]\n",
        "                )\n",
        "\n",
        "\n",
        "def _sparse_emb_signsgd_dist(\n",
        "    local_weights_grad: torch.Tensor,\n",
        "    local_ids: torch.Tensor,\n",
        "    weights: torch.Tensor,\n",
        "    \n",
        "    lr: float,\n",
        "    weight_decay: float,\n",
        "    world_size: int = 1,  # Single GPU, not used but kept for compatibility\n",
        "    ) -> None:\n",
        "    N, D = local_weights_grad.shape\n",
        "    \n",
        "    # All-gather\n",
        "    all_weights_grad = local_weights_grad\n",
        "    all_ids = local_ids\n",
        "\n",
        "    if False:  # Single GPU\n",
        "        all_weights_grad = torch.empty((world_size * N, D), dtype=local_weights_grad.dtype, device=local_weights_grad.device)\n",
        "        all_ids = torch.empty(world_size * N,               dtype=local_ids.dtype,          device=local_ids.device)\n",
        "    \n",
        "        # dist.all_gather_into_tensor(all_weights_grad, local_weights_grad)  # Single GPU\n",
        "        dist.all_gather_into_tensor(all_ids,          local_ids)\n",
        "\n",
        "    # Unique\n",
        "    grad_ids, inv = all_ids.unique(return_inverse=True)\n",
        "\n",
        "    grad = torch.zeros((grad_ids.shape[0], D), dtype=all_weights_grad.dtype, device=all_weights_grad.device)\n",
        "    grad.scatter_add_(0, inv.unsqueeze(-1).expand(-1, D), all_weights_grad)\n",
        "\n",
        "    # SignSGD with decoupled weight decay\n",
        "    p = weights[grad_ids]\n",
        "\n",
        "    p.mul_(1.0 - lr * weight_decay).add_(torch.sign(grad), alpha=-lr)\n",
        "\n",
        "    # Write updated slices back\n",
        "    weights[grad_ids] = p\n",
        "\n",
        "\n",
        "# Distributed version (for compatibility with original code, but works for single GPU)\n",
        "class CastedSparseEmbeddingSignSGD_Distributed(Optimizer):\n",
        "    \"\"\"\n",
        "    Distributed version of CastedSparseEmbeddingSignSGD.\n",
        "    For single GPU, this is equivalent to CastedSparseEmbeddingSignSGD.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: ParamsT,\n",
        "        world_size: int,\n",
        "        lr: Union[float, torch.Tensor] = 1e-3,\n",
        "        weight_decay: float = 1e-2,\n",
        "    ):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            world_size=world_size\n",
        "        )\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad\n",
        "    def step(self, closure=None):  # type: ignore\n",
        "        for group in self.param_groups:\n",
        "            # Find the sparse embedding weights\n",
        "            local_weights_grad = None\n",
        "            local_ids = None\n",
        "            weights = None\n",
        "            \n",
        "            assert len(group[\"params\"]) == 3\n",
        "            for p in group[\"params\"]:\n",
        "                if p.requires_grad:\n",
        "                    local_weights_grad = p.grad\n",
        "                elif p.ndim == 1:\n",
        "                    local_ids = p\n",
        "                elif p.ndim == 2:\n",
        "                    weights = p\n",
        "                else:\n",
        "                    assert False\n",
        "                \n",
        "            assert local_ids is not None\n",
        "            assert weights is not None\n",
        "        \n",
        "            # Apply SignSGD\n",
        "            if local_weights_grad is not None:\n",
        "                _sparse_emb_signsgd_dist(\n",
        "                    local_weights_grad,\n",
        "                    local_ids,\n",
        "                    weights,\n",
        "                    lr=group[\"lr\"],\n",
        "                    weight_decay=group[\"weight_decay\"],\n",
        "                    world_size=group[\"world_size\"]\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any, Tuple, Dict, Sequence, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "IGNORE_LABEL_ID = -100\n",
        "\n",
        "\n",
        "def s(x, epsilon=1e-30):\n",
        "    return torch.where(\n",
        "        x<0,\n",
        "        1/(1-x+ epsilon),\n",
        "        x + 1\n",
        "    )\n",
        "\n",
        "\n",
        "def log_stablemax(x, dim=-1):\n",
        "    s_x = s(x)\n",
        "    return torch.log(s_x/torch.sum(s_x, dim=dim, keepdim=True))\n",
        "\n",
        "\n",
        "def stablemax_cross_entropy(logits, labels, ignore_index: int = -100, valid_mask=None):\n",
        "    logprobs = log_stablemax(logits.to(torch.float64), dim=-1)\n",
        "\n",
        "    if valid_mask is None:\n",
        "        valid_mask = (labels != ignore_index)\n",
        "    transformed_labels = torch.where(valid_mask, labels, 0)\n",
        "    prediction_logprobs = torch.gather(logprobs, index=transformed_labels.to(torch.long).unsqueeze(-1), dim=-1).squeeze(-1)\n",
        "\n",
        "    return -torch.where(valid_mask, prediction_logprobs, 0)\n",
        "\n",
        "\n",
        "def softmax_cross_entropy(logits, labels, ignore_index: int = -100):\n",
        "    # Cast logits to f32\n",
        "    # Flatten logits\n",
        "    return F.cross_entropy(logits.to(torch.float32).view(-1, logits.shape[-1]), labels.to(torch.long).view(-1), ignore_index=ignore_index, reduction=\"none\").view(labels.shape)\n",
        "\n",
        "\n",
        "class ACTLossHead(nn.Module):\n",
        "    def __init__(self, model: nn.Module, loss_type: str):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.loss_fn = globals()[loss_type]\n",
        "        \n",
        "    def initial_carry(self, *args, **kwargs):\n",
        "        return self.model.initial_carry(*args, **kwargs)  # type: ignore\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        return_keys: Sequence[str],\n",
        "        # Model args\n",
        "        **model_kwargs,\n",
        "    ) -> Tuple[Any, torch.Tensor, Dict[str, torch.Tensor], Optional[Dict[str, torch.Tensor]], torch.Tensor]:\n",
        "        # Model logits\n",
        "        # B x SeqLen x D\n",
        "        new_carry, outputs = self.model(**model_kwargs)\n",
        "        labels = new_carry.current_data[\"labels\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Preds\n",
        "            outputs[\"preds\"] = torch.argmax(outputs[\"logits\"], dim=-1)\n",
        "\n",
        "            # Correctness\n",
        "            mask = (labels != IGNORE_LABEL_ID)\n",
        "            loss_counts = mask.sum(-1)\n",
        "            loss_divisor = loss_counts.clamp_min(1).unsqueeze(-1)  # Avoid NaNs in division\n",
        "\n",
        "            is_correct = mask & (torch.argmax(outputs[\"logits\"], dim=-1) == labels)\n",
        "            seq_is_correct = is_correct.sum(-1) == loss_counts\n",
        "            \n",
        "            # Metrics (halted)\n",
        "            valid_metrics = new_carry.halted & (loss_counts > 0)\n",
        "            metrics = {\n",
        "                \"count\": valid_metrics.sum(),\n",
        "                \n",
        "                \"accuracy\":       torch.where(valid_metrics, (is_correct.to(torch.float32) / loss_divisor).sum(-1), 0).sum(),\n",
        "                \"exact_accuracy\": (valid_metrics & seq_is_correct).sum(),\n",
        "\n",
        "                \"q_halt_accuracy\": (valid_metrics & ((outputs[\"q_halt_logits\"] >= 0) == seq_is_correct)).sum(),\n",
        "                \"steps\":          torch.where(valid_metrics, new_carry.steps, 0).sum(),\n",
        "            }\n",
        "\n",
        "        # Losses\n",
        "\n",
        "        lm_loss = (self.loss_fn(outputs[\"logits\"], labels, ignore_index=IGNORE_LABEL_ID, valid_mask=mask) / loss_divisor).sum()\n",
        "        q_halt_loss = F.binary_cross_entropy_with_logits(outputs[\"q_halt_logits\"], seq_is_correct.to(outputs[\"q_halt_logits\"].dtype), reduction=\"sum\")\n",
        "        metrics.update({\n",
        "            \"lm_loss\": lm_loss.detach(),\n",
        "            \"q_halt_loss\": q_halt_loss.detach(),\n",
        "        })\n",
        "        # Q continue (bootstrapping target loss); Alexia: This fits Q-learning, but seems totally unecessary\n",
        "        q_continue_loss = 0\n",
        "        if \"target_q_continue\" in outputs:\n",
        "            q_continue_loss = F.binary_cross_entropy_with_logits(outputs[\"q_continue_logits\"], outputs[\"target_q_continue\"], reduction=\"sum\")\n",
        "\n",
        "            metrics[\"q_continue_loss\"] = q_continue_loss.detach()\n",
        "        # Filter outputs for return\n",
        "        detached_outputs = {k: outputs[k].detach() for k in return_keys if k in outputs}\n",
        "\n",
        "        return new_carry, lm_loss + 0.5 * (q_halt_loss + q_continue_loss), metrics, detached_outputs, new_carry.halted.all()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: TRM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Tuple, List, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "import torch\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from pydantic import BaseModel\n",
        "import random\n",
        "# trunc_normal_init_ defined in Part 3\n",
        "# All layer classes defined in Part 4\n",
        "# CastedSparseEmbedding defined in Part 5\n",
        "\n",
        "IGNORE_LABEL_ID = -100\n",
        "\n",
        "@dataclass\n",
        "class TinyRecursiveReasoningModel_ACTV1InnerCarry:\n",
        "    z_H: torch.Tensor\n",
        "    z_L: torch.Tensor\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TinyRecursiveReasoningModel_ACTV1Carry:\n",
        "    inner_carry: TinyRecursiveReasoningModel_ACTV1InnerCarry\n",
        "    \n",
        "    steps: torch.Tensor\n",
        "    halted: torch.Tensor\n",
        "    \n",
        "    current_data: Dict[str, torch.Tensor]\n",
        "\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1Config(BaseModel):\n",
        "    batch_size: int\n",
        "    seq_len: int\n",
        "    puzzle_emb_ndim: int = 0\n",
        "    num_puzzle_identifiers: int\n",
        "    vocab_size: int\n",
        "\n",
        "    H_cycles: int\n",
        "    L_cycles: int\n",
        "\n",
        "    H_layers: int # ignored\n",
        "    L_layers: int\n",
        "\n",
        "    # Transformer config\n",
        "    hidden_size: int\n",
        "    expansion: float\n",
        "    num_heads: int\n",
        "    pos_encodings: str\n",
        "\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    rope_theta: float = 10000.0\n",
        "    \n",
        "    # Halting Q-learning config\n",
        "    halt_max_steps: int\n",
        "    halt_exploration_prob: float\n",
        "\n",
        "    forward_dtype: str = \"bfloat16\"\n",
        "\n",
        "    # Alexia: added\n",
        "    mlp_t: bool = False # use mlp on L instead of transformer\n",
        "    puzzle_emb_len: int = 16 # if non-zero, its specified to this value\n",
        "    no_ACT_continue: bool =  True # No continue ACT loss, only use the sigmoid of the halt which makes much more sense\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1Block(nn.Module):\n",
        "    def __init__(self, config: TinyRecursiveReasoningModel_ACTV1Config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        if self.config.mlp_t:\n",
        "            self.puzzle_emb_len = -(self.config.puzzle_emb_ndim // -self.config.hidden_size) if self.config.puzzle_emb_len == 0 else self.config.puzzle_emb_len\n",
        "            self.mlp_t = SwiGLU(\n",
        "                hidden_size=self.config.seq_len + self.puzzle_emb_len, # L\n",
        "                expansion=config.expansion,\n",
        "            )\n",
        "        else:\n",
        "            self.self_attn = Attention(\n",
        "                hidden_size=config.hidden_size,\n",
        "                head_dim=config.hidden_size // config.num_heads,\n",
        "                num_heads=config.num_heads,\n",
        "                num_key_value_heads=config.num_heads,\n",
        "                causal=False\n",
        "            )\n",
        "        self.mlp = SwiGLU(\n",
        "            hidden_size=config.hidden_size,\n",
        "            expansion=config.expansion,\n",
        "        )\n",
        "        self.norm_eps = config.rms_norm_eps\n",
        "\n",
        "    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        # B, L, D = hidden_states.shape\n",
        "        # Post Norm\n",
        "        if self.config.mlp_t:\n",
        "            hidden_states = hidden_states.transpose(1,2)\n",
        "            out = self.mlp_t(hidden_states)\n",
        "            hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "            hidden_states = hidden_states.transpose(1,2)\n",
        "        else:\n",
        "            # Self Attention\n",
        "            hidden_states = rms_norm(hidden_states + self.self_attn(cos_sin=cos_sin, hidden_states=hidden_states), variance_epsilon=self.norm_eps)\n",
        "        # Fully Connected\n",
        "        out = self.mlp(hidden_states)\n",
        "        hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "        return hidden_states\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1ReasoningModule(nn.Module):\n",
        "    def __init__(self, layers: List[TinyRecursiveReasoningModel_ACTV1Block]):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_injection: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "        hidden_states = hidden_states + input_injection\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states=hidden_states, **kwargs)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1_Inner(nn.Module):\n",
        "    def __init__(self, config: TinyRecursiveReasoningModel_ACTV1Config) -> None:\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.forward_dtype = getattr(torch, self.config.forward_dtype)\n",
        "\n",
        "        # I/O\n",
        "\n",
        "        self.embed_scale = math.sqrt(self.config.hidden_size)\n",
        "        embed_init_std = 1.0 / self.embed_scale\n",
        "\n",
        "        self.embed_tokens = CastedEmbedding(self.config.vocab_size, self.config.hidden_size, init_std=embed_init_std, cast_to=self.forward_dtype)\n",
        "        self.lm_head      = CastedLinear(self.config.hidden_size, self.config.vocab_size, bias=False)\n",
        "        self.q_head       = CastedLinear(self.config.hidden_size, 2, bias=True)\n",
        "\n",
        "        self.puzzle_emb_len = -(self.config.puzzle_emb_ndim // -self.config.hidden_size)  if self.config.puzzle_emb_len == 0 else self.config.puzzle_emb_len  # ceil div\n",
        "        if self.config.puzzle_emb_ndim > 0:\n",
        "            # Zero init puzzle embeddings\n",
        "            self.puzzle_emb = CastedSparseEmbedding(self.config.num_puzzle_identifiers, self.config.puzzle_emb_ndim,\n",
        "                                                    batch_size=self.config.batch_size, init_std=0, cast_to=self.forward_dtype)\n",
        "\n",
        "        # LM Blocks\n",
        "        if self.config.pos_encodings == \"rope\":\n",
        "            self.rotary_emb = RotaryEmbedding(dim=self.config.hidden_size // self.config.num_heads,\n",
        "                                              max_position_embeddings=self.config.seq_len + self.puzzle_emb_len,\n",
        "                                              base=self.config.rope_theta)\n",
        "        elif self.config.pos_encodings == \"learned\":\n",
        "            self.embed_pos = CastedEmbedding(self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, init_std=embed_init_std, cast_to=self.forward_dtype)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        # Reasoning Layers\n",
        "        self.L_level = TinyRecursiveReasoningModel_ACTV1ReasoningModule(layers=[TinyRecursiveReasoningModel_ACTV1Block(self.config) for _i in range(self.config.L_layers)])\n",
        "\n",
        "        # Initial states\n",
        "        self.H_init = nn.Buffer(trunc_normal_init_(torch.empty(self.config.hidden_size, dtype=self.forward_dtype), std=1), persistent=True)\n",
        "        self.L_init = nn.Buffer(trunc_normal_init_(torch.empty(self.config.hidden_size, dtype=self.forward_dtype), std=1), persistent=True)\n",
        "\n",
        "        # Q head special init\n",
        "        # Init Q to (almost) zero for faster learning during bootstrapping\n",
        "        with torch.no_grad():\n",
        "            self.q_head.weight.zero_()\n",
        "            self.q_head.bias.fill_(-5)  # type: ignore\n",
        "\n",
        "    def _input_embeddings(self, input: torch.Tensor, puzzle_identifiers: torch.Tensor):\n",
        "        # Token embedding\n",
        "        embedding = self.embed_tokens(input.to(torch.int32))\n",
        "\n",
        "        # Puzzle embeddings\n",
        "        if self.config.puzzle_emb_ndim > 0:\n",
        "            puzzle_embedding = self.puzzle_emb(puzzle_identifiers)\n",
        "            \n",
        "            pad_count = self.puzzle_emb_len * self.config.hidden_size - puzzle_embedding.shape[-1]\n",
        "            if pad_count > 0:\n",
        "                puzzle_embedding = F.pad(puzzle_embedding, (0, pad_count))\n",
        "\n",
        "            embedding = torch.cat((puzzle_embedding.view(-1, self.puzzle_emb_len, self.config.hidden_size), embedding), dim=-2)\n",
        "\n",
        "        # Position embeddings\n",
        "        if self.config.pos_encodings == \"learned\":\n",
        "            # scale by 1/sqrt(2) to maintain forward variance\n",
        "            embedding = 0.707106781 * (embedding + self.embed_pos.embedding_weight.to(self.forward_dtype))\n",
        "\n",
        "        # Scale\n",
        "        return self.embed_scale * embedding\n",
        "\n",
        "    def empty_carry(self, batch_size: int):\n",
        "        return TinyRecursiveReasoningModel_ACTV1InnerCarry(\n",
        "            z_H=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype),\n",
        "            z_L=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype),\n",
        "        )\n",
        "        \n",
        "    def reset_carry(self, reset_flag: torch.Tensor, carry: TinyRecursiveReasoningModel_ACTV1InnerCarry):\n",
        "        return TinyRecursiveReasoningModel_ACTV1InnerCarry(\n",
        "            z_H=torch.where(reset_flag.view(-1, 1, 1), self.H_init, carry.z_H),\n",
        "            z_L=torch.where(reset_flag.view(-1, 1, 1), self.L_init, carry.z_L),\n",
        "        )\n",
        "\n",
        "    def forward(self, carry: TinyRecursiveReasoningModel_ACTV1InnerCarry, batch: Dict[str, torch.Tensor]) -> Tuple[TinyRecursiveReasoningModel_ACTV1InnerCarry, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        seq_info = dict(\n",
        "            cos_sin=self.rotary_emb() if hasattr(self, \"rotary_emb\") else None,\n",
        "        )\n",
        "\n",
        "        # Input encoding\n",
        "        input_embeddings = self._input_embeddings(batch[\"inputs\"], batch[\"puzzle_identifiers\"])\n",
        "\n",
        "        # Forward iterations\n",
        "        it = 0\n",
        "        z_H, z_L = carry.z_H, carry.z_L\n",
        "        # H_cycles-1 without grad\n",
        "        with torch.no_grad():\n",
        "            for _H_step in range(self.config.H_cycles-1):\n",
        "                for _L_step in range(self.config.L_cycles):\n",
        "                    z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "                z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "        # 1 with grad\n",
        "        for _L_step in range(self.config.L_cycles):\n",
        "            z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "        z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "\n",
        "        # LM Outputs\n",
        "        new_carry = TinyRecursiveReasoningModel_ACTV1InnerCarry(z_H=z_H.detach(), z_L=z_L.detach())  # New carry no grad\n",
        "        output = self.lm_head(z_H)[:, self.puzzle_emb_len:]\n",
        "        q_logits = self.q_head(z_H[:, 0]).to(torch.float32) # Q-head; uses the first puzzle_emb position\n",
        "        return new_carry, output, (q_logits[..., 0], q_logits[..., 1])\n",
        "\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1(nn.Module):\n",
        "    \"\"\"ACT wrapper.\"\"\"\n",
        "\n",
        "    def __init__(self, config_dict: dict):\n",
        "        super().__init__()\n",
        "        self.config = TinyRecursiveReasoningModel_ACTV1Config(**config_dict)\n",
        "        self.inner = TinyRecursiveReasoningModel_ACTV1_Inner(self.config)\n",
        "\n",
        "    @property\n",
        "    def puzzle_emb(self):\n",
        "        return self.inner.puzzle_emb\n",
        "\n",
        "    def initial_carry(self, batch: Dict[str, torch.Tensor]):\n",
        "        batch_size = batch[\"inputs\"].shape[0]\n",
        "\n",
        "        return TinyRecursiveReasoningModel_ACTV1Carry(\n",
        "            inner_carry=self.inner.empty_carry(batch_size),  # Empty is expected, it will be reseted in first pass as all sequences are halted.\n",
        "            \n",
        "            steps=torch.zeros((batch_size, ), dtype=torch.int32),\n",
        "            halted=torch.ones((batch_size, ), dtype=torch.bool),  # Default to halted\n",
        "            \n",
        "            current_data={k: torch.empty_like(v) for k, v in batch.items()}\n",
        "        )\n",
        "        \n",
        "    def forward(self, carry: TinyRecursiveReasoningModel_ACTV1Carry, batch: Dict[str, torch.Tensor]) -> Tuple[TinyRecursiveReasoningModel_ACTV1Carry, Dict[str, torch.Tensor]]:\n",
        "\n",
        "        # Update data, carry (removing halted sequences)\n",
        "        new_inner_carry = self.inner.reset_carry(carry.halted, carry.inner_carry)\n",
        "        \n",
        "        new_steps = torch.where(carry.halted, 0, carry.steps)\n",
        "\n",
        "        new_current_data = {k: torch.where(carry.halted.view((-1, ) + (1, ) * (batch[k].ndim - 1)), batch[k], v) for k, v in carry.current_data.items()}\n",
        "\n",
        "        # Forward inner model\n",
        "        new_inner_carry, logits, (q_halt_logits, q_continue_logits) = self.inner(new_inner_carry, new_current_data)\n",
        "\n",
        "        outputs = {\n",
        "            \"logits\": logits,\n",
        "            \"q_halt_logits\": q_halt_logits,\n",
        "            \"q_continue_logits\": q_continue_logits\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Step\n",
        "            new_steps = new_steps + 1\n",
        "            is_last_step = new_steps >= self.config.halt_max_steps\n",
        "            \n",
        "            halted = is_last_step\n",
        "\n",
        "            # if training, and ACT is enabled\n",
        "            if self.training and (self.config.halt_max_steps > 1):\n",
        "\n",
        "                # Halt signal\n",
        "                # NOTE: During evaluation, always use max steps, this is to guarantee the same halting steps inside a batch for batching purposes\n",
        "                \n",
        "                if self.config.no_ACT_continue:\n",
        "                    halted = halted | (q_halt_logits > 0)\n",
        "                else:\n",
        "                    halted = halted | (q_halt_logits > q_continue_logits)\n",
        "\n",
        "                # Exploration\n",
        "                min_halt_steps = (torch.rand_like(q_halt_logits) < self.config.halt_exploration_prob) * torch.randint_like(new_steps, low=2, high=self.config.halt_max_steps + 1)\n",
        "                halted = halted & (new_steps >= min_halt_steps)\n",
        "\n",
        "                if not self.config.no_ACT_continue:\n",
        "                    # Compute target Q\n",
        "                    # NOTE: No replay buffer and target networks for computing target Q-value.\n",
        "                    # As batch_size is large, there're many parallel envs.\n",
        "                    # Similar concept as PQN https://arxiv.org/abs/2407.04811\n",
        "                    _, _, (next_q_halt_logits, next_q_continue_logits), _, _ = self.inner(new_inner_carry, new_current_data)\n",
        "                    outputs[\"target_q_continue\"] = torch.sigmoid(torch.where(is_last_step, next_q_halt_logits, torch.maximum(next_q_halt_logits, next_q_continue_logits)))\n",
        "\n",
        "        return TinyRecursiveReasoningModel_ACTV1Carry(new_inner_carry, new_steps, halted, new_current_data), outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Dataset Common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "import pydantic\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Global list mapping each dihedral transform id to its inverse.\n",
        "# Index corresponds to the original tid, and the value is its inverse.\n",
        "DIHEDRAL_INVERSE = [0, 3, 2, 1, 4, 5, 6, 7]\n",
        "\n",
        "\n",
        "class PuzzleDatasetMetadata(pydantic.BaseModel):\n",
        "    pad_id: int\n",
        "    ignore_label_id: Optional[int]\n",
        "    blank_identifier_id: int\n",
        "    vocab_size: int\n",
        "    seq_len: int\n",
        "    num_puzzle_identifiers: int\n",
        "    total_groups: int\n",
        "    mean_puzzle_examples: float\n",
        "    total_puzzles: int\n",
        "    sets: List[str]\n",
        "\n",
        "\n",
        "def dihedral_transform(arr: np.ndarray, tid: int) -> np.ndarray:\n",
        "    \"\"\"8 dihedral symmetries by rotate, flip and mirror\"\"\"\n",
        "    \n",
        "    if tid == 0:\n",
        "        return arr  # identity\n",
        "    elif tid == 1:\n",
        "        return np.rot90(arr, k=1)\n",
        "    elif tid == 2:\n",
        "        return np.rot90(arr, k=2)\n",
        "    elif tid == 3:\n",
        "        return np.rot90(arr, k=3)\n",
        "    elif tid == 4:\n",
        "        return np.fliplr(arr)       # horizontal flip\n",
        "    elif tid == 5:\n",
        "        return np.flipud(arr)       # vertical flip\n",
        "    elif tid == 6:\n",
        "        return arr.T                # transpose (reflection along main diagonal)\n",
        "    elif tid == 7:\n",
        "        return np.fliplr(np.rot90(arr, k=1))  # anti-diagonal reflection\n",
        "    else:\n",
        "        return arr\n",
        "    \n",
        "    \n",
        "def inverse_dihedral_transform(arr: np.ndarray, tid: int) -> np.ndarray:\n",
        "    return dihedral_transform(arr, DIHEDRAL_INVERSE[tid])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Puzzle Dataset (Non-distributed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import numpy as np\n",
        "import pydantic\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset, get_worker_info\n",
        "\n",
        "# IGNORE_LABEL_ID defined in Part 6\n",
        "# PuzzleDatasetMetadata defined in Part 8\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "def _sample_batch(rng: np.random.Generator, group_order: np.ndarray, puzzle_indices: np.ndarray, group_indices: np.ndarray, start_index: int, global_batch_size: int):\n",
        "    # Pack examples into a full batch\n",
        "    batch = []\n",
        "    batch_puzzle_indices = []\n",
        "    current_size = 0\n",
        "\n",
        "    while (start_index < group_order.size) and (current_size < global_batch_size):\n",
        "        # Pick a group and a puzzle from that group\n",
        "        group_id = group_order[start_index]\n",
        "        puzzle_id = rng.integers(group_indices[group_id], group_indices[group_id + 1])\n",
        "        start_index += 1\n",
        "\n",
        "        # Get range of the puzzle\n",
        "        puzzle_start = puzzle_indices[puzzle_id]\n",
        "        puzzle_size = int(puzzle_indices[puzzle_id + 1] - puzzle_start)\n",
        "\n",
        "        append_size = min(puzzle_size, global_batch_size - current_size)\n",
        "\n",
        "        # Put into batch\n",
        "        batch_puzzle_indices.append(np.full(append_size, puzzle_id, dtype=np.int32))\n",
        "        batch.append(puzzle_start + np.random.choice(puzzle_size, append_size, replace=False))\n",
        "\n",
        "        current_size += append_size\n",
        "\n",
        "    return start_index, np.concatenate(batch), np.concatenate(batch_puzzle_indices)\n",
        "\n",
        "\n",
        "class PuzzleDatasetConfig(pydantic.BaseModel):\n",
        "    seed: int\n",
        "    dataset_paths: List[str]\n",
        "    global_batch_size: int\n",
        "    test_set_mode: bool\n",
        "    epochs_per_iter: int  # Batch X epochs in an iteration to reduce overhead.\n",
        "    rank: int = 0  # Single GPU\n",
        "    num_replicas: int = 1  # Single GPU\n",
        "class PuzzleDataset(IterableDataset):\n",
        "    def __init__(self, config: PuzzleDatasetConfig, split: str = \"train\"):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.split = split\n",
        "\n",
        "        # Merge multiple metadata\n",
        "        prev_seq_len = None\n",
        "        prev_vocab_size = None\n",
        "        prev_pad_id = None\n",
        "        prev_ignore_label_id = None\n",
        "        prev_blank_identifier_id = None\n",
        "        prev_sets = None\n",
        "        prev_num_identifiers = None\n",
        "        mean_puzzle_examples = 0\n",
        "        total_puzzles = 0\n",
        "        total_groups = 0\n",
        "        num_identifiers = 0\n",
        "        for dataset_path in config.dataset_paths:\n",
        "            current_metadata = self._load_metadata(dataset_path)\n",
        "            if prev_seq_len is None:\n",
        "                prev_seq_len = current_metadata.seq_len\n",
        "                prev_vocab_size = current_metadata.vocab_size\n",
        "                prev_pad_id = current_metadata.pad_id\n",
        "                prev_ignore_label_id = current_metadata.ignore_label_id\n",
        "                prev_blank_identifier_id = current_metadata.blank_identifier_id\n",
        "                prev_sets = current_metadata.sets\n",
        "                prev_num_identifiers = current_metadata.num_puzzle_identifiers\n",
        "            else:\n",
        "                assert prev_seq_len == current_metadata.seq_len\n",
        "                assert prev_vocab_size == current_metadata.vocab_size\n",
        "                assert prev_pad_id == current_metadata.pad_id\n",
        "                assert prev_ignore_label_id == current_metadata.ignore_label_id\n",
        "                assert prev_blank_identifier_id == current_metadata.blank_identifier_id\n",
        "                assert prev_sets == current_metadata.sets\n",
        "                assert prev_num_identifiers == current_metadata.num_puzzle_identifiers\n",
        "            mean_puzzle_examples += current_metadata.mean_puzzle_examples*current_metadata.total_puzzles\n",
        "            total_puzzles += current_metadata.total_puzzles\n",
        "            total_groups += current_metadata.total_groups\n",
        "            num_identifiers += current_metadata.num_puzzle_identifiers\n",
        "        mean_puzzle_examples = mean_puzzle_examples / total_puzzles\n",
        "\n",
        "        self.metadata = PuzzleDatasetMetadata(\n",
        "            seq_len=prev_seq_len,\n",
        "            vocab_size=prev_vocab_size,\n",
        "            pad_id=prev_pad_id,\n",
        "            ignore_label_id=prev_ignore_label_id,\n",
        "            blank_identifier_id=prev_blank_identifier_id,\n",
        "            num_puzzle_identifiers=num_identifiers,\n",
        "            total_groups=total_groups,\n",
        "            mean_puzzle_examples=mean_puzzle_examples,\n",
        "            total_puzzles=total_puzzles,\n",
        "            sets=prev_sets\n",
        "        )\n",
        "\n",
        "        # Checks\n",
        "        assert self.config.global_batch_size % self.config.num_replicas == 0, f\"Global batch size {self.config.global_batch_size} must be multiples of nodes {self.config.num_replicas}.\"\n",
        "        self.local_batch_size = self.config.global_batch_size // self.config.num_replicas\n",
        "\n",
        "        # State\n",
        "        self._data = None\n",
        "        self._iters = 0\n",
        "\n",
        "    def _load_metadata(self, dataset_path) -> PuzzleDatasetMetadata:\n",
        "        with open(os.path.join(dataset_path, self.split, \"dataset.json\"), \"r\") as f:\n",
        "            return PuzzleDatasetMetadata(**json.load(f))\n",
        "\n",
        "    def _lazy_load_dataset(self):\n",
        "        if self._data is not None:\n",
        "            return\n",
        "\n",
        "        field_mmap_modes = {\n",
        "            \"inputs\": \"r\",\n",
        "            \"labels\": \"r\",\n",
        "\n",
        "            # Keep indices in memory\n",
        "            \"puzzle_identifiers\": None,\n",
        "            \"puzzle_indices\": None,\n",
        "            \"group_indices\": None\n",
        "        }\n",
        "\n",
        "        # Load data\n",
        "        self._data = {}\n",
        "        for set_name in self.metadata.sets: # Load subset\n",
        "            for i, dataset_path in enumerate(self.config.dataset_paths):\n",
        "                if i > 0:\n",
        "                    set_name_ = set_name + str(i)\n",
        "                else:\n",
        "                    set_name_ = set_name\n",
        "                self._data[set_name_] = {\n",
        "                    field_name: np.load(os.path.join(dataset_path, self.split, f\"{set_name}__{field_name}.npy\"), mmap_mode=mmap_mode)\n",
        "                    for field_name, mmap_mode in field_mmap_modes.items()\n",
        "                }\n",
        "\n",
        "\n",
        "    def _collate_batch(self, batch):\n",
        "        # Convert dtype\n",
        "        batch = {k: v.astype(np.int32) for k, v in batch.items()}\n",
        "\n",
        "        # Convert ignore label IDs\n",
        "        if self.metadata.ignore_label_id is not None:\n",
        "            batch[\"labels\"][batch[\"labels\"] == self.metadata.ignore_label_id] = IGNORE_LABEL_ID\n",
        "\n",
        "        # Pad\n",
        "        if batch[\"puzzle_identifiers\"].size < self.local_batch_size:\n",
        "            pad_size = self.local_batch_size - batch[\"puzzle_identifiers\"].size\n",
        "            pad_values = {\n",
        "                \"inputs\": self.metadata.pad_id,\n",
        "                \"labels\": IGNORE_LABEL_ID,\n",
        "                \"puzzle_identifiers\": self.metadata.blank_identifier_id\n",
        "            }\n",
        "            batch = {k: np.pad(v, ((0, pad_size), ) + ((0, 0), ) * (v.ndim - 1), constant_values=pad_values[k]) for k, v in batch.items()}\n",
        "\n",
        "        # To tensor\n",
        "        return {k: torch.from_numpy(v) for k, v in batch.items()}\n",
        "    \n",
        "    def _iter_test(self):\n",
        "        for set_i, (set_name, dataset) in enumerate(self._data.items()):  # type: ignore\n",
        "            total_examples = len(dataset[\"inputs\"])\n",
        "\n",
        "            # Load examples one by one\n",
        "            start_index = 0\n",
        "            while start_index < total_examples:\n",
        "                # Compute indices\n",
        "                end_index = min(total_examples, start_index + self.config.global_batch_size)\n",
        "                \n",
        "                local_start = start_index + 0  # Single GPU\n",
        "                local_end   = min(start_index + self.local_batch_size, end_index)  # Single GPU\n",
        "                \n",
        "                # Get batch of examples, and also puzzle IDs\n",
        "                puzzle_indices = []\n",
        "                puzzle_index = np.searchsorted(dataset[\"puzzle_indices\"], local_start, side=\"right\") - 1\n",
        "                for i in range(local_start, local_end):\n",
        "                    while puzzle_index + 1 < len(dataset[\"puzzle_indices\"]) and i >= dataset[\"puzzle_indices\"][puzzle_index + 1]:\n",
        "                        puzzle_index += 1\n",
        "\n",
        "                    puzzle_indices.append(puzzle_index)\n",
        "                \n",
        "                batch = self._collate_batch({\n",
        "                    \"inputs\": dataset[\"inputs\"][local_start: local_end],\n",
        "                    \"labels\": dataset[\"labels\"][local_start: local_end],\n",
        "                    \"puzzle_identifiers\": dataset[\"puzzle_identifiers\"][puzzle_indices]\n",
        "                })\n",
        "\n",
        "                yield set_name, batch, end_index - start_index\n",
        "                \n",
        "                # Advance to next batch\n",
        "                start_index += self.config.global_batch_size\n",
        "\n",
        "    def _iter_train(self):\n",
        "        for set_name, dataset in self._data.items():  # type: ignore\n",
        "            # Increase epoch count\n",
        "            self._iters += 1\n",
        "\n",
        "            # Randomly shuffle groups\n",
        "            rng = np.random.Generator(np.random.Philox(seed=self.config.seed + self._iters))\n",
        "\n",
        "            group_order = np.concatenate([rng.permutation(dataset[\"group_indices\"].size - 1) for _i in range(self.config.epochs_per_iter)])\n",
        "            start_index = 0\n",
        "            \n",
        "            while start_index < group_order.size:\n",
        "                start_index, batch_indices, batch_puzzle_indices = _sample_batch(\n",
        "                    rng,\n",
        "                    group_order=group_order,\n",
        "                    puzzle_indices=dataset[\"puzzle_indices\"],\n",
        "                    group_indices=dataset[\"group_indices\"],\n",
        "                    start_index=start_index,\n",
        "                    global_batch_size=self.config.global_batch_size,\n",
        "                )\n",
        "\n",
        "                # Select current rank and collate\n",
        "                global_effective_batch_size = batch_puzzle_indices.size  # Global effective batch size, excluding pads\n",
        "\n",
        "                # Drop last batch\n",
        "                if global_effective_batch_size < self.config.global_batch_size:\n",
        "                    break\n",
        "\n",
        "                batch_indices        = batch_indices       [0: self.local_batch_size]  # Single GPU\n",
        "                batch_puzzle_indices = batch_puzzle_indices[0: self.local_batch_size]  # Single GPU\n",
        "                batch = self._collate_batch({\n",
        "                    \"inputs\": dataset[\"inputs\"][batch_indices],\n",
        "                    \"labels\": dataset[\"labels\"][batch_indices],\n",
        "                    \"puzzle_identifiers\": dataset[\"puzzle_identifiers\"][batch_puzzle_indices]\n",
        "                })\n",
        "\n",
        "                yield set_name, batch, global_effective_batch_size\n",
        "                \n",
        "    def __iter__(self):\n",
        "        worker_info = get_worker_info()\n",
        "        assert worker_info is None or worker_info.num_workers == 1, \"Multithreaded data loading is not currently supported.\"\n",
        "        \n",
        "        self._lazy_load_dataset()\n",
        "        \n",
        "        # Iterate using specified mode\n",
        "        if self.config.test_set_mode:\n",
        "            yield from self._iter_test()\n",
        "        else:\n",
        "            yield from self._iter_train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: EMA Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch.nn as nn\n",
        "\n",
        "class EMAHelper(object):\n",
        "    def __init__(self, mu=0.999):\n",
        "        self.mu = mu\n",
        "        self.shadow = {}\n",
        "\n",
        "    def register(self, module):\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self, module):\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name].data = (1. - self.mu) * param.data + self.mu * self.shadow[name].data\n",
        "\n",
        "    def ema(self, module):\n",
        "        if isinstance(module, nn.DataParallel):\n",
        "            module = module.module\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data.copy_(self.shadow[name].data)\n",
        "\n",
        "    def ema_copy(self, module):\n",
        "        module_copy = copy.deepcopy(module)\n",
        "        self.ema(module_copy)\n",
        "        return module_copy\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.shadow\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.shadow = state_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 11: Utils Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import inspect\n",
        "\n",
        "\n",
        "# Class registry for notebook environment\n",
        "_MODEL_CLASS_REGISTRY = {}\n",
        "\n",
        "def register_model_class(identifier: str, cls):\n",
        "    \"\"\"Register a model class for notebook environment.\"\"\"\n",
        "    _MODEL_CLASS_REGISTRY[identifier] = cls\n",
        "\n",
        "def load_model_class(identifier: str, prefix: str = \"models.\"):\n",
        "    \"\"\"Load model class from identifier. Works in notebook environment.\"\"\"\n",
        "    # Check registry first\n",
        "    if identifier in _MODEL_CLASS_REGISTRY:\n",
        "        return _MODEL_CLASS_REGISTRY[identifier]\n",
        "    \n",
        "    module_path, class_name = identifier.split('@')\n",
        "    \n",
        "    # Map common identifiers to class names\n",
        "    class_name_map = {\n",
        "        'TinyRecursiveReasoningModel_ACTV1': 'TinyRecursiveReasoningModel_ACTV1',\n",
        "        'ACTLossHead': 'ACTLossHead',\n",
        "    }\n",
        "    \n",
        "    # Get from global namespace (notebook environment)\n",
        "    import sys\n",
        "    frame = sys._getframe(1)\n",
        "    globals_dict = frame.f_globals\n",
        "    \n",
        "    # Try to find class in globals\n",
        "    if class_name in globals_dict:\n",
        "        cls = globals_dict[class_name]\n",
        "        if isinstance(cls, type):\n",
        "            return cls\n",
        "    \n",
        "    # Fallback: try importing (for evaluators)\n",
        "    try:\n",
        "        if prefix.startswith('evaluators.'):\n",
        "            # For evaluators, try to import\n",
        "            module = importlib.import_module(prefix + module_path)\n",
        "            return getattr(module, class_name)\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    raise ValueError(f'Class {class_name} not found. Make sure all cells are executed in order.')\n",
        "\n",
        "def get_model_source_path(identifier: str, prefix: str = \"models.\"):\n",
        "    \"\"\"Get source path. In notebook, return None.\"\"\"\n",
        "    # In notebook environment, we don't have source files\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 12: Training Framework (Non-distributed, H100 Optimized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Part 12: Training Framework (Non-distributed, H100 Optimized)\n",
        "# ============================================================================\n",
        "\n",
        "# Configuration classes\n",
        "class LossConfig(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra='allow')\n",
        "    name: str\n",
        "\n",
        "class ArchConfig(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra='allow')\n",
        "    name: str\n",
        "    loss: LossConfig\n",
        "\n",
        "class EvaluatorConfig(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra=\"allow\")\n",
        "    name: str\n",
        "\n",
        "class PretrainConfig(pydantic.BaseModel):\n",
        "    # Config\n",
        "    arch: ArchConfig\n",
        "    # Data\n",
        "    data_paths: List[str]\n",
        "    data_paths_test: List[str] = []\n",
        "    # Evaluators\n",
        "    evaluators: List[EvaluatorConfig] = []\n",
        "\n",
        "    # Hyperparams\n",
        "    global_batch_size: int\n",
        "    epochs: int\n",
        "\n",
        "    lr: float\n",
        "    lr_min_ratio: float\n",
        "    lr_warmup_steps: int\n",
        "\n",
        "    weight_decay: float\n",
        "    beta1: float\n",
        "    beta2: float\n",
        "\n",
        "    # Puzzle embedding\n",
        "    puzzle_emb_lr: float\n",
        "    puzzle_emb_weight_decay: float\n",
        "\n",
        "    # Names\n",
        "    project_name: Optional[str] = None\n",
        "    run_name: Optional[str] = None\n",
        "    load_checkpoint: Optional[str] = None\n",
        "    checkpoint_path: Optional[str] = None\n",
        "\n",
        "    # Extras\n",
        "    seed: int = 0\n",
        "    checkpoint_every_eval: bool = False\n",
        "    eval_interval: Optional[int] = None\n",
        "    min_eval_interval: Optional[int] = 0\n",
        "    eval_save_outputs: List[str] = []\n",
        "    max_eval_batches: Optional[int] = None\n",
        "\n",
        "    ema: bool = False\n",
        "    ema_rate: float = 0.999\n",
        "    freeze_weights: bool = False\n",
        "\n",
        "@dataclass\n",
        "class TrainState:\n",
        "    model: nn.Module\n",
        "    optimizers: Sequence[torch.optim.Optimizer]\n",
        "    optimizer_lrs: Sequence[float]\n",
        "    carry: Any\n",
        "\n",
        "    step: int\n",
        "    total_steps: int\n",
        "\n",
        "def create_dataloader(config: PretrainConfig, split: str, rank: int = 0, world_size: int = 1, **kwargs):\n",
        "    dataset = PuzzleDataset(PuzzleDatasetConfig(\n",
        "        seed=config.seed,\n",
        "        dataset_paths=config.data_paths_test if len(config.data_paths_test)>0 and split==\"test\" else config.data_paths,\n",
        "        rank=rank,\n",
        "        num_replicas=world_size,\n",
        "        **kwargs\n",
        "    ), split=split)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=None,\n",
        "        num_workers=1,\n",
        "        prefetch_factor=8,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "    return dataloader, dataset.metadata\n",
        "\n",
        "def create_model(config: PretrainConfig, train_metadata: PuzzleDatasetMetadata, rank: int = 0, world_size: int = 1):\n",
        "    model_cfg = dict(\n",
        "        **config.arch.__pydantic_extra__,  # type: ignore\n",
        "        batch_size=config.global_batch_size // world_size,\n",
        "        vocab_size=train_metadata.vocab_size,\n",
        "        seq_len=train_metadata.seq_len,\n",
        "        num_puzzle_identifiers=train_metadata.num_puzzle_identifiers,\n",
        "        causal=False\n",
        "    )\n",
        "\n",
        "    # Instantiate model with loss head\n",
        "    model_cls = load_model_class(config.arch.name)\n",
        "    loss_head_cls = load_model_class(config.arch.loss.name)\n",
        "\n",
        "    with torch.device(\"cuda\"):\n",
        "        model: nn.Module = model_cls(model_cfg)\n",
        "        print(model)\n",
        "        model = loss_head_cls(model, **config.arch.loss.__pydantic_extra__)  # type: ignore\n",
        "        if \"DISABLE_COMPILE\" not in os.environ:\n",
        "            model = torch.compile(model)  # type: ignore\n",
        "\n",
        "        # Load checkpoint\n",
        "        if rank == 0:\n",
        "            load_checkpoint(model, config)\n",
        "\n",
        "    # Optimizers and lr (using AdamW instead of AdamAtan2)\n",
        "    if config.arch.puzzle_emb_ndim == 0:\n",
        "        optimizers = [\n",
        "            torch.optim.AdamW(\n",
        "                model.parameters(),\n",
        "                lr=0,  # Needs to be set by scheduler\n",
        "                weight_decay=config.weight_decay,\n",
        "                betas=(config.beta1, config.beta2),\n",
        "                eps=1e-8\n",
        "            )\n",
        "        ]\n",
        "        optimizer_lrs = [config.lr]\n",
        "    elif config.freeze_weights:\n",
        "        # For frozen weights, we still need an optimizer for puzzle_emb\n",
        "        # CastedSparseEmbeddingSignSGD_Distributed is defined in Part 5 (same cell)\n",
        "        optimizers = [\n",
        "            CastedSparseEmbeddingSignSGD_Distributed(\n",
        "                model.model.puzzle_emb.buffers(),  # type: ignore\n",
        "                lr=0,\n",
        "                weight_decay=config.puzzle_emb_weight_decay,\n",
        "                world_size=world_size\n",
        "            )\n",
        "        ]\n",
        "        optimizer_lrs = [config.puzzle_emb_lr]\n",
        "    else:\n",
        "        # CastedSparseEmbeddingSignSGD_Distributed is defined in Part 5 (same cell)\n",
        "        optimizers = [\n",
        "            CastedSparseEmbeddingSignSGD_Distributed(\n",
        "                model.model.puzzle_emb.buffers(),  # type: ignore\n",
        "                lr=0,\n",
        "                weight_decay=config.puzzle_emb_weight_decay,\n",
        "                world_size=world_size\n",
        "            ),\n",
        "            torch.optim.AdamW(\n",
        "                model.parameters(),\n",
        "                lr=0,\n",
        "                weight_decay=config.weight_decay,\n",
        "                betas=(config.beta1, config.beta2),\n",
        "                eps=1e-8\n",
        "            )\n",
        "        ]\n",
        "        optimizer_lrs = [config.puzzle_emb_lr, config.lr]\n",
        "\n",
        "    return model, optimizers, optimizer_lrs\n",
        "\n",
        "def cosine_schedule_with_warmup_lr_lambda(\n",
        "    current_step: int, *, base_lr: float, num_warmup_steps: int, num_training_steps: int, min_ratio: float = 0.0, num_cycles: float = 0.5\n",
        "):\n",
        "    if current_step < num_warmup_steps:\n",
        "        return base_lr * float(current_step) / float(max(1, num_warmup_steps))\n",
        "\n",
        "    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "    return base_lr * (min_ratio + max(0.0, (1 - min_ratio) * 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))))\n",
        "\n",
        "def init_train_state(config: PretrainConfig, train_metadata: PuzzleDatasetMetadata, rank: int = 0, world_size: int = 1):\n",
        "    # Estimated total training steps\n",
        "    total_steps = int(config.epochs * train_metadata.total_groups * train_metadata.mean_puzzle_examples / config.global_batch_size)\n",
        "\n",
        "    # Model\n",
        "    model, optimizers, optimizer_lrs = create_model(config, train_metadata, rank=rank, world_size=world_size)\n",
        "\n",
        "    return TrainState(\n",
        "        step=0,\n",
        "        total_steps=total_steps,\n",
        "        model=model,\n",
        "        optimizers=optimizers,\n",
        "        optimizer_lrs=optimizer_lrs,\n",
        "        carry=None\n",
        "    )\n",
        "\n",
        "def save_train_state(config: PretrainConfig, train_state: TrainState):\n",
        "    if config.checkpoint_path is None:\n",
        "        return\n",
        "\n",
        "    os.makedirs(config.checkpoint_path, exist_ok=True)\n",
        "    torch.save(train_state.model.state_dict(), os.path.join(config.checkpoint_path, f\"step_{train_state.step}\"))\n",
        "\n",
        "def load_checkpoint(model: nn.Module, config: PretrainConfig):\n",
        "    if config.load_checkpoint is not None:\n",
        "        print(f\"Loading checkpoint {config.load_checkpoint}\")\n",
        "        state_dict = torch.load(config.load_checkpoint, map_location=\"cuda\")\n",
        "        \n",
        "        # Resize and reset puzzle emb if needed\n",
        "        puzzle_emb_name = \"_orig_mod.model.inner.puzzle_emb.weights\"\n",
        "        expected_shape: torch.Size = model.model.puzzle_emb.weights.shape  # type: ignore\n",
        "        if puzzle_emb_name in state_dict:\n",
        "            puzzle_emb = state_dict[puzzle_emb_name]\n",
        "            if puzzle_emb.shape != expected_shape:\n",
        "                print(f\"Resetting puzzle embedding as shape is different. Found {puzzle_emb.shape}, Expected {expected_shape}\")\n",
        "                state_dict[puzzle_emb_name] = (\n",
        "                    torch.mean(puzzle_emb, dim=0, keepdim=True).expand(expected_shape).contiguous()\n",
        "                )\n",
        "        model.load_state_dict(state_dict, assign=True)\n",
        "\n",
        "def compute_lr(base_lr: float, config: PretrainConfig, train_state: TrainState):\n",
        "    return cosine_schedule_with_warmup_lr_lambda(\n",
        "        current_step=train_state.step,\n",
        "        base_lr=base_lr,\n",
        "        num_warmup_steps=round(config.lr_warmup_steps),\n",
        "        num_training_steps=train_state.total_steps,\n",
        "        min_ratio=config.lr_min_ratio\n",
        "    )\n",
        "\n",
        "def compute_grad_norm(model: nn.Module) -> float:\n",
        "    \"\"\"\n",
        "    Compute the total gradient norm across all parameters.\n",
        "    Returns the L2 norm of all gradients.\n",
        "    \"\"\"\n",
        "    total_norm = 0.0\n",
        "    param_count = 0\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            param_norm = param.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "            param_count += 1\n",
        "    total_norm = total_norm ** (1. / 2)\n",
        "    return total_norm if param_count > 0 else 0.0\n",
        "\n",
        "def train_batch(config: PretrainConfig, train_state: TrainState, batch: Any, global_batch_size: int, rank: int = 0, world_size: int = 1):\n",
        "    train_state.step += 1\n",
        "    if train_state.step > train_state.total_steps:\n",
        "        return\n",
        "    \n",
        "    # To device\n",
        "    batch = {k: v.cuda() for k, v in batch.items()}\n",
        "    \n",
        "    # Init carry if it is None\n",
        "    if train_state.carry is None:\n",
        "        with torch.device(\"cuda\"):\n",
        "            train_state.carry = train_state.model.initial_carry(batch)  # type: ignore\n",
        "    \n",
        "    # Forward\n",
        "    train_state.carry, loss, metrics, _, _ = train_state.model(carry=train_state.carry, batch=batch, return_keys=[])\n",
        "    \n",
        "    # Check for NaN or Inf in loss\n",
        "    loss_value = loss.item() if isinstance(loss, torch.Tensor) else loss\n",
        "    if not (torch.isfinite(loss) if isinstance(loss, torch.Tensor) else (math.isfinite(loss_value) if isinstance(loss_value, float) else True)):\n",
        "        print(f\"âš ï¸ WARNING: Step {train_state.step} - Loss is NaN or Inf: {loss_value}\")\n",
        "        return None\n",
        "    \n",
        "    ((1 / global_batch_size) * loss).backward()\n",
        "    \n",
        "    # Allreduce (single GPU, skip)\n",
        "    if False:  # Single GPU\n",
        "        pass\n",
        "    \n",
        "    # Compute gradient norm for monitoring (no clipping, just recording)\n",
        "    grad_norm = compute_grad_norm(train_state.model)\n",
        "    \n",
        "    # Apply optimizer\n",
        "    lr_this_step = None    \n",
        "    for optim, base_lr in zip(train_state.optimizers, train_state.optimizer_lrs):\n",
        "        lr_this_step = compute_lr(base_lr, config, train_state)\n",
        "        for param_group in optim.param_groups:\n",
        "            param_group['lr'] = lr_this_step\n",
        "            \n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    \n",
        "    # Reduce metrics\n",
        "    if len(metrics):\n",
        "        assert not any(v.requires_grad for v in metrics.values())\n",
        "        metric_keys = list(sorted(metrics.keys()))\n",
        "        metric_values = torch.stack([metrics[k] for k in metric_keys])\n",
        "        if False:  # Single GPU\n",
        "            pass\n",
        "        if True:  # Single GPU, always rank 0\n",
        "            metric_values = metric_values.cpu().numpy()\n",
        "            reduced_metrics = {k: metric_values[i] for i, k in enumerate(metric_keys)}\n",
        "            \n",
        "            # Postprocess\n",
        "            count = max(reduced_metrics[\"count\"], 1)\n",
        "            reduced_metrics = {f\"train/{k}\": v / (global_batch_size if k.endswith(\"loss\") else count) for k, v in reduced_metrics.items()}\n",
        "            reduced_metrics[\"train/lr\"] = lr_this_step\n",
        "            \n",
        "            # Add gradient norm monitoring\n",
        "            if grad_norm is not None:\n",
        "                reduced_metrics[\"train/grad_norm\"] = float(grad_norm)\n",
        "            \n",
        "            # Note: GPU/system stats are automatically logged by wandb (_disable_stats=False)\n",
        "            # No need to manually record them\n",
        "            \n",
        "            return reduced_metrics\n",
        "\n",
        "def create_evaluators(config: PretrainConfig, eval_metadata: PuzzleDatasetMetadata) -> List[Any]:\n",
        "    data_paths = config.data_paths_test if len(config.data_paths_test)>0 else config.data_paths\n",
        "    evaluators = []\n",
        "    for cfg in config.evaluators:\n",
        "        for data_path in data_paths:\n",
        "            cls = load_model_class(cfg.name, \"evaluators.\")(\n",
        "                data_path=data_path, eval_metadata=eval_metadata, **cfg.__pydantic_extra__\n",
        "            )  # type: ignore\n",
        "            evaluators.append(cls)\n",
        "    return evaluators\n",
        "\n",
        "def evaluate(\n",
        "    config: PretrainConfig,\n",
        "    train_state: TrainState,\n",
        "    eval_loader: torch.utils.data.DataLoader,\n",
        "    eval_metadata: PuzzleDatasetMetadata,\n",
        "    evaluators: List[Any],\n",
        "    rank: int = 0,\n",
        "    world_size: int = 1,\n",
        "    cpu_group: Optional[Any] = None,\n",
        "):\n",
        "    reduced_metrics = None\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        return_keys = set(config.eval_save_outputs)\n",
        "        for evaluator in evaluators:\n",
        "            evaluator.begin_eval()\n",
        "            return_keys.update(evaluator.required_outputs)\n",
        "        \n",
        "        # Run evaluation\n",
        "        set_ids = {k: idx for idx, k in enumerate(eval_metadata.sets)}\n",
        "        save_preds = {}\n",
        "        metric_keys = []\n",
        "        metric_values = None\n",
        "        carry = None\n",
        "        processed_batches = 0\n",
        "        \n",
        "        for set_name, batch, global_batch_size in eval_loader:\n",
        "            if config.max_eval_batches is not None and processed_batches >= config.max_eval_batches:\n",
        "                break\n",
        "            processed_batches += 1\n",
        "            if rank == 0:\n",
        "                print(f\"Processing batch {processed_batches}: {set_name}\")\n",
        "            \n",
        "            # To device\n",
        "            batch = {k: v.cuda() for k, v in batch.items()}\n",
        "            with torch.device(\"cuda\"):\n",
        "                carry = train_state.model.initial_carry(batch)  # type: ignore\n",
        "\n",
        "            # Forward\n",
        "            inference_steps = 0\n",
        "            while True:\n",
        "                carry, loss, metrics, preds, all_finish = train_state.model(\n",
        "                    carry=carry, batch=batch, return_keys=return_keys\n",
        "                )\n",
        "                inference_steps += 1\n",
        "                if all_finish:\n",
        "                    break\n",
        "\n",
        "            if rank == 0:\n",
        "                print(f\"  Completed inference in {inference_steps} steps\")\n",
        "\n",
        "            for collection in (batch, preds):\n",
        "                for k, v in collection.items():\n",
        "                    if k in config.eval_save_outputs:\n",
        "                        save_preds.setdefault(k, [])\n",
        "                        save_preds[k].append(v.cpu())\n",
        "\n",
        "            for evaluator in evaluators:\n",
        "                evaluator.update_batch(batch, preds)\n",
        "\n",
        "            del carry, loss, preds, batch, all_finish\n",
        "\n",
        "            # Aggregate metrics\n",
        "            set_id = set_ids[set_name]\n",
        "            if metric_values is None:\n",
        "                metric_keys = list(sorted(metrics.keys()))\n",
        "                metric_values = torch.zeros(\n",
        "                    (len(set_ids), len(metrics.values())), dtype=torch.float32, device=\"cuda\"\n",
        "                )\n",
        "            metric_values[set_id] += torch.stack([metrics[k] for k in metric_keys])\n",
        "            del metrics\n",
        "\n",
        "        # Concatenate save preds\n",
        "        save_preds = {k: torch.cat(v, dim=0) for k, v in save_preds.items()}\n",
        "\n",
        "        # Save preds\n",
        "        if config.checkpoint_path is not None and len(save_preds):\n",
        "            os.makedirs(os.path.dirname(config.checkpoint_path), exist_ok=True)\n",
        "            torch.save(\n",
        "                save_preds, os.path.join(config.checkpoint_path, f\"step_{train_state.step}_all_preds.{rank}\")\n",
        "            )\n",
        "        del save_preds\n",
        "\n",
        "        # Reduce to rank 0\n",
        "        if metric_values is not None:\n",
        "            if False:  # Single GPU\n",
        "                pass\n",
        "            if True:  # Single GPU, always rank 0\n",
        "                reduced_metrics = metric_values.cpu().numpy()\n",
        "                reduced_metrics = {\n",
        "                    set_name: {\n",
        "                        metric_name: reduced_metrics[set_id, metric_id]\n",
        "                        for metric_id, metric_name in enumerate(metric_keys)\n",
        "                    }\n",
        "                    for set_id, set_name in enumerate(set_ids)\n",
        "                }\n",
        "                # Postprocess\n",
        "                for set_name, m in reduced_metrics.items():\n",
        "                    count = m.pop(\"count\")\n",
        "                    reduced_metrics[set_name] = {k: v / count for k, v in m.items()}\n",
        "\n",
        "        # Run evaluators\n",
        "        if rank == 0:\n",
        "            print(f\"\\nRunning {len(evaluators)} evaluator(s)...\")\n",
        "        for i, evaluator in enumerate(evaluators):\n",
        "            if rank == 0:\n",
        "                print(f\"Running evaluator {i+1}/{len(evaluators)}: {evaluator.__class__.__name__}\")\n",
        "            evaluator_save_path = None\n",
        "            if config.checkpoint_path is not None:\n",
        "                evaluator_save_path = os.path.join(\n",
        "                    config.checkpoint_path,\n",
        "                    f\"evaluator_{evaluator.__class__.__name__}_step_{train_state.step}\",\n",
        "                )\n",
        "                os.makedirs(evaluator_save_path, exist_ok=True)\n",
        "            metrics = evaluator.result(evaluator_save_path, rank=rank, world_size=world_size, group=cpu_group)\n",
        "            if rank == 0 and metrics is not None:\n",
        "                if reduced_metrics is None:\n",
        "                    reduced_metrics = {}\n",
        "                reduced_metrics.update(metrics)\n",
        "                print(f\"  Completed {evaluator.__class__.__name__}\")\n",
        "        if rank == 0:\n",
        "            print(\"All evaluators completed!\")\n",
        "\n",
        "    return reduced_metrics\n",
        "\n",
        "def launch(config_dict: dict):\n",
        "    \"\"\"\n",
        "    Launch training with a configuration dictionary.\n",
        "    Single GPU, non-distributed version.\n",
        "    \"\"\"\n",
        "    RANK = 0\n",
        "    WORLD_SIZE = 1\n",
        "    CPU_PROCESS_GROUP = None\n",
        "\n",
        "    # Load config\n",
        "    config = PretrainConfig(**config_dict)\n",
        "    \n",
        "    # Naming\n",
        "    if config.project_name is None:\n",
        "        config.project_name = \"TRM-A100-Sudoku\"\n",
        "    if config.run_name is None:\n",
        "        config.run_name = f\"{config.arch.name.split('@')[-1]} {coolname.generate_slug(2)}\"\n",
        "    if config.checkpoint_path is None:\n",
        "        config.checkpoint_path = os.path.join(\"checkpoints\", config.project_name, config.run_name)\n",
        "\n",
        "    # Seed RNGs\n",
        "    torch.random.manual_seed(config.seed + RANK)\n",
        "\n",
        "    # Dataset\n",
        "    train_epochs_per_iter = config.eval_interval if config.eval_interval is not None else config.epochs\n",
        "    total_iters = config.epochs // train_epochs_per_iter\n",
        "    assert config.epochs % train_epochs_per_iter == 0, \"Eval interval must be a divisor of total epochs.\"\n",
        "\n",
        "    train_loader, train_metadata = create_dataloader(\n",
        "        config, \"train\", test_set_mode=False, epochs_per_iter=train_epochs_per_iter,\n",
        "        global_batch_size=config.global_batch_size, rank=RANK, world_size=WORLD_SIZE\n",
        "    )\n",
        "    try:\n",
        "        eval_loader, eval_metadata = create_dataloader(\n",
        "            config, \"test\", test_set_mode=True, epochs_per_iter=1,\n",
        "            global_batch_size=config.global_batch_size, rank=RANK, world_size=WORLD_SIZE\n",
        "        )\n",
        "    except:\n",
        "        print(\"NO EVAL DATA FOUND\")\n",
        "        eval_loader = eval_metadata = None\n",
        "\n",
        "    try:\n",
        "        evaluators = create_evaluators(config, eval_metadata)\n",
        "    except:\n",
        "        print(\"No evaluator found\")\n",
        "        evaluators = []\n",
        "\n",
        "    # Train state\n",
        "    train_state = init_train_state(config, train_metadata, rank=RANK, world_size=WORLD_SIZE)\n",
        "\n",
        "    # Progress bar and logger\n",
        "    progress_bar = None\n",
        "    ema_helper = None\n",
        "    if RANK == 0:\n",
        "        progress_bar = tqdm(total=train_state.total_steps)\n",
        "        wandb.init(\n",
        "            project=config.project_name,\n",
        "            name=config.run_name,\n",
        "            config=config.model_dump(),\n",
        "            settings=wandb.Settings(_disable_stats=False)  # Enable automatic system stats\n",
        "        )\n",
        "        wandb.log({\"num_params\": sum(x.numel() for x in train_state.model.parameters())}, step=0)\n",
        "    if config.ema:\n",
        "        print('Setup EMA')\n",
        "        # EMAHelper is defined in Part 10 (cell 10) of this notebook\n",
        "        # No need to import from models.ema - it's already in the global namespace\n",
        "        ema_helper = EMAHelper(mu=config.ema_rate)\n",
        "        ema_helper.register(train_state.model)\n",
        "\n",
        "    # Training Loop\n",
        "    for _iter_id in range(total_iters):\n",
        "        print(f\"Epoch {_iter_id * train_epochs_per_iter}\")\n",
        "\n",
        "        # Train Iter\n",
        "        if RANK == 0:\n",
        "            print(\"TRAIN\")\n",
        "        train_state.model.train()\n",
        "        for set_name, batch, global_batch_size in train_loader:\n",
        "            metrics = train_batch(config, train_state, batch, global_batch_size, rank=RANK, world_size=WORLD_SIZE)\n",
        "            if RANK == 0 and metrics is not None:\n",
        "                wandb.log(metrics, step=train_state.step)\n",
        "                progress_bar.update(train_state.step - progress_bar.n)  # type: ignore\n",
        "            if config.ema:\n",
        "                ema_helper.update(train_state.model)\n",
        "\n",
        "        if _iter_id >= config.min_eval_interval:\n",
        "            # Evaluation\n",
        "            if RANK == 0:\n",
        "                print(\"EVALUATE\")\n",
        "            if config.ema:\n",
        "                print(\"SWITCH TO EMA\")\n",
        "                train_state_eval = copy.deepcopy(train_state)\n",
        "                train_state_eval.model = ema_helper.ema_copy(train_state_eval.model)\n",
        "            else:\n",
        "                train_state_eval = train_state\n",
        "            train_state_eval.model.eval()\n",
        "            metrics = evaluate(\n",
        "                config, train_state_eval, eval_loader, eval_metadata, evaluators,\n",
        "                rank=RANK, world_size=WORLD_SIZE, cpu_group=CPU_PROCESS_GROUP\n",
        "            )\n",
        "            if RANK == 0 and metrics is not None:\n",
        "                wandb.log(metrics, step=train_state.step)\n",
        "            \n",
        "            # Checkpointing\n",
        "            if RANK == 0:\n",
        "                print(\"SAVE CHECKPOINT\")\n",
        "            if RANK == 0 and (config.checkpoint_every_eval or (_iter_id == total_iters - 1)):\n",
        "                save_train_state(config, train_state_eval)\n",
        "            if config.ema:\n",
        "                del train_state_eval\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 13: Example Usage & Dataset Preparation\n",
        "\n",
        "### ðŸ“¦ Dataset Preparation\n",
        "\n",
        "Before training, you need to prepare your dataset. For **Sudoku-Extreme** dataset:\n",
        "\n",
        "```bash\n",
        "# Build Sudoku dataset (1000 examples, 1000 augmentations)\n",
        "python TinyRecursiveModels/dataset/build_sudoku_dataset.py \\\n",
        "  --output-dir data/sudoku-extreme-1k-aug-1000 \\\n",
        "  --subsample-size 1000 \\\n",
        "  --num-aug 1000\n",
        "```\n",
        "\n",
        "This will create the dataset in `data/sudoku-extreme-1k-aug-1000/` directory.\n",
        "\n",
        "**Note:** Make sure you have the `TinyRecursiveModels` folder in your workspace, or adjust the path accordingly.\n",
        "\n",
        "### âš™ï¸ Configuration Options\n",
        "\n",
        "Below are three pre-configured setups ready to use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ðŸ“‹ Available Configurations:\n",
            "======================================================================\n",
            "1. arc_config - ARC-AGI dataset\n",
            "2. sudoku_att_config - Sudoku-Extreme with Attention (â­ Recommended)\n",
            "3. sudoku_mlp_config - Sudoku-Extreme with MLP\n",
            "4. sudoku_test_config - Sudoku Minimal Test (ðŸ§ª Quick test: 100 epochs, batch 256)\n",
            "======================================================================\n",
            "âœ… Current configuration: Sudoku (Attention)\n",
            "ðŸ“ Data path: ['data/sudoku-extreme-1k-aug-1000']\n",
            "ðŸ”„ Epochs: 50000, Eval interval: 5000\n",
            "ðŸ“Š Batch size: 768\n",
            "ðŸŽ¯ Architecture: Attention (Transformer)\n",
            "ðŸ’¾ EMA: Enabled\n",
            "ðŸš€ To start training, call: launch(example_config)\n",
            "  Or use: launch(sudoku_att_config), launch(sudoku_mlp_config), or launch(sudoku_test_config)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Example Configurations\n",
        "# ============================================================================\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Configuration 1: ARC-AGI Dataset (Original)\n",
        "# ----------------------------------------------------------------------------\n",
        "arc_config = {\n",
        "    'arch': {\n",
        "        'name': 'recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1',\n",
        "        'loss': {\n",
        "            'name': 'losses@ACTLossHead',\n",
        "            'loss_type': 'stablemax_cross_entropy'\n",
        "        },\n",
        "        'halt_exploration_prob': 0.1,\n",
        "        'halt_max_steps': 16,\n",
        "        'H_cycles': 3,\n",
        "        'L_cycles': 6,\n",
        "        'H_layers': 0,\n",
        "        'L_layers': 2,\n",
        "        'hidden_size': 512,\n",
        "        'num_heads': 8,\n",
        "        'expansion': 4,\n",
        "        'puzzle_emb_ndim': 512,\n",
        "        'pos_encodings': 'rope',\n",
        "        'forward_dtype': 'bfloat16',\n",
        "        'mlp_t': False,\n",
        "        'puzzle_emb_len': 16,\n",
        "        'no_ACT_continue': True\n",
        "    },\n",
        "    'data_paths': ['data/arc1concept-aug-1000'],\n",
        "    'data_paths_test': [],\n",
        "    'evaluators': [{'name': 'arc@ARC'}],\n",
        "    'global_batch_size': 768,\n",
        "    'epochs': 100000,\n",
        "    'eval_interval': 10000,\n",
        "    'checkpoint_every_eval': True,\n",
        "    'lr': 1e-4,\n",
        "    'lr_min_ratio': 1.0,\n",
        "    'lr_warmup_steps': 2000,\n",
        "    'beta1': 0.9,\n",
        "    'beta2': 0.95,\n",
        "    'weight_decay': 0.1,\n",
        "    'puzzle_emb_weight_decay': 0.1,\n",
        "    'puzzle_emb_lr': 1e-2,\n",
        "    'seed': 0,\n",
        "    'min_eval_interval': 0,\n",
        "\t'max_eval_batches': 1000,  # Limit to 1000 batches for faster evaluation (~76,800 examples with batch_size=768)\n",
        "    'ema': False,\n",
        "    'ema_rate': 0.999,\n",
        "    'freeze_weights': False\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Configuration 2: Sudoku-Extreme Dataset (Attention Version) â­ RECOMMENDED\n",
        "# Based on: pretrain_att_sudoku from README\n",
        "# ----------------------------------------------------------------------------\n",
        "sudoku_att_config = {\n",
        "    'arch': {\n",
        "        'name': 'recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1',\n",
        "        'loss': {\n",
        "            'name': 'losses@ACTLossHead',\n",
        "            'loss_type': 'stablemax_cross_entropy'\n",
        "        },\n",
        "        'halt_exploration_prob': 0.1,\n",
        "        'halt_max_steps': 16,\n",
        "        'H_cycles': 3,\n",
        "        'L_cycles': 6,\n",
        "        'H_layers': 0,\n",
        "        'L_layers': 2,\n",
        "        'hidden_size': 512,\n",
        "        'num_heads': 8,\n",
        "        'expansion': 4,\n",
        "        'puzzle_emb_ndim': 512,\n",
        "        'pos_encodings': 'rope',  # Use RoPE for attention\n",
        "        'forward_dtype': 'bfloat16',\n",
        "        'mlp_t': False,  # Use attention (transformer)\n",
        "        'puzzle_emb_len': 16,\n",
        "        'no_ACT_continue': True\n",
        "    },\n",
        "    'data_paths': ['data/sudoku-extreme-1k-aug-1000'],\n",
        "    'data_paths_test': [],\n",
        "    'evaluators': [],  # No evaluator for Sudoku\n",
        "    'global_batch_size': 768,\n",
        "    'epochs': 50000,\n",
        "    'eval_interval': 5000,\n",
        "    'checkpoint_every_eval': True,\n",
        "    'lr': 1e-4,\n",
        "    'lr_min_ratio': 1.0,\n",
        "    'lr_warmup_steps': 2000,\n",
        "    'beta1': 0.9,\n",
        "    'beta2': 0.95,\n",
        "    'weight_decay': 1.0,  # Higher weight decay for Sudoku\n",
        "    'puzzle_emb_weight_decay': 1.0,  # Higher weight decay for Sudoku\n",
        "    'puzzle_emb_lr': 1e-4,  # Same as main lr for Sudoku\n",
        "    'seed': 0,\n",
        "    'min_eval_interval': 0,\n",
        "    'max_eval_batches': 1000,  # Limit to 1000 batches for faster evaluation (~76,800 examples with batch_size=768)\n",
        "    'ema': True,  # Use EMA for Sudoku\n",
        "    'ema_rate': 0.999,\n",
        "    'freeze_weights': False,\n",
        "\t'project_name': 'TRM-A100-Sudoku'  # Wandb project name\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Configuration 3: Sudoku-Extreme Dataset (MLP Version)\n",
        "# Based on: pretrain_mlp_t_sudoku from README\n",
        "# ----------------------------------------------------------------------------\n",
        "sudoku_mlp_config = {\n",
        "    'arch': {\n",
        "        'name': 'recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1',\n",
        "        'loss': {\n",
        "            'name': 'losses@ACTLossHead',\n",
        "            'loss_type': 'stablemax_cross_entropy'\n",
        "        },\n",
        "        'halt_exploration_prob': 0.1,\n",
        "        'halt_max_steps': 16,\n",
        "        'H_cycles': 3,\n",
        "        'L_cycles': 6,\n",
        "        'H_layers': 0,\n",
        "        'L_layers': 2,\n",
        "        'hidden_size': 512,\n",
        "        'num_heads': 8,\n",
        "        'expansion': 4,\n",
        "        'puzzle_emb_ndim': 512,\n",
        "        'pos_encodings': 'none',  # No positional encoding for MLP\n",
        "        'forward_dtype': 'bfloat16',\n",
        "        'mlp_t': True,  # Use MLP instead of transformer\n",
        "        'puzzle_emb_len': 16,\n",
        "        'no_ACT_continue': True\n",
        "    },\n",
        "    'data_paths': ['data/sudoku-extreme-1k-aug-1000'],\n",
        "    'data_paths_test': [],\n",
        "    'evaluators': [],  # No evaluator for Sudoku\n",
        "    'global_batch_size': 768,\n",
        "    'epochs': 50000,\n",
        "    'eval_interval': 5000,\n",
        "    'checkpoint_every_eval': True,\n",
        "    'lr': 1e-4,\n",
        "    'lr_min_ratio': 1.0,\n",
        "    'lr_warmup_steps': 2000,\n",
        "    'beta1': 0.9,\n",
        "    'beta2': 0.95,\n",
        "    'weight_decay': 1.0,  # Higher weight decay for Sudoku\n",
        "    'puzzle_emb_weight_decay': 1.0,  # Higher weight decay for Sudoku\n",
        "    'puzzle_emb_lr': 1e-4,  # Same as main lr for Sudoku\n",
        "    'seed': 0,\n",
        "    'min_eval_interval': 0,\n",
        "\t'max_eval_batches': 1000,  # Limit to 100 batches for faster evaluation (~76,800 examples with batch_size=768)\n",
        "    'ema': True,  # Use EMA for Sudoku\n",
        "    'ema_rate': 0.999,\n",
        "    'freeze_weights': False ,\n",
        "\t'project_name': 'TRM-A100-Sudoku'  # Wandb project name\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Select Configuration\n",
        "# ----------------------------------------------------------------------------\n",
        "# Choose one of the configurations above:\n",
        "# - arc_config: For ARC-AGI dataset\n",
        "# - sudoku_att_config: For Sudoku with Attention (Transformer) â­ RECOMMENDED\n",
        "# - sudoku_mlp_config: For Sudoku with MLP\n",
        "\n",
        "# Default to Sudoku Attention version\n",
        "example_config = sudoku_att_config\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ“‹ Available Configurations:\")\n",
        "print(\"=\"*70)\n",
        "print(\"1. arc_config - ARC-AGI dataset\")\n",
        "print(\"2. sudoku_att_config - Sudoku-Extreme with Attention (â­ Recommended)\")\n",
        "print(\"3. sudoku_mlp_config - Sudoku-Extreme with MLP\")\n",
        "print(\"4. sudoku_test_config - Sudoku Minimal Test (ðŸ§ª Quick test: 100 epochs, batch 256)\")\n",
        "print(\"=\"*70)\n",
        "config_name = 'Sudoku (Attention)' if example_config == sudoku_att_config else 'Sudoku (MLP)' if example_config == sudoku_mlp_config else 'ARC-AGI'\n",
        "print(f\"âœ… Current configuration: {config_name}\")\n",
        "print(f\"ðŸ“ Data path: {example_config['data_paths']}\")\n",
        "print(f\"ðŸ”„ Epochs: {example_config['epochs']}, Eval interval: {example_config['eval_interval']}\")\n",
        "print(f\"ðŸ“Š Batch size: {example_config['global_batch_size']}\")\n",
        "print(f\"ðŸŽ¯ Architecture: {'MLP' if example_config['arch']['mlp_t'] else 'Attention (Transformer)'}\")\n",
        "print(f\"ðŸ’¾ EMA: {'Enabled' if example_config['ema'] else 'Disabled'}\")\n",
        "print(\"ðŸš€ To start training, call: launch(example_config)\")\n",
        "print(\"  Or use: launch(sudoku_att_config), launch(sudoku_mlp_config), or launch(sudoku_test_config)\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration 4: Sudoku-Extreme Minimal Test Configuration "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ðŸ§ª Minimal Test Configuration Available:\n",
            "======================================================================\n",
            "sudoku_test_config - Quick test with reduced epochs (100) and batch size (256)\n",
            "To run test: launch(sudoku_test_config)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------------------\n",
        "# Configuration 4: Sudoku-Extreme Minimal Test Configuration ðŸ§ª\n",
        "# For quick testing - reduced epochs and batch size\n",
        "# ----------------------------------------------------------------------------\n",
        "sudoku_test_config_A100 = {\n",
        "    'arch': {\n",
        "        'name': 'recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1',\n",
        "        'loss': {\n",
        "            'name': 'losses@ACTLossHead',\n",
        "            'loss_type': 'stablemax_cross_entropy'\n",
        "        },\n",
        "        'halt_exploration_prob': 0.1,\n",
        "        'halt_max_steps': 16,\n",
        "        'H_cycles': 3,\n",
        "        'L_cycles': 6,\n",
        "        'H_layers': 0,\n",
        "        'L_layers': 2,\n",
        "        'hidden_size': 512,\n",
        "        'num_heads': 8,\n",
        "        'expansion': 4,\n",
        "        'puzzle_emb_ndim': 512,\n",
        "        'pos_encodings': 'rope',\n",
        "        'forward_dtype': 'bfloat16',\n",
        "        'mlp_t': False,\n",
        "        'puzzle_emb_len': 16,\n",
        "        'no_ACT_continue': True\n",
        "    },\n",
        "    'data_paths': ['data/sudoku-extreme-1k-aug-1000'],\n",
        "    'data_paths_test': [],\n",
        "    'evaluators': [],\n",
        "    'global_batch_size': 256,  # Reduced for testing\n",
        "    'epochs': 5000,  # Minimal epochs for quick test\n",
        "    'eval_interval': 1000,  # Evaluate every 50 epochs\n",
        "    'checkpoint_every_eval': True,\n",
        "    'lr': 2e-4,\n",
        "    'lr_min_ratio': 1.0,\n",
        "    'lr_warmup_steps': 100,  # Reduced warmup\n",
        "    'beta1': 0.9,\n",
        "    'beta2': 0.95,\n",
        "    'weight_decay': 1.0,\n",
        "    'puzzle_emb_weight_decay': 1.0,\n",
        "    'puzzle_emb_lr': 1e-4,\n",
        "    'seed': 0,\n",
        "    'min_eval_interval': 0,\n",
        "    'max_eval_batches': 100,  # Limit to 50 batches for faster evaluation (~12,800 examples with batch_size=256)\n",
        "    'ema': True,\n",
        "    'ema_rate': 0.999,\n",
        "    'freeze_weights': False,\n",
        "    'project_name': 'TRM-A100-Sudoku'  # Wandb project name\n",
        "}\n",
        "\n",
        "print(\"\" + \"=\"*70)\n",
        "print(\"ðŸ§ª Minimal Test Configuration Available:\")\n",
        "print(\"=\"*70)\n",
        "print(\"sudoku_test_config - Quick test with reduced epochs (100) and batch size (256)\")\n",
        "print(\"To run test: launch(sudoku_test_config)\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## build dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Successfully authenticated with Hugging Face Hub\n",
            "======================================================================\n",
            "ðŸ“¦ Building Sudoku Dataset\n",
            "======================================================================\n",
            "Source: sapientinc/sudoku-extreme\n",
            "Output: data/sudoku-extreme-1k-aug-1000\n",
            "Train subsample: 1000\n",
            "Augmentation: 1000\n",
            "======================================================================\n",
            "\n",
            "ðŸ“¥ Processing train set...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adc576c88c7c4ea4b1cc9eefb4a1cd7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train.csv:   0%|          | 0.00/719M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading CSV: 3831994it [00:32, 118403.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loaded 3831994 puzzles\n",
            "  Subsampled to 1000 puzzles\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Augmenting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:15<00:00,  7.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  âœ… Saved to data/sudoku-extreme-1k-aug-1000/train\n",
            "  ðŸ“Š Total examples: 1001000\n",
            "\n",
            "ðŸ“¥ Processing test set...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4efeeae2a73e43d2a2ac64dbd5474180",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test.csv:   0%|          | 0.00/79.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading CSV: 422786it [00:03, 119913.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loaded 422786 puzzles\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Augmenting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 422786/422786 [00:00<00:00, 1836918.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  âœ… Saved to data/sudoku-extreme-1k-aug-1000/test\n",
            "  ðŸ“Š Total examples: 422786\n",
            "\n",
            "======================================================================\n",
            "âœ… Dataset build complete!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Check and build dataset if needed\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm  import tqdm\n",
        "from huggingface_hub import hf_hub_download, login\n",
        "import warnings\n",
        "\n",
        "# Set Hugging Face Token for authentication\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "# Login to Hugging Face Hub\n",
        "try:\n",
        "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "    print(\"âœ… Successfully authenticated with Hugging Face Hub\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Warning: Could not login to Hugging Face Hub: {e}\")\n",
        "    print(\"   Continuing with token in environment variable...\")\n",
        "\n",
        "DATASET_DIR = \"data/sudoku-extreme-1k-aug-1000\"\n",
        "TRAIN_SUBSAMPLE_SIZE = 1000\n",
        "NUM_AUG = 1000  # Augmentation count\n",
        "MIN_DIFFICULTY = None  # Optional: filter by minimum difficulty rating\n",
        "\n",
        "def shuffle_sudoku(board: np.ndarray, solution: np.ndarray):\n",
        "    \"\"\"Apply equivalent transformations to Sudoku (preserves validity)\"\"\"\n",
        "    # Digit mapping: random permutation of 1-9\n",
        "    digit_map = np.pad(np.random.permutation(np.arange(1, 10)), (1, 0))\n",
        "    \n",
        "    # Random transpose\n",
        "    transpose_flag = np.random.rand() < 0.5\n",
        "\n",
        "    # Row permutation: shuffle 3 bands, then shuffle rows within each band\n",
        "    bands = np.random.permutation(3)\n",
        "    row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
        "\n",
        "    # Column permutation: same for columns\n",
        "    stacks = np.random.permutation(3)\n",
        "    col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
        "\n",
        "    # Build 81->81 position mapping\n",
        "    mapping = np.array([row_perm[i // 9] * 9 + col_perm[i % 9] for i in range(81)])\n",
        "\n",
        "    def apply_transformation(x: np.ndarray) -> np.ndarray:\n",
        "        if transpose_flag:\n",
        "            x = x.T\n",
        "        new_board = x.flatten()[mapping].reshape(9, 9).copy()\n",
        "        return digit_map[new_board]\n",
        "\n",
        "    return apply_transformation(board), apply_transformation(solution)\n",
        "\n",
        "def convert_subset(set_name: str):\n",
        "    \"\"\"Process train or test set\"\"\"\n",
        "    print(f\"\\nðŸ“¥ Processing {set_name} set...\")\n",
        "\n",
        "    # Download CSV from HuggingFace\n",
        "    # Use HF_TOKEN from environment variable for authentication\n",
        "    csv_path = hf_hub_download(\"sapientinc/sudoku-extreme\", f\"{set_name}.csv\", repo_type=\"dataset\", token=HF_TOKEN)\n",
        "\n",
        "    # Read CSV\n",
        "    inputs, labels = [], []\n",
        "    with open(csv_path, newline=\"\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)  # Skip header\n",
        "        for source, q, a, rating in tqdm(reader, desc=\"Reading CSV\"):\n",
        "            # Filter by difficulty if specified (matching original implementation)\n",
        "            if (MIN_DIFFICULTY is None) or (int(rating) >= MIN_DIFFICULTY):\n",
        "                assert len(q) == 81 and len(a) == 81\n",
        "                inputs.append(np.frombuffer(q.replace('.', '0').encode(), dtype=np.uint8).reshape(9, 9) - ord('0'))\n",
        "                labels.append(np.frombuffer(a.encode(), dtype=np.uint8).reshape(9, 9) - ord('0'))\n",
        "\n",
        "    print(f\"  Loaded {len(inputs)} puzzles\")\n",
        "\n",
        "    # Dataset subsampling (only for train set)\n",
        "    if set_name == \"train\" and TRAIN_SUBSAMPLE_SIZE is not None and TRAIN_SUBSAMPLE_SIZE < len(inputs):\n",
        "        indices = np.random.choice(len(inputs), size=TRAIN_SUBSAMPLE_SIZE, replace=False)\n",
        "        inputs = [inputs[i] for i in indices]\n",
        "        labels = [labels[i] for i in indices]\n",
        "        print(f\"  Subsampled to {len(inputs)} puzzles\")\n",
        "\n",
        "    # Data augmentation (only for train set)\n",
        "    num_augments = NUM_AUG if set_name == \"train\" else 0\n",
        "\n",
        "    # Build results\n",
        "    results = {k: [] for k in [\"inputs\", \"labels\", \"puzzle_identifiers\", \"puzzle_indices\", \"group_indices\"]}\n",
        "    puzzle_id = 0\n",
        "    example_id = 0\n",
        "\n",
        "    results[\"puzzle_indices\"].append(0)\n",
        "    results[\"group_indices\"].append(0)\n",
        "\n",
        "    for orig_inp, orig_out in tqdm(zip(inputs, labels), total=len(inputs), desc=\"Augmenting\"):\n",
        "        for aug_idx in range(1 + num_augments):\n",
        "            if aug_idx == 0:\n",
        "                inp, out = orig_inp, orig_out\n",
        "            else:\n",
        "                inp, out = shuffle_sudoku(orig_inp, orig_out)\n",
        "\n",
        "            results[\"inputs\"].append(inp)\n",
        "            results[\"labels\"].append(out)\n",
        "            example_id += 1\n",
        "            puzzle_id += 1\n",
        "\n",
        "            results[\"puzzle_indices\"].append(example_id)\n",
        "            results[\"puzzle_identifiers\"].append(0)\n",
        "\n",
        "        results[\"group_indices\"].append(puzzle_id)\n",
        "\n",
        "    # Convert to NumPy arrays\n",
        "    def seq_to_numpy(seq):\n",
        "        arr = np.concatenate(seq).reshape(len(seq), -1)\n",
        "        assert np.all((arr >= 0) & (arr <= 9))\n",
        "        return arr + 1  # Offset +1, 0 reserved for PAD\n",
        "\n",
        "    results = {\n",
        "        \"inputs\": seq_to_numpy(results[\"inputs\"]),\n",
        "        \"labels\": seq_to_numpy(results[\"labels\"]),\n",
        "        \"group_indices\": np.array(results[\"group_indices\"], dtype=np.int32),\n",
        "        \"puzzle_indices\": np.array(results[\"puzzle_indices\"], dtype=np.int32),\n",
        "        \"puzzle_identifiers\": np.array(results[\"puzzle_identifiers\"], dtype=np.int32),\n",
        "    }\n",
        "\n",
        "    # Metadata (matching original implementation exactly)\n",
        "    metadata = PuzzleDatasetMetadata(\n",
        "        seq_len=81,\n",
        "        vocab_size=10 + 1,  # PAD + \"0\" ... \"9\" (matching original)\n",
        "        pad_id=0,\n",
        "        ignore_label_id=0,\n",
        "        blank_identifier_id=0,\n",
        "        num_puzzle_identifiers=1,\n",
        "        total_groups=len(results[\"group_indices\"]) - 1,\n",
        "        mean_puzzle_examples=1,  # Fixed value as in original (even with augmentation)\n",
        "        total_puzzles=len(results[\"group_indices\"]) - 1,\n",
        "        sets=[\"all\"]\n",
        "    )\n",
        "\n",
        "    # Save\n",
        "    save_dir = os.path.join(DATASET_DIR, set_name)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    with open(os.path.join(save_dir, \"dataset.json\"), \"w\") as f:\n",
        "        json.dump(metadata.model_dump(), f)  # No indent to match original\n",
        "\n",
        "    for k, v in results.items():\n",
        "        np.save(os.path.join(save_dir, f\"all__{k}.npy\"), v)\n",
        "\n",
        "    print(f\"  âœ… Saved to {save_dir}\")\n",
        "    print(f\"  ðŸ“Š Total examples: {results['inputs'].shape[0]}\")\n",
        "    \n",
        "    return metadata\n",
        "\n",
        "# Check if dataset exists\n",
        "train_metadata_path = os.path.join(DATASET_DIR, \"train\", \"dataset.json\")\n",
        "test_metadata_path = os.path.join(DATASET_DIR, \"test\", \"dataset.json\")\n",
        "\n",
        "if os.path.exists(train_metadata_path) and os.path.exists(test_metadata_path):\n",
        "    print(\"=\"*70)\n",
        "    print(\"âœ… Dataset already exists!\")\n",
        "    print(f\"ðŸ“ Path: {DATASET_DIR}\")\n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"=\"*70)\n",
        "    print(\"ðŸ“¦ Building Sudoku Dataset\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Source: sapientinc/sudoku-extreme\")\n",
        "    print(f\"Output: {DATASET_DIR}\")\n",
        "    print(f\"Train subsample: {TRAIN_SUBSAMPLE_SIZE}\")\n",
        "    print(f\"Augmentation: {NUM_AUG}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Build train set\n",
        "    train_metadata = convert_subset(\"train\")\n",
        "    \n",
        "    # Build test set\n",
        "    test_metadata = convert_subset(\"test\")\n",
        "    \n",
        "    # Save identifiers.json\n",
        "    with open(os.path.join(DATASET_DIR, \"identifiers.json\"), \"w\") as f:\n",
        "        json.dump([\"<blank>\"], f)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"âœ… Dataset build complete!\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Starting minimal test configuration...\n",
            "======================================================================\n",
            "ðŸ“Š Test Configuration:\n",
            "  - Epochs: 5000 (quick test)\n",
            "  - Batch size: 256\n",
            "  - Eval interval: 1000\n",
            "  - Architecture: Attention (Transformer)\n",
            "  - EMA: Enabled\n",
            "======================================================================\n",
            "ðŸ’¡ This is a quick test. For full training, use: launch(sudoku_att_config)\n",
            "======================================================================\n",
            "TinyRecursiveReasoningModel_ACTV1(\n",
            "  (inner): TinyRecursiveReasoningModel_ACTV1_Inner(\n",
            "    (embed_tokens): CastedEmbedding()\n",
            "    (lm_head): CastedLinear()\n",
            "    (q_head): CastedLinear()\n",
            "    (puzzle_emb): CastedSparseEmbedding()\n",
            "    (rotary_emb): RotaryEmbedding()\n",
            "    (L_level): TinyRecursiveReasoningModel_ACTV1ReasoningModule(\n",
            "      (layers): ModuleList(\n",
            "        (0-1): 2 x TinyRecursiveReasoningModel_ACTV1Block(\n",
            "          (self_attn): Attention(\n",
            "            (qkv_proj): CastedLinear()\n",
            "            (o_proj): CastedLinear()\n",
            "          )\n",
            "          (mlp): SwiGLU(\n",
            "            (gate_up_proj): CastedLinear()\n",
            "            (down_proj): CastedLinear()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/19531 [00:00<?, ?it/s]"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251204_151708-3t0en1c3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Sudoku/runs/3t0en1c3' target=\"_blank\">TinyRecursiveReasoningModel_ACTV1 amiable-jackdaw</a></strong> to <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Sudoku' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Sudoku' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Sudoku</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Sudoku/runs/3t0en1c3' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Sudoku/runs/3t0en1c3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup EMA\n",
            "Epoch 0\n",
            "TRAIN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|â–ˆâ–‰        | 3905/19531 [07:10<24:54, 10.46it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|â–ˆâ–‰        | 3906/19531 [07:21<24:54, 10.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Completed inference in 16 steps\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 1000\n",
            "TRAIN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 7811/19531 [15:27<18:37, 10.49it/s]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 13: all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 7812/19531 [15:39<18:37, 10.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Completed inference in 16 steps\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 2000\n",
            "TRAIN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 11717/19531 [23:10<12:25, 10.48it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 13: all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 11718/19531 [23:21<12:25, 10.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Completed inference in 16 steps\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 100: all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11719/19531 [24:39<29:03:38, 13.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Completed inference in 16 steps\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 3000\n",
            "TRAIN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15623/19531 [30:53<06:15, 10.41it/s]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 16: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 18: all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15624/19531 [31:09<06:15, 10.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Completed inference in 16 steps\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 100: all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 15625/19531 [32:22<14:32:02, 13.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Completed inference in 16 steps\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n",
            "Epoch 4000\n",
            "TRAIN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 19529/19531 [38:35<00:00, 10.40it/s]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATE\n",
            "SWITCH TO EMA\n",
            "Processing batch 1: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 2: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 3: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 4: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 5: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 6: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 7: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 8: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 9: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 10: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 11: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 12: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 13: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 14: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 15: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 16: all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 19530/19531 [38:49<00:00, 10.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Completed inference in 16 steps\n",
            "Processing batch 17: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 18: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 19: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 20: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 21: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 22: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 23: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 24: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 25: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 26: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 27: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 28: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 29: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 30: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 31: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 32: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 33: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 34: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 35: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 36: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 37: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 38: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 39: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 40: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 41: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 42: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 43: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 44: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 45: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 46: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 47: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 48: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 49: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 50: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 51: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 52: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 53: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 54: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 55: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 56: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 57: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 58: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 59: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 60: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 61: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 62: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 63: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 64: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 65: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 66: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 67: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 68: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 69: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 70: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 71: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 72: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 73: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 74: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 75: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 76: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 77: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 78: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 79: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 80: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 81: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 82: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 83: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 84: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 85: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 86: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 87: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 88: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 89: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 90: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 91: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 92: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 93: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 94: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 95: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 96: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 97: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 98: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 99: all\n",
            "  Completed inference in 16 steps\n",
            "Processing batch 100: all\n",
            "  Completed inference in 16 steps\n",
            "\n",
            "Running 0 evaluator(s)...\n",
            "All evaluators completed!\n",
            "SAVE CHECKPOINT\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>num_params</td><td>â–</td></tr><tr><td>train/accuracy</td><td>â–â–â–â–†â–ˆâ–†â–†â–†â–‡â–†â–†â–‡â–†â–†â–†â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–†â–‡â–‡â–†â–‡â–†â–†â–‡â–†â–†â–‡â–†â–†â–‡</td></tr><tr><td>train/count</td><td>â–â–â–â–â–ˆâ–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/exact_accuracy</td><td>â–â–â–â–â–â–â–ƒâ–„â–â–‚â–‚â–â–â–‚â–ƒâ–„â–„â–„â–‡â–‚â–…â–„â–„â–ƒâ–â–ƒâ–…â–ˆâ–ƒâ–…â–†â–„â–†â–†â–‡â–†â–…â–†â–ˆâ–‡</td></tr><tr><td>train/grad_norm</td><td>â–…â–‚â–â–â–…â–â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–„â–ƒâ–„â–â–ƒâ–â–‚â–‚â–ƒâ–ƒâ–â–„â–‚â–‚â–ƒâ–â–‚â–â–â–ˆâ–†â–…â–â–„â–„â–</td></tr><tr><td>train/lm_loss</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/lr</td><td>â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/q_halt_accuracy</td><td>â–ˆâ–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/q_halt_loss</td><td>â–â–â–â–â–â–„â–â–â–â–â–„â–ƒâ–‚â–‚â–‚â–„â–â–‚â–ƒâ–â–…â–â–„â–‚â–„â–ƒâ–â–‡â–â–â–â–‡â–ˆâ–…â–â–…â–…â–„â–‚â–‚</td></tr><tr><td>train/steps</td><td>â–â–â–â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–†â–‡â–‡â–†â–‡</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>num_params</td><td>6828034</td></tr><tr><td>train/accuracy</td><td>0.81191</td></tr><tr><td>train/count</td><td>1</td></tr><tr><td>train/exact_accuracy</td><td>0.35294</td></tr><tr><td>train/grad_norm</td><td>0.21474</td></tr><tr><td>train/lm_loss</td><td>0.74323</td></tr><tr><td>train/lr</td><td>0.0002</td></tr><tr><td>train/q_halt_accuracy</td><td>0.94118</td></tr><tr><td>train/q_halt_loss</td><td>0.0184</td></tr><tr><td>train/steps</td><td>12.70588</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">TinyRecursiveReasoningModel_ACTV1 amiable-jackdaw</strong> at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Sudoku/runs/3t0en1c3' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Sudoku/runs/3t0en1c3</a><br> View project at: <a href='https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Sudoku' target=\"_blank\">https://wandb.ai/jarviszhang-new-york-university/TRM-A100-Sudoku</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251204_151708-3t0en1c3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 19530/19531 [40:06<00:00,  8.12it/s]\n"
          ]
        }
      ],
      "source": [
        "# Run the minimal test configuration for quick testing\n",
        "print(\"ðŸš€ Starting minimal test configuration...\")\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ“Š Test Configuration:\")\n",
        "print(f\"  - Epochs: {sudoku_test_config_A100['epochs']} (quick test)\")\n",
        "print(f\"  - Batch size: {sudoku_test_config_A100['global_batch_size']}\")\n",
        "print(f\"  - Eval interval: {sudoku_test_config_A100['eval_interval']}\")\n",
        "print(f\"  - Architecture: Attention (Transformer)\")\n",
        "print(f\"  - EMA: {'Enabled' if sudoku_test_config_A100['ema'] else 'Disabled'}\")\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ’¡ This is a quick test. For full training, use: launch(sudoku_att_config)\")\n",
        "print(\"=\"*70)\n",
        "launch(sudoku_test_config_A100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
